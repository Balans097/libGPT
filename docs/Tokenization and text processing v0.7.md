# –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏

**–í–µ—Ä—Å–∏—è:** 0.7  
**–î–∞—Ç–∞:** 2026-02-01  
**–ê–≤—Ç–æ—Ä:** github.com/Balans097

---

## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–í–≤–µ–¥–µ–Ω–∏–µ](#–≤–≤–µ–¥–µ–Ω–∏–µ)
2. [–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö](#—Ç–∏–ø—ã-–¥–∞–Ω–Ω—ã—Ö)
3. [–û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤](#–æ–±—É—á–µ–Ω–∏–µ-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤)
4. [–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ](#—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è-–∏-–¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ)
5. [–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞](#–ø–∞–∫–µ—Ç–Ω–∞—è-–æ–±—Ä–∞–±–æ—Ç–∫–∞)
6. [–†–∞–±–æ—Ç–∞ —Å —Ç–µ–∫—Å—Ç–æ–º](#—Ä–∞–±–æ—Ç–∞-—Å-—Ç–µ–∫—Å—Ç–æ–º)
7. [–ú–µ—Ç—Ä–∏–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑](#–º–µ—Ç—Ä–∏–∫–∏-–∏-–∞–Ω–∞–ª–∏–∑)
8. [–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞](#—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ-–∏-–∑–∞–≥—Ä—É–∑–∫–∞)
9. [–£—Ç–∏–ª–∏—Ç—ã](#—É—Ç–∏–ª–∏—Ç—ã)
10. [–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏](#–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ-—Ñ—É–Ω–∫—Ü–∏–∏)

---

## –í–≤–µ–¥–µ–Ω–∏–µ

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP). –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Ä–µ–∞–ª–∏–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –≤ —Ç–∞–∫–∏—Ö –º–æ–¥–µ–ª—è—Ö –∫–∞–∫ BERT, GPT-2/3, T5 –∏ –¥—Ä—É–≥–∏—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö.

### –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- **–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏:**
  - **BPE (Byte Pair Encoding)** - –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ–¥—Å–ª–æ–≤–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
  - **WordPiece** - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ BERT, —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–∏–º–≤–æ–ª–æ–≤ —Å –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏ –ø–æ–¥—Å–ª–æ–≤
  - **SentencePiece** - —è–∑—ã–∫–æ–≤–æ-–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–∞ –æ—Å–Ω–æ–≤–µ unigram language model
  - **Byte-Level BPE** - —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å GPT-2/GPT-3, —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ –±–∞–π—Ç–æ–≤

- **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å:**
  - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (20-50x –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –Ω–∞–∏–≤–Ω—ã–º–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è–º–∏)
  - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–π —Å LRU –ø–æ–ª–∏—Ç–∏–∫–æ–π
  - –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –ø–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
  - –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Ñ–∞–π–ª–æ–≤ –ª—é–±–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
  - –ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π

- **–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞:**
  - –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
  - –û–±—Ä–∞–±–æ—Ç–∫–∞ Unicode (NFKC –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, zero-width —Å–∏–º–≤–æ–ª—ã)
  - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –¥–µ—Ç–µ–∫—Ü–∏–µ–π —è–∑—ã–∫–∞
  - –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∏—Å–µ–ª, —ç–º–æ–¥–∑–∏, —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
  - –†–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ü–∏–π

- **–ê–Ω–∞–ª–∏–∑ –∏ –º–µ—Ç—Ä–∏–∫–∏:**
  - –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
  - –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–≤–∞—Ä—è –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤
  - –î–µ—Ç–µ–∫—Ü–∏—è OOV (out-of-vocabulary) —Å–ª–æ–≤
  - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
  - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ—Ç–ª–∞–¥–∫–∞

- **–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—ë–º:**
  - –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è
  - –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–µ–π
  - Pruning (–æ—á–∏—Å—Ç–∫–∞) —Ä–µ–¥–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
  - –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏

- **–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
  - Subword regularization (BPE-dropout) –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
  - Tracking –ø–æ–∑–∏—Ü–∏–π —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è NER/QA –∑–∞–¥–∞—á
  - –ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è Masked Language Modeling
  - –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏ (HuggingFace, SentencePiece, TikToken)

### –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

1. **–û–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π** - –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è BERT, GPT, T5 –∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π
2. **Fine-tuning** - –∞–¥–∞–ø—Ç–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –ø–æ–¥ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –¥–æ–º–µ–Ω—ã
3. **–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è** - –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö
4. **Production —Å–∏—Å—Ç–µ–º—ã** - –±—ã—Å—Ç—Ä–∞—è –∏ –Ω–∞–¥—ë–∂–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
5. **–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è** - —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏

### –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –≤—ã—Å–æ–∫–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã:
- –û–±—Ä–∞–±–æ—Ç–∫–∞ **–º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É** –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö CPU
- –ü–∞–º—è—Ç—å: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∑–∞ —Å—á—ë—Ç –ø–æ—Ç–æ–∫–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
- –ö–æ–º–ø–∏–ª—è—Ü–∏—è: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–µ–∂–∏–º–æ–≤ `-d:release` –∏ `-d:danger --opt:speed` –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º–∏ –∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏:
- HuggingFace Tokenizers
- SentencePiece –º–æ–¥–µ–ª–∏
- TikToken (OpenAI)
- –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —Å–ª–æ–≤–∞—Ä–µ–π (JSON)

### –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

```nim
import tokenization

# 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ—Ä–ø—É—Å–∞
let corpus = @[
  "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —ç—Ç–æ –≤–∞–∂–Ω–∞—è –æ–±–ª–∞—Å—Ç—å",
  "–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏"
]

# 2. –û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
let tokenizer = trainBPE(corpus, vocabSize = 1000)

# 3. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
let tokens = tokenizer.encode("–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")

# 4. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
let text = tokenizer.decode(tokens)

# 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
tokenizer.save("tokenizer.json")
```

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏

–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–∞ –ø–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º —Ä–∞–∑–¥–µ–ª–∞–º:
- **–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö** - –æ–ø–∏—Å–∞–Ω–∏–µ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤, –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–π –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä
- **–û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤** - —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
- **–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ** - –æ—Å–Ω–æ–≤–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å —Ç–µ–∫—Å—Ç–æ–º
- **–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞** - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤
- **–†–∞–±–æ—Ç–∞ —Å —Ç–µ–∫—Å—Ç–æ–º** - –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, –æ—á–∏—Å—Ç–∫–∞ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
- **–ú–µ—Ç—Ä–∏–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑** - –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –∞–Ω–∞–ª–∏–∑ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
- **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞** - –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π
- **–£—Ç–∏–ª–∏—Ç—ã** - –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
- **–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏** - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏


---

## –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö

### –ü–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è (Enumerations)

#### TokenizerKind
–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.

```nim
type TokenizerKind* = enum
  tkBPE = 0              # Byte Pair Encoding
  tkWordPiece = 1        # WordPiece (BERT-style)
  tkSentencePiece = 2    # SentencePiece (unigram)
  tkByteLevelBPE = 3     # Byte-Level BPE (GPT-2/3 style)
```

**–û–ø–∏—Å–∞–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π:**
- `tkBPE` - –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π Byte Pair Encoding, —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–∏–º–≤–æ–ª–æ–≤
- `tkWordPiece` - –∞–ª–≥–æ—Ä–∏—Ç–º WordPiece, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ BERT
- `tkSentencePiece` - SentencePiece —Å unigram language model
- `tkByteLevelBPE` - BPE –Ω–∞ —É—Ä–æ–≤–Ω–µ –±–∞–π—Ç–æ–≤, —Å–æ–≤–º–µ—Å—Ç–∏–º —Å GPT-2/GPT-3

#### NumberNormalizationStrategy
–°—Ç—Ä–∞—Ç–µ–≥–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —á–∏—Å–µ–ª –≤ —Ç–µ–∫—Å—Ç–µ.

```nim
type NumberNormalizationStrategy* = enum
  nsKeepOriginal      # –û—Å—Ç–∞–≤–∏—Ç—å —á–∏—Å–ª–∞ –∫–∞–∫ –µ—Å—Ç—å
  nsReplaceWithToken  # –ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ [NUM]
  nsReplaceWithDigits # –ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Ä–∞–∑—Ä—è–¥—ã (123 -> [NUM_3DIGIT])
  nsNormalize         # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å (1,234.56 -> 1234.56)
```

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:**
- `nsKeepOriginal` - —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —á–∏—Å–µ–ª
- `nsReplaceWithToken` - –∑–∞–º–µ–Ω—è–µ—Ç –≤—Å–µ —á–∏—Å–ª–∞ –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω `[NUM]`
- `nsReplaceWithDigits` - –∑–∞–º–µ–Ω—è–µ—Ç —á–∏—Å–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ä–∞–∑—Ä—è–¥–æ–≤
- `nsNormalize` - –ø—Ä–∏–≤–æ–¥–∏—Ç —á–∏—Å–ª–∞ –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É –±–µ–∑ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π

#### EmojiStrategy
–°—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–º–æ–¥–∑–∏.

```nim
type EmojiStrategy* = enum
  esKeep      # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —ç–º–æ–¥–∑–∏
  esRemove    # –£–¥–∞–ª–∏—Ç—å —ç–º–æ–¥–∑–∏
  esReplace   # –ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
  esTokenize  # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ
```

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:**
- `esKeep` - –æ—Å—Ç–∞–≤–ª—è–µ—Ç —ç–º–æ–¥–∑–∏ –∫–∞–∫ –µ—Å—Ç—å
- `esRemove` - –ø–æ–ª–Ω–æ—Å—Ç—å—é —É–¥–∞–ª—è–µ—Ç —ç–º–æ–¥–∑–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞
- `esReplace` - –∑–∞–º–µ–Ω—è–µ—Ç —ç–º–æ–¥–∑–∏ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
- `esTokenize` - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —ç–º–æ–¥–∑–∏ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã

### –°—Ç—Ä—É–∫—Ç—É—Ä—ã (Objects)

#### SpecialTokens
–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á NLP.

```nim
type SpecialTokens* = object
  padToken*: string      # –¢–æ–∫–µ–Ω –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è (padding)
  unkToken*: string      # –¢–æ–∫–µ–Ω –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞ (unknown)
  bosToken*: string      # –ù–∞—á–∞–ª–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
  eosToken*: string      # –ö–æ–Ω–µ—Ü –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
  sepToken*: string      # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
  clsToken*: string      # –¢–æ–∫–µ–Ω –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
  maskToken*: string     # –¢–æ–∫–µ–Ω –º–∞—Å–∫–∏
```

**–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**
- `padToken`: `"[PAD]"` - –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
- `unkToken`: `"[UNK]"` - –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
- `bosToken`: `"[BOS]"` - –º–∞—Ä–∫–µ—Ä –Ω–∞—á–∞–ª–∞ —Ç–µ–∫—Å—Ç–∞
- `eosToken`: `"[EOS]"` - –º–∞—Ä–∫–µ—Ä –∫–æ–Ω—Ü–∞ —Ç–µ–∫—Å—Ç–∞
- `sepToken`: `"[SEP]"` - —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (BERT)
- `clsToken`: `"[CLS]"` - –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
- `maskToken`: `"[MASK]"` - –¥–ª—è masked language modeling

#### TokenOffset
–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–∑–∏—Ü–∏–∏ —Ç–æ–∫–µ–Ω–∞ –≤ —Ç–µ–∫—Å—Ç–µ.

```nim
type TokenOffset* = object
  token*: string       # –°–∞–º —Ç–æ–∫–µ–Ω
  tokenId*: int        # ID —Ç–æ–∫–µ–Ω–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ
  startChar*: int      # –ù–∞—á–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –≤ —Å–∏–º–≤–æ–ª–∞—Ö
  endChar*: int        # –ö–æ–Ω–µ—á–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –≤ —Å–∏–º–≤–æ–ª–∞—Ö
  startByte*: int      # –ù–∞—á–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –≤ –±–∞–π—Ç–∞—Ö
  endByte*: int        # –ö–æ–Ω–µ—á–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –≤ –±–∞–π—Ç–∞—Ö
```

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∑–∞–¥–∞—á NER (Named Entity Recognition) –∏ QA (Question Answering).

#### Tokenizer
–û—Å–Ω–æ–≤–Ω–æ–π –æ–±—ä–µ–∫—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.

```nim
type Tokenizer* = ref object
  kind*: TokenizerKind                    # –¢–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
  vocab*: Table[string, int]              # –°–ª–æ–≤–∞—Ä—å: —Ç–æ–∫–µ–Ω ‚Üí ID
  inverseVocab*: seq[string]              # –û–±—Ä–∞—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å: ID ‚Üí —Ç–æ–∫–µ–Ω
  merges*: seq[BPEMerge]                  # –ü—Ä–∞–≤–∏–ª–∞ —Å–ª–∏—è–Ω–∏—è –¥–ª—è BPE
  specialTokens*: SpecialTokens           # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
  specialTokenIds*: Table[string, int]    # ID —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
  maxInputCharsPerWord*: int              # –ú–∞–∫—Å. –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞
  continuingSubwordPrefix*: string        # –ü—Ä–µ—Ñ–∏–∫—Å –ø–æ–¥—Å–ª–æ–≤ ("##")
  scores*: Table[string, float]           # Scores –¥–ª—è SentencePiece
  byteFallback*: bool                     # Byte fallback
  preserveCase*: bool                     # –°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–≥–∏—Å—Ç—Ä
  cache*: Table[string, seq[int]]         # –ö—ç—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–π
  cacheMaxSize*: int                      # –†–∞–∑–º–µ—Ä –∫—ç—à–∞
  cacheHits*: int                         # –ü–æ–ø–∞–¥–∞–Ω–∏—è –≤ –∫—ç—à
  cacheMisses*: int                       # –ü—Ä–æ–º–∞—Ö–∏ –∫—ç—à–∞
  byteEncoder*: Table[int, string]        # –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ –±–∞–π—Ç–æ–≤
  byteDecoder*: Table[string, int]        # –î–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –±–∞–π—Ç–æ–≤
```

#### BatchEncoding
–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞–∫–µ—Ç–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.

```nim
type BatchEncoding* = object
  inputIds*: seq[seq[int]]         # ID —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
  attentionMask*: seq[seq[int]]    # –ú–∞—Å–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è
  tokenTypeIds*: seq[seq[int]]     # ID —Ç–∏–ø–∞ —Ç–æ–∫–µ–Ω–∞
  lengths*: seq[int]               # –î–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
```

#### TokenizerMetrics
–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.

```nim
type TokenizerMetrics* = object
  vocabSize*: int              # –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
  compressionRatio*: float     # –ö–æ—ç—Ñ—Ñ. —Å–∂–∞—Ç–∏—è (chars/tokens)
  avgTokensPerWord*: float     # –°—Ä–µ–¥–Ω–µ–µ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —Å–ª–æ–≤–æ
  vocabUtilization*: float     # –î–æ–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è
  unkTokenRate*: float         # –î–æ–ª—è UNK —Ç–æ–∫–µ–Ω–æ–≤
  tokensPerSecond*: float      # –°–∫–æ—Ä–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
```

#### VocabAnalysis
–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–≤–∞—Ä—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.

```nim
type VocabAnalysis* = object
  vocabSize*: int
  avgTokenLength*: float
  typeTokenRatio*: float
  coverageRate*: float
  oovRate*: float
  mostFrequent*: seq[tuple[token: string, freq: int]]
  leastFrequent*: seq[tuple[token: string, freq: int]]
  lengthDistribution*: CountTable[int]
```

### –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã

```nim
const
  MAX_INPUT_LENGTH* = 1_000_000    # –ú–∞–∫—Å. –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (1M —Å–∏–º–≤–æ–ª–æ–≤)
  MAX_VOCAB_SIZE* = 100_000        # –ú–∞–∫—Å. —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
  TOKENIZER_VERSION* = "1.0.0"     # –í–µ—Ä—Å–∏—è —Ñ–æ—Ä–º–∞—Ç–∞
```


---

## –û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤

### trainBPE
–û–±—É—á–∞–µ—Ç Byte Pair Encoding —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä.

```nim
proc trainBPE*(
  corpus: seq[string],
  vocabSize: int = 5000,
  specialTokens: SpecialTokens = defaultSpecialTokens(),
  minFrequency: int = 2,
  progressCallback: proc(current, total: int) = nil
): Tokenizer
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `corpus` - –æ–±—É—á–∞—é—â–∏–π –∫–æ—Ä–ø—É—Å (—Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤)
- `vocabSize` - –∂–µ–ª–∞–µ–º—ã–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5000)
- `specialTokens` - —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
- `minFrequency` - –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –ø–∞—Ä—ã –¥–ª—è —Å–ª–∏—è–Ω–∏—è
- `progressCallback` - —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** –û–±—É—á–µ–Ω–Ω—ã–π BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä.

**–ü—Ä–∏–º–µ—Ä:**
```nim
let corpus = @[
  "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —ç—Ç–æ –∫—Ä—É—Ç–æ",
  "–æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π"
]
let tokenizer = trainBPE(corpus, vocabSize = 1000)
```

### trainWordPiece
–û–±—É—á–∞–µ—Ç WordPiece —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä.

```nim
proc trainWordPiece*(
  corpus: seq[string],
  vocabSize: int = 5000,
  specialTokens: SpecialTokens = defaultSpecialTokens(),
  continuingSubwordPrefix: string = "##",
  minFrequency: int = 2
): Tokenizer
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `corpus` - –æ–±—É—á–∞—é—â–∏–π –∫–æ—Ä–ø—É—Å
- `vocabSize` - —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
- `specialTokens` - —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
- `continuingSubwordPrefix` - –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –ø–æ–¥—Å–ª–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é `"##"`)
- `minFrequency` - –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** –û–±—É—á–µ–Ω–Ω—ã–π WordPiece —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä.

**–ü—Ä–∏–º–µ—Ä:**
```nim
let tokenizer = trainWordPiece(corpus, vocabSize = 1000)
let tokens = tokenizer.encode("–Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–π")
# –ú–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å: ["–Ω–µ", "##–ø—Ä–µ–¥—Å–∫–∞–∑", "##—É–µ–º—ã–π"]
```

### trainSentencePiece
–û–±—É—á–∞–µ—Ç SentencePiece —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä.

```nim
proc trainSentencePiece*(
  corpus: seq[string],
  vocabSize: int = 5000,
  specialTokens: SpecialTokens = defaultSpecialTokens(),
  characterCoverage: float = 0.9995,
  minFrequency: int = 2
): Tokenizer
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `corpus` - –æ–±—É—á–∞—é—â–∏–π –∫–æ—Ä–ø—É—Å
- `vocabSize` - —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
- `characterCoverage` - –¥–æ–ª—è —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø–æ–∫—Ä—ã—Ç–∏—è
- `minFrequency` - –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** –û–±—É—á–µ–Ω–Ω—ã–π SentencePiece —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä.

### trainByteLevelBPE
–û–±—É—á–∞–µ—Ç Byte-Level BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (GPT-2/GPT-3 style).

```nim
proc trainByteLevelBPE*(
  corpus: seq[string],
  vocabSize: int = 5000,
  specialTokens: SpecialTokens = defaultSpecialTokens(),
  minFrequency: int = 2
): Tokenizer
```

**–û–ø–∏—Å–∞–Ω–∏–µ:** –†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ –±–∞–π—Ç–æ–≤, –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ª—é–±–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ UNK —Ç–æ–∫–µ–Ω–æ–≤.

**–ü—Ä–∏–º–µ—Ä:**
```nim
let tokenizer = trainByteLevelBPE(corpus, vocabSize = 5000)
let tokens = tokenizer.encode("Hello üåç")  # –û–±—Ä–∞–±–æ—Ç–∞–µ—Ç –ª—é–±—ã–µ —Å–∏–º–≤–æ–ª—ã
```

---

## –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ

### encode
–ö–æ–¥–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å ID —Ç–æ–∫–µ–Ω–æ–≤.

```nim
proc encode*(
  tokenizer: Tokenizer,
  text: string,
  addSpecialTokens: bool = true,
  maxLength: int = -1,
  truncation: bool = false,
  padding: bool = false
): seq[int]
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `tokenizer` - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
- `text` - –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
- `addSpecialTokens` - –¥–æ–±–∞–≤–ª—è—Ç—å –ª–∏ BOS/EOS —Ç–æ–∫–µ–Ω—ã
- `maxLength` - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ (-1 = –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)
- `truncation` - –æ–±—Ä–µ–∑–∞—Ç—å –ª–∏ –¥–æ maxLength
- `padding` - –¥–æ–ø–æ–ª–Ω—è—Ç—å –ª–∏ –¥–æ maxLength

**–ü—Ä–∏–º–µ—Ä:**
```nim
let ids = tokenizer.encode("–ü—Ä–∏–≤–µ—Ç –º–∏—Ä", addSpecialTokens = true)
// ids = [1, 245, 678, 2]  # –≥–¥–µ 1 = [BOS], 2 = [EOS]

let paddedIds = tokenizer.encode("–ö–æ—Ä–æ—Ç–∫–∏–π", maxLength = 10, padding = true)
```

### decode
–î–µ–∫–æ–¥–∏—Ä—É–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å ID –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç.

```nim
proc decode*(
  tokenizer: Tokenizer,
  ids: seq[int],
  skipSpecialTokens: bool = true,
  cleanUpTokenization: bool = true
): string
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `tokenizer` - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
- `ids` - –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å ID —Ç–æ–∫–µ–Ω–æ–≤
- `skipSpecialTokens` - –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å –ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
- `cleanUpTokenization` - —É–±–∏—Ä–∞—Ç—å –ª–∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏

**–ü—Ä–∏–º–µ—Ä:**
```nim
let text = tokenizer.decode(@[1, 245, 678, 2], skipSpecialTokens = true)
// text = "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä"
```

### tokenize
–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–æ–∫–µ–Ω—ã (—Å—Ç—Ä–æ–∫–∏).

```nim
proc tokenize*(
  tokenizer: Tokenizer,
  text: string,
  addSpecialTokens: bool = false
): seq[string]
```

**–ü—Ä–∏–º–µ—Ä:**
```nim
let tokens = tokenizer.tokenize("–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")
// tokens = @["–º–∞—à–∏–Ω", "##–Ω–æ–µ", "–æ–±—É—á", "##–µ–Ω–∏–µ"]
```

### tokenizeWithOffsets
–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏–π.

```nim
proc tokenizeWithOffsets*(
  tokenizer: Tokenizer,
  text: string
): seq[TokenOffset]
```

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** –î–ª—è –∑–∞–¥–∞—á NER, QA, –≥–¥–µ –Ω—É–∂–Ω–æ –∑–Ω–∞—Ç—å –ø–æ–∑–∏—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤.

**–ü—Ä–∏–º–µ—Ä:**
```nim
let offsets = tokenizer.tokenizeWithOffsets("Hello world")
for offset in offsets:
  echo "Token: ", offset.token
  echo "  Chars: ", offset.startChar, "..", offset.endChar
  echo "  Bytes: ", offset.startByte, "..", offset.endByte
```

---

## –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞

### encodeBatch
–ö–æ–¥–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ.

```nim
proc encodeBatch*(
  tokenizer: Tokenizer,
  texts: seq[string],
  addSpecialTokens: bool = true,
  maxLength: int = -1,
  truncation: bool = false,
  padding: bool = false
): BatchEncoding
```

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** `BatchEncoding` —Å –≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã–º–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏.

**–ü—Ä–∏–º–µ—Ä:**
```nim
let texts = @["–ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç", "–≠—Ç–æ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"]
let batch = tokenizer.encodeBatch(
  texts,
  maxLength = 15,
  truncation = true,
  padding = true
)
// batch.inputIds - –≤—Å–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª–∏–Ω—ã 15
// batch.attentionMask - –º–∞—Å–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è
```

### encodeBatchParallel
–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –ø–∞–∫–µ—Ç–Ω–∞—è –∫–æ–¥–∏—Ä–æ–≤–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤.

```nim
proc encodeBatchParallel*(
  tokenizer: Tokenizer,
  texts: seq[string],
  addSpecialTokens: bool = true,
  maxLength: int = -1,
  numThreads: int = 0
): seq[seq[int]]
```

**–û–ø–∏—Å–∞–Ω–∏–µ:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è. –ú–æ–∂–µ—Ç –¥–∞—Ç—å —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ 2-4 —Ä–∞–∑–∞ –Ω–∞ –º–Ω–æ–≥–æ—è–¥–µ—Ä–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞—Ö.

**–ü—Ä–∏–º–µ—Ä:**
```nim
let largeCorpus = readLargeCorpus()  # –ú–∏–ª–ª–∏–æ–Ω —Ç–µ–∫—Å—Ç–æ–≤
let encoded = tokenizer.encodeBatchParallel(largeCorpus, numThreads = 8)
```


---

## –†–∞–±–æ—Ç–∞ —Å —Ç–µ–∫—Å—Ç–æ–º

### cleanText
–ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞.

```nim
proc cleanText*(
  text: string,
  removeHtml: bool = true,
  removeUrls: bool = true,
  removeEmails: bool = false,
  normalizeWhitespace: bool = true,
  toLowerCase: bool = false
): string
```

**–ü—Ä–∏–º–µ—Ä:**
```nim
let dirty = "<p>–ü–æ—Å–µ—Ç–∏—Ç–µ https://example.com</p>  \n\n  –¥–ª—è –¥–µ—Ç–∞–ª–µ–π"
let clean = cleanText(dirty, removeHtml = true, removeUrls = true)
// clean = "–ü–æ—Å–µ—Ç–∏—Ç–µ –¥–ª—è –¥–µ—Ç–∞–ª–µ–π"
```

### normalizeNumbers
–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —á–∏—Å–ª–∞ –≤ —Ç–µ–∫—Å—Ç–µ —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.

```nim
proc normalizeNumbers*(
  text: string,
  strategy: NumberNormalizationStrategy = nsNormalize
): string
```

**–ü—Ä–∏–º–µ—Ä:**
```nim
let text1 = normalizeNumbers("–£ –º–µ–Ω—è 5 —è–±–ª–æ–∫", nsReplaceWithToken)
// text1 = "–£ –º–µ–Ω—è [NUM] —è–±–ª–æ–∫"

let text2 = normalizeNumbers("–¶–µ–Ω–∞ 1234 —Ä—É–±–ª—è", nsReplaceWithDigits)
// text2 = "–¶–µ–Ω–∞ [NUM_4DIGIT] —Ä—É–±–ª—è"
```

### handleContractions
–†–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ.

```nim
proc handleContractions*(
  text: string,
  language: string = "en"
): string
```

**–ü—Ä–∏–º–µ—Ä:**
```nim
let en = handleContractions("I'm learning", language = "en")
// en = "I am learning"

let ru = handleContractions("—ç—Ç–æ —Ç.–µ. –ø—Ä–∏–º–µ—Ä", language = "ru")
// ru = "—ç—Ç–æ —Ç–æ –µ—Å—Ç—å –ø—Ä–∏–º–µ—Ä"
```

### normalizeWhitespaceAdvanced
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤.

```nim
proc normalizeWhitespaceAdvanced*(
  text: string,
  preserveNewlines: bool = false
): string
```

**–û–ø–∏—Å–∞–Ω–∏–µ:** –ó–∞–º–µ–Ω—è–µ—Ç –≤—Å–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –æ–¥–∏–Ω–æ—á–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã.

### handleZeroWidthChars
–£–¥–∞–ª—è–µ—Ç zero-width —Å–∏–º–≤–æ–ª—ã.

```nim
proc handleZeroWidthChars*(text: string): string
```

**–û–ø–∏—Å–∞–Ω–∏–µ:** –£–¥–∞–ª—è–µ—Ç –Ω–µ–≤–∏–¥–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω—É–ª–µ–≤–æ–π —à–∏—Ä–∏–Ω—ã.

### fullNormalization
–ü–æ–ª–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (NFKC + –æ—á–∏—Å—Ç–∫–∞).

```nim
proc fullNormalization*(text: string): string
```

**–û–ø–∏—Å–∞–Ω–∏–µ:** –í—ã–ø–æ–ª–Ω—è–µ—Ç Unicode –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é NFKC, —É–¥–∞–ª—è–µ—Ç zero-width —Å–∏–º–≤–æ–ª—ã –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø—Ä–æ–±–µ–ª—ã.

---

## –ú–µ—Ç—Ä–∏–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑

### getMetrics
–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.

```nim
proc getMetrics*(
  tokenizer: Tokenizer,
  corpus: seq[string]
): TokenizerMetrics
```

**–ü—Ä–∏–º–µ—Ä:**
```nim
let metrics = getMetrics(tokenizer, testCorpus)
echo "–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: ", metrics.vocabSize
echo "–ö–æ—ç—Ñ—Ñ. —Å–∂–∞—Ç–∏—è: ", metrics.compressionRatio
echo "UNK rate: ", metrics.unkTokenRate * 100, "%"
echo "–°–∫–æ—Ä–æ—Å—Ç—å: ", metrics.tokensPerSecond, " —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫"
```

### analyzeVocabulary
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ—Å—Ç–∞–≤ —Å–ª–æ–≤–∞—Ä—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.

```nim
proc analyzeVocabulary*(
  tokenizer: Tokenizer,
  corpus: seq[string],
  topK: int = 10
): VocabAnalysis
```

**–ü—Ä–∏–º–µ—Ä:**
```nim
let analysis = analyzeVocabulary(tokenizer, corpus, topK = 20)
echo "–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: ", analysis.vocabSize
echo "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∞: ", analysis.avgTokenLength
echo "–ü–æ–∫—Ä—ã—Ç–∏–µ: ", analysis.coverageRate * 100, "%"

echo "\n–°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã:"
for (token, freq) in analysis.mostFrequent:
  echo "  ", token, ": ", freq
```

### compareTokenizers
–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –Ω–∞ –æ–¥–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ.

```nim
proc compareTokenizers*(
  tokenizers: seq[Tokenizer],
  corpus: seq[string],
  names: seq[string] = @[]
): Table[string, TokenizerMetrics]
```

**–ü—Ä–∏–º–µ—Ä:**
```nim
let bpe = trainBPE(corpus, 5000)
let wp = trainWordPiece(corpus, 5000)
let sp = trainSentencePiece(corpus, 5000)

let comparison = compareTokenizers(
  @[bpe, wp, sp],
  testCorpus,
  names = @["BPE", "WordPiece", "SentencePiece"]
)

for name, metrics in comparison:
  echo name, ": compression=", metrics.compressionRatio
```

### getTokenStatistics
–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–æ–∫–µ–Ω–∞–º –≤ –∫–æ—Ä–ø—É—Å–µ.

```nim
proc getTokenStatistics*(
  tokenizer: Tokenizer,
  corpus: seq[string]
): tuple[
  totalTokens: int,
  uniqueTokens: int,
  avgLength: float,
  maxLength: int,
  minLength: int
]
```

---

## –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞

### save
–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ —Ñ–∞–π–ª.

```nim
proc save*(tokenizer: Tokenizer, filepath: string)
```

**–§–æ—Ä–º–∞—Ç:** JSON —Å–æ –≤—Å–µ–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏, —Å–ª–æ–≤–∞—Ä—ë–º –∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏ —Å–ª–∏—è–Ω–∏—è.

**–ü—Ä–∏–º–µ—Ä:**
```nim
tokenizer.save("models/my_tokenizer.json")
```

### load
–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ —Ñ–∞–π–ª–∞.

```nim
proc load*(filepath: string): Tokenizer
```

**–ü—Ä–∏–º–µ—Ä:**
```nim
let tokenizer = load("models/my_tokenizer.json")
```

### saveVersioned
–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –≤–µ—Ä—Å–∏–∏.

```nim
proc saveVersioned*(
  tokenizer: Tokenizer,
  filepath: string,
  trainedOn: string = ""
)
```

**–û–ø–∏—Å–∞–Ω–∏–µ:** –î–æ–±–∞–≤–ª—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –≤–µ—Ä—Å–∏–∏, –¥–∞—Ç–µ —Å–æ–∑–¥–∞–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏—è.

**–ü—Ä–∏–º–µ—Ä:**
```nim
tokenizer.saveVersioned(
  "models/tokenizer_v2.json",
  trainedOn = "Wikipedia RU + News 2025"
)
```

### loadVersioned
–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≤–µ—Ä—Å–∏–∏.

```nim
proc loadVersioned*(filepath: string): VersionedTokenizer
```

**–ü—Ä–∏–º–µ—Ä:**
```nim
let versioned = loadVersioned("models/tokenizer_v2.json")
echo "–í–µ—Ä—Å–∏—è: ", versioned.metadata.version
echo "–û–±—É—á–µ–Ω –Ω–∞: ", versioned.metadata.trainedOn
let tokenizer = versioned.tokenizer
```


---

## –£—Ç–∏–ª–∏—Ç—ã

### –†–∞–±–æ—Ç–∞ —Å–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏

#### defaultSpecialTokens
–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–±–æ—Ä —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.

```nim
proc defaultSpecialTokens*(): SpecialTokens
```

**–ü—Ä–∏–º–µ—Ä:**
```nim
let special = defaultSpecialTokens()
echo special.padToken   // "[PAD]"
echo special.unkToken   // "[UNK]"
```

#### addSpecialTokens
–î–æ–±–∞–≤–ª—è–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä.

```nim
proc addSpecialTokens*(tokenizer: var Tokenizer, tokens: seq[string])
```

**–ü—Ä–∏–º–µ—Ä:**
```nim
var tokenizer = trainBPE(corpus, 5000)
tokenizer.addSpecialTokens(@["[QUERY]", "[ANSWER]", "[CONTEXT]"])
```

#### maskTokens
–°–ª—É—á–∞–π–Ω–æ –º–∞—Å–∫–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω—ã –¥–ª—è Masked Language Modeling.

```nim
proc maskTokens*(
  tokenIds: seq[int],
  tokenizer: Tokenizer,
  maskProb: float = 0.15,
  randomProb: float = 0.1,
  keepProb: float = 0.1
): tuple[masked: seq[int], labels: seq[int]]
```

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è BERT-–ø–æ–¥–æ–±–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.

**–ü—Ä–∏–º–µ—Ä:**
```nim
let tokenIds = @[101, 2003, 4521, 8765, 102]
let (masked, labels) = maskTokens(tokenIds, tokenizer, maskProb = 0.15)
// masked = @[101, 2003, [MASK_ID], 8765, 102]
// labels = @[-100, -100, 4521, -100, -100]  # -100 = –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å
```

### –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ

#### newLRUCache
–°–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–π LRU –∫—ç—à.

```nim
proc newLRUCache*(maxSize: int = 10000): LRUCache
```

#### put (LRUCache)
–î–æ–±–∞–≤–ª—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∫—ç—à.

```nim
proc put*(cache: var LRUCache, key: string, value: seq[int])
```

#### get (LRUCache)
–ü–æ–ª—É—á–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫—ç—à–∞.

```nim
proc get*(cache: var LRUCache, key: string): Option[seq[int]]
```

**–ü—Ä–∏–º–µ—Ä:**
```nim
var cache = newLRUCache(maxSize = 5000)
cache.put("—Ç–µ–∫—Å—Ç", @[1, 2, 3, 4])

let result = cache.get("—Ç–µ–∫—Å—Ç")
if result.isSome:
  echo "–ù–∞–π–¥–µ–Ω–æ –≤ –∫—ç—à–µ: ", result.get()
```

#### getStats (LRUCache)
–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞.

```nim
proc getStats*(cache: LRUCache): tuple[size: int, hits: int, misses: int, hitRate: float]
```

### –í–∞–ª–∏–¥–∞—Ü–∏—è

#### validateInput
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.

```nim
proc validateInput*(text: string): Option[string]
```

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** `Some(errorMessage)` –µ—Å–ª–∏ –µ—Å—Ç—å –æ—à–∏–±–∫–∞, `None` –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –≤–∞–ª–∏–¥–µ–Ω.

**–ü—Ä–æ–≤–µ—Ä–∫–∏:**
- –¢–µ–∫—Å—Ç –Ω–µ –ø—É—Å—Ç–æ–π
- –î–ª–∏–Ω–∞ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç MAX_INPUT_LENGTH
- –ù–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö Unicode –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π

**–ü—Ä–∏–º–µ—Ä:**
```nim
let error = validateInput(userInput)
if error.isSome:
  echo "–û—à–∏–±–∫–∞: ", error.get()
  return
```

#### validateTokenizer
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.

```nim
proc validateTokenizer*(tokenizer: Tokenizer): Option[string]
```

### –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ID —Ç–æ–∫–µ–Ω–æ–≤

```nim
proc getUnkTokenId*(tokenizer: Tokenizer): int
proc getPadTokenId*(tokenizer: Tokenizer): int
proc getBosTokenId*(tokenizer: Tokenizer): int
proc getEosTokenId*(tokenizer: Tokenizer): int
proc getSepTokenId*(tokenizer: Tokenizer): int
proc getClsTokenId*(tokenizer: Tokenizer): int
proc getMaskTokenId*(tokenizer: Tokenizer): int
```

**–ü—Ä–∏–º–µ—Ä:**
```nim
let unkId = tokenizer.getUnkTokenId()
let padId = tokenizer.getPadTokenId()
```

### Unicode –æ–ø–µ—Ä–∞—Ü–∏–∏

#### runeCount
–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä—É–Ω (Unicode —Å–∏–º–≤–æ–ª–æ–≤).

```nim
proc runeCount*(text: string): int
```

**–ü—Ä–∏–º–µ—Ä:**
```nim
echo runeCount("Hello")    // 5
echo runeCount("–ü—Ä–∏–≤–µ—Ç")   // 6
echo runeCount("üåçüåé")     // 2
```

#### truncateToRunes
–û–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä—É–Ω.

```nim
proc truncateToRunes*(text: string, maxRunes: int): string
```

**–û–ø–∏—Å–∞–Ω–∏–µ:** –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å —É—á—ë—Ç–æ–º –º–Ω–æ–≥–æ–±–∞–π—Ç–æ–≤—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤.

---

## –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏

### –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ BPE

#### encodeWithDropout
–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å BPE-dropout –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö.

```nim
proc encodeWithDropout*(
  tokenizer: Tokenizer,
  text: string,
  dropoutRate: float = 0.1,
  seed: int = 0
): seq[int]
```

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** –°—É–±—Å–ª–æ–≤–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è - —Å–æ–∑–¥–∞–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏.

**–ü—Ä–∏–º–µ—Ä:**
```nim
let normal = tokenizer.encode("–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")
// normal = @[245, 678]

let dropout1 = tokenizer.encodeWithDropout("–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", dropoutRate = 0.3)
// dropout1 = @[24, 56, 67, 89]  # –±–æ–ª–µ–µ –º–µ–ª–∫–∏–µ —Ç–æ–∫–µ–Ω—ã

let dropout2 = tokenizer.encodeWithDropout("–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", dropoutRate = 0.3)
// dropout2 = @[245, 67, 89]  # –¥—Ä—É–≥–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è
```

#### reverseBPE
–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.

```nim
proc reverseBPE*(
  tokenizer: Tokenizer,
  text: string,
  maxSegmentations: int = 10
): seq[seq[string]]
```

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** –ê–Ω–∞–ª–∏–∑ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, –ø–æ–∏—Å–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö —Ä–∞–∑–±–∏–µ–Ω–∏–π.

### –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—ë–º

#### addTokens
–î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ —Å–ª–æ–≤–∞—Ä—å.

```nim
proc addTokens*(tokenizer: var Tokenizer, tokens: seq[string])
```

**–û–ø–∏—Å–∞–Ω–∏–µ:** –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ —Ä–∞—Å—à–∏—Ä—è–µ—Ç —Å–ª–æ–≤–∞—Ä—å –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.

#### findCommonTokens
–ù–∞—Ö–æ–¥–∏—Ç –æ–±—â–∏–µ —Ç–æ–∫–µ–Ω—ã –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞—Ö.

```nim
proc findCommonTokens*(
  tokenizers: seq[Tokenizer],
  minCount: int = 2
): seq[string]
```

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** –ê–Ω–∞–ª–∏–∑ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤–∞—Ä–µ–π, –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ–±—â–µ–≥–æ —Å–ª–æ–≤–∞—Ä—è.

#### alignVocabularies
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–ª–æ–≤–∞—Ä–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤.

```nim
proc alignVocabularies*(
  tokenizer1: Tokenizer,
  tokenizer2: Tokenizer
): Tokenizer
```

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** –°–æ–∑–¥–∞–Ω–∏–µ –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤.

**–ü—Ä–∏–º–µ—Ä:**
```nim
let generalTokenizer = trainBPE(generalCorpus, 30000)
let domainTokenizer = trainBPE(medicalCorpus, 10000)
let combined = alignVocabularies(generalTokenizer, domainTokenizer)
```

#### pruneVocabulary
–£–¥–∞–ª—è–µ—Ç —Ä–µ–¥–∫–∏–µ —Ç–æ–∫–µ–Ω—ã –∏–∑ —Å–ª–æ–≤–∞—Ä—è.

```nim
proc pruneVocabulary*(
  tokenizer: var Tokenizer,
  corpus: seq[string],
  minFrequency: int = 5,
  keepSpecial: bool = true
)
```

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏.

**–ü—Ä–∏–º–µ—Ä:**
```nim
var tokenizer = load("large_tokenizer.json")
echo "–†–∞–∑–º–µ—Ä –¥–æ: ", tokenizer.vocab.len
tokenizer.pruneVocabulary(corpus, minFrequency = 10)
echo "–†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ: ", tokenizer.vocab.len
```

### –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç—å

#### detectLanguage
–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ (—Å–∫—Ä–∏–ø—Ç) —Ç–µ–∫—Å—Ç–∞.

```nim
proc detectLanguage*(text: string): string
```

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** `"latin"`, `"cyrillic"`, `"cjk"`, `"arabic"`, `"mixed"` –∏ —Ç.–¥.

**–ü—Ä–∏–º–µ—Ä:**
```nim
echo detectLanguage("Hello world")        // "latin"
echo detectLanguage("–ü—Ä–∏–≤–µ—Ç –º–∏—Ä")         // "cyrillic"
echo detectLanguage("Hello –ü—Ä–∏–≤–µ—Ç")       // "mixed"
```

#### detectScriptMixing
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–º–µ—à–∏–≤–∞–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ.

```nim
proc detectScriptMixing*(
  text: string
): seq[tuple[script: string, count: int]]
```

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** –î–µ—Ç–µ–∫—Ü–∏—è code-switching, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–º–µ—à–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤.

### –ê–Ω–∞–ª–∏–∑ OOV

#### analyzeOOVWords
–ù–∞—Ö–æ–¥–∏—Ç out-of-vocabulary —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ.

```nim
proc analyzeOOVWords*(tokenizer: Tokenizer, text: string): seq[string]
```

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** –û—Ü–µ–Ω–∫–∞ –ø–æ–∫—Ä—ã—Ç–∏—è —Å–ª–æ–≤–∞—Ä—è, –ø–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è.

### –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü

#### extractTokenBoundaries
–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ.

```nim
proc extractTokenBoundaries*(
  tokenizer: Tokenizer,
  text: string
): seq[tuple[start: int, end: int, token: string]]
```

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.

### –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞

#### tokenizeStream
–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –≤ –ø–æ—Ç–æ–∫–æ–≤–æ–º —Ä–µ–∂–∏–º–µ.

```nim
proc tokenizeStream*(
  tokenizer: Tokenizer,
  inputPath: string,
  outputPath: string,
  batchSize: int = 1000,
  progressCallback: proc(processed: int) = nil
)
```

**–û–ø–∏—Å–∞–Ω–∏–µ:** –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤ –ø–∞–º—è—Ç—å.

**–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞:** JSON Lines
```json
{"tokens": [1, 245, 678, 2], "length": 4}
{"tokens": [1, 890, 123, 456, 2], "length": 5}
```

**–ü—Ä–∏–º–µ—Ä:**
```nim
tokenizer.tokenizeStream(
  "large_corpus.txt",
  "tokenized_corpus.jsonl",
  batchSize = 5000,
  progressCallback = proc(processed: int) =
    if processed mod 10000 == 0:
      echo "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–æ–∫: ", processed
)
```

### –ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å

#### newThreadSafeTokenizer
–°–æ–∑–¥–∞—ë—Ç –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω—É—é –æ–±—ë—Ä—Ç–∫—É.

```nim
proc newThreadSafeTokenizer*(tokenizer: Tokenizer): ThreadSafeTokenizer
```

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ—Ç–æ–∫–æ–≤.

**–ü—Ä–∏–º–µ—Ä:**
```nim
let safeTokenizer = newThreadSafeTokenizer(tokenizer)
// –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–∑ —Ä–∞–∑–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
```

---

## –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ü—Ä–∏–º–µ—Ä 1: –ë–∞–∑–æ–≤–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è

```nim
import tokenization

let corpus = @[
  "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —ç—Ç–æ –≤–∞–∂–Ω–∞—è –æ–±–ª–∞—Å—Ç—å",
  "–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏"
]

let tokenizer = trainBPE(corpus, vocabSize = 1000)
let text = "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏"
let tokens = tokenizer.tokenize(text)
let tokenIds = tokenizer.encode(text)

echo "–¢–æ–∫–µ–Ω—ã: ", tokens
echo "IDs: ", tokenIds

let decoded = tokenizer.decode(tokenIds)
echo "–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: ", decoded

tokenizer.save("my_tokenizer.json")
```

### –ü—Ä–∏–º–µ—Ä 2: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤

```nim
let corpus = readCorpus("data/train.txt")

let bpe = trainBPE(corpus, vocabSize = 5000)
let wordpiece = trainWordPiece(corpus, vocabSize = 5000)
let sentencepiece = trainSentencePiece(corpus, vocabSize = 5000)

let comparison = compareTokenizers(
  @[bpe, wordpiece, sentencepiece],
  testCorpus,
  names = @["BPE", "WordPiece", "SentencePiece"]
)

for name, metrics in comparison:
  echo name, ": compression=", metrics.compressionRatio
```

### –ü—Ä–∏–º–µ—Ä 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è BERT

```nim
let tokenizer = load("bert_tokenizer.json")

let sent1 = "–ö–∞–∫–∞—è –ø–æ–≥–æ–¥–∞ —Å–µ–≥–æ–¥–Ω—è?"
let sent2 = "–°–µ–≥–æ–¥–Ω—è —Å–æ–ª–Ω–µ—á–Ω–æ –∏ —Ç–µ–ø–ª–æ."

let ids1 = tokenizer.encode(sent1, addSpecialTokens = false)
let ids2 = tokenizer.encode(sent2, addSpecialTokens = false)

// [CLS] sent1 [SEP] sent2 [SEP]
var inputIds = @[tokenizer.getClsTokenId()]
inputIds.add(ids1)
inputIds.add(tokenizer.getSepTokenId())
inputIds.add(ids2)
inputIds.add(tokenizer.getSepTokenId())

var tokenTypeIds: seq[int] = @[]
tokenTypeIds.add(0)  // [CLS]
for i in 0..<ids1.len: tokenTypeIds.add(0)
tokenTypeIds.add(0)  // [SEP]
for i in 0..<ids2.len: tokenTypeIds.add(1)
tokenTypeIds.add(1)  // [SEP]

let attentionMask = newSeqWith(inputIds.len, 1)
```

### –ü—Ä–∏–º–µ—Ä 4: Masked Language Modeling

```nim
let tokenizer = load("mlm_tokenizer.json")
let text = "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º—ã"
let tokenIds = tokenizer.encode(text, addSpecialTokens = true)

let (maskedIds, labels) = maskTokens(
  tokenIds,
  tokenizer,
  maskProb = 0.15
)

echo "–û—Ä–∏–≥–∏–Ω–∞–ª: ", tokenizer.decode(tokenIds)
echo "–ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω: ", tokenizer.decode(maskedIds)
```

### –ü—Ä–∏–º–µ—Ä 5: –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞

```nim
let tokenizer = load("tokenizer.json")

tokenizer.tokenizeStream(
  inputPath = "large_dataset.txt",
  outputPath = "tokenized_dataset.jsonl",
  batchSize = 10000,
  progressCallback = proc(processed: int) =
    if processed mod 100000 == 0:
      echo "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: ", processed
)
```

### –ü—Ä–∏–º–µ—Ä 6: –°—É–±—Å–ª–æ–≤–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è

```nim
let tokenizer = trainBPE(corpus, vocabSize = 5000)
let text = "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"

for i in 1..5:
  let tokens = tokenizer.encodeWithDropout(text, dropoutRate = 0.3, seed = i)
  echo "–í–∞—Ä–∏–∞–Ω—Ç ", i, ": ", tokenizer.decode(tokens)
```

### –ü—Ä–∏–º–µ—Ä 7: –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–≤–∞—Ä—è

```nim
let tokenizer = load("tokenizer.json")
let analysis = analyzeVocabulary(tokenizer, testCorpus, topK = 20)

echo "–†–∞–∑–º–µ—Ä: ", analysis.vocabSize
echo "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: ", analysis.avgTokenLength
echo "–ü–æ–∫—Ä—ã—Ç–∏–µ: ", analysis.coverageRate * 100, "%"

echo "\n–¢–æ–ø-20 —á–∞—Å—Ç—ã—Ö:"
for (token, freq) in analysis.mostFrequent:
  echo "  '", token, "': ", freq
```

### –ü—Ä–∏–º–µ—Ä 8: –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è

```nim
let tokenizerRU = trainBPE(russianCorpus, vocabSize = 30000)
let tokenizerEN = trainBPE(englishCorpus, vocabSize = 30000)

let multilingualTokenizer = alignVocabularies(tokenizerRU, tokenizerEN)

let mixedText = "Hello! –ü—Ä–∏–≤–µ—Ç! How are you?"
let tokens = multilingualTokenizer.tokenize(mixedText)
echo "–¢–æ–∫–µ–Ω—ã: ", tokens
echo "–Ø–∑—ã–∫: ", detectLanguage(mixedText)
```

---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ (BPE, WordPiece, SentencePiece, Byte-Level BPE). –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:

- **–û–±—É—á–µ–Ω–∏–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ** —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
- **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞** –º–æ–¥–µ–ª–µ–π —Å –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º
- **–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞** –∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è
- **–ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞** –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
- **–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è** —Ç–µ–∫—Å—Ç–∞
- **–ú–µ—Ç—Ä–∏–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑** –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
- **–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞** –∏ –¥–µ—Ç–µ–∫—Ü–∏—è —è–∑—ã–∫–æ–≤
- **–°—É–±—Å–ª–æ–≤–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è** –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
- **–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ** –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã
- **–ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å** –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –º–∏–ª–ª–∏–æ–Ω—ã —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞—Ö.

