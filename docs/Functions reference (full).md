# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ TOKENIZATION - –ü–æ–¥—Ä–æ–±–Ω—ã–π —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ API

**–í–µ—Ä—Å–∏—è:** 0.7  
**–î–∞—Ç–∞:** 2026-02-01  
**–ê–≤—Ç–æ—Ä:** github.com/Balans097  
**–Ø–∑—ã–∫:** Nim

---

## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

- [1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤](#1-–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è-–∏-–æ–±—É—á–µ–Ω–∏–µ-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤)
- [2. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ](#2-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è-–∏-–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ)
- [3. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ](#3-–¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ)
- [4. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞](#4-–ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞-–∏-–æ—á–∏—Å—Ç–∫–∞-—Ç–µ–∫—Å—Ç–∞)
- [5. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–µ–º](#5-—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ-—Å–ª–æ–≤–∞—Ä–µ–º)
- [6. –ú–µ—Ç—Ä–∏–∫–∏, –∞–Ω–∞–ª–∏–∑ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞](#6-–º–µ—Ç—Ä–∏–∫–∏-–∞–Ω–∞–ª–∏–∑-–∏-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞)
- [7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ, –∑–∞–≥—Ä—É–∑–∫–∞ –∏ —ç–∫—Å–ø–æ—Ä—Ç](#7-—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ-–∑–∞–≥—Ä—É–∑–∫–∞-–∏-—ç–∫—Å–ø–æ—Ä—Ç)
- [8. –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞](#8-–ø–∞–∫–µ—Ç–Ω–∞—è-–æ–±—Ä–∞–±–æ—Ç–∫–∞)
- [9. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è](#9-–∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ-–∏-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
- [10. –ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å](#10-–ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å)
- [11. –û—Ç–ª–∞–¥–∫–∞ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è](#11-–æ—Ç–ª–∞–¥–∫–∞-–∏-–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è)
- [12. –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏](#12-—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ-–∏-–≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ-—Ñ—É–Ω–∫—Ü–∏–∏)

---

# 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤

## trainBPE

```nim
proc trainBPE*(corpus: seq[string], vocabSize: int = 10000, minFrequency: int = 2): Tokenizer
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–û–±—É—á–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ Byte Pair Encoding (BPE). BPE –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ –ø–∞—Ä—ã —Å–∏–º–≤–æ–ª–æ–≤/—Ç–æ–∫–µ–Ω–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ, —Å–æ–∑–¥–∞–≤–∞—è —Å–ª–æ–≤–∞—Ä—å –ø–æ–¥—Å–ª–æ–≤.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `corpus` | `seq[string]` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –ö–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 1000 —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. |
| `vocabSize` | `int` | **–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10000**. –ñ–µ–ª–∞–µ–º—ã–π —Ä–∞–∑–º–µ—Ä –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è. –ê–ª–≥–æ—Ä–∏—Ç–º –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å —Å–ª–∏—è–Ω–∏—è (merges) –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —ç—Ç–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –¢–∏–ø–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: 5000-50000. –ë–æ–ª—å—à–∏–π —Å–ª–æ–≤–∞—Ä—å = –ª—É—á—à–µ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ, –Ω–æ –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏. |
| `minFrequency` | `int` | **–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: 2**. –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ –ø–∞—Ä—ã —Å–∏–º–≤–æ–ª–æ–≤/—Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –µ—ë –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è. –ü–∞—Ä—ã —Å —á–∞—Å—Ç–æ—Ç–æ–π –Ω–∏–∂–µ —ç—Ç–æ–≥–æ –ø–æ—Ä–æ–≥–∞ –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ, –Ω–æ –º–æ–∂–µ—Ç —Å–Ω–∏–∑–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `Tokenizer`

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–±—É—á–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å —Ç–∏–ø–æ–º `tkBPE`, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:
- `vocab`: —Å–ª–æ–≤–∞—Ä—å —Ç–æ–∫–µ–Ω–æ–≤ (—Å—Ç—Ä–æ–∫–∞ ‚Üí ID)
- `inverseVocab`: –æ–±—Ä–∞—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å (ID ‚Üí —Å—Ç—Ä–æ–∫–∞)
- `merges`: —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö BPE —Å–ª–∏—è–Ω–∏–π –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
- `specialTokens`: —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã ([PAD], [UNK], [BOS], [EOS])

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
# –ë–∞–∑–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä
let corpus = @[
  "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä",
  "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
  "BPE –∞–ª–≥–æ—Ä–∏—Ç–º —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ"
]
var tok = trainBPE(corpus, vocabSize = 5000)

# –° –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
var customTok = trainBPE(
  corpus = largeCorpus,
  vocabSize = 30000,
  minFrequency = 5  # –¢–æ–ª—å–∫–æ —á–∞—Å—Ç—ã–µ –ø–∞—Ä—ã
)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
echo "–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: ", tok.vocab.len
echo "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–∏—è–Ω–∏–π: ", tok.merges.len
```

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞

- –ù–∞—á–∏–Ω–∞–µ—Ç —Å–æ —Å–ª–æ–≤–∞—Ä—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
- –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ –ø–∞—Ä—ã
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ —Å–ª–∏—è–Ω–∏—è –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –î–ª—è —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–∞—Ö
- ‚úÖ –ö–æ–≥–¥–∞ –Ω—É–∂–µ–Ω –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ä–∞–∑–º–µ—Ä–æ–º —Å–ª–æ–≤–∞—Ä—è –∏ –∫–∞—á–µ—Å—Ç–≤–æ–º
- ‚úÖ –î–ª—è –º–æ–¥–µ–ª–µ–π —Ç–∏–ø–∞ GPT (—Ö–æ—Ç—è –¥–ª—è GPT —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è Byte-Level BPE)
- ‚ùå –ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏—Ö –∫–æ—Ä–ø—É—Å–æ–≤ (<100 —Ç–µ–∫—Å—Ç–æ–≤)

---

## trainWordPiece

```nim
proc trainWordPiece*(corpus: seq[string], 
                     vocabSize: int = 10000,
                     continuingSubwordPrefix: string = "##",
                     maxInputCharsPerWord: int = 100): Tokenizer
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–û–±—É—á–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä WordPiece - –∞–ª–≥–æ—Ä–∏—Ç–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –≤ BERT –∏ –¥—Ä—É–≥–∏—Ö transformer –º–æ–¥–µ–ª—è—Ö. WordPiece —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Å–ª–æ–≤–∞ –Ω–∞ –ø–æ–¥—Å–ª–æ–≤–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∂–∞–¥–Ω–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–∞—é—â–∏—Ö –ø–æ–¥—Å–ª–æ–≤.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `corpus` | `seq[string]` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –ö–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. WordPiece —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤, –ø–æ—ç—Ç–æ–º—É —Ç–µ–∫—Å—Ç—ã –±—É–¥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–∑–±–∏—Ç—ã –Ω–∞ —Å–ª–æ–≤–∞ –ø–æ –ø—Ä–æ–±–µ–ª–∞–º. |
| `vocabSize` | `int` | **–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10000**. –¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è. –¢–∏–ø–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è BERT: 30000-50000. |
| `continuingSubwordPrefix` | `string` | **–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: "##"**. –ü—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–æ–¥–æ–ª–∂–∞—é—â–∏—Ö –ø–æ–¥—Å–ª–æ–≤ (–Ω–µ –Ω–∞—á–∞–ª–∞ —Å–ª–æ–≤–∞). –ù–∞–ø—Ä–∏–º–µ—Ä, "–∏–≥—Ä–∞—Ç—å" ‚Üí ["–∏–≥—Ä–∞—Ç—å"], "–∏–≥—Ä–∞–ª" ‚Üí ["–∏–≥—Ä–∞", "##–ª"]. –°—Ç–∞–Ω–¥–∞—Ä—Ç BERT: "##". |
| `maxInputCharsPerWord` | `int` | **–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: 100**. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö. –°–ª–æ–≤–∞ –¥–ª–∏–Ω–Ω–µ–µ —ç—Ç–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –±—É–¥—É—Ç –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ [UNK] —Ç–æ–∫–µ–Ω. –ó–∞—â–∏—Ç–∞ –æ—Ç –∞–Ω–æ–º–∞–ª—å–Ω–æ –¥–ª–∏–Ω–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `Tokenizer`

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å —Ç–∏–ø–æ–º `tkWordPiece`, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:
- –°–ª–æ–≤–∞—Ä—å –ø–æ–¥—Å–ª–æ–≤ —Å –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏ "##" –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–π
- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∂–∞–¥–Ω–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
- –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ BERT ([PAD], [UNK], [CLS], [SEP], [MASK])

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
# BERT-style —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
var wpTok = trainWordPiece(
  corpus = bertCorpus,
  vocabSize = 30000,
  continuingSubwordPrefix = "##"
)

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
let tokens = tokenize("—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è", wpTok)
# –ú–æ–∂–µ—Ç –ø–æ–ª—É—á–∏—Ç—å—Å—è: ["—Ç–æ–∫–µ–Ω", "##–∏–∑–∞—Ü–∏—è"] –∏–ª–∏ ["—Ç–æ–∫–µ–Ω–∏–∑", "##–∞—Ü–∏—è"]

# –ö–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–µ—Ñ–∏–∫—Å
var customWP = trainWordPiece(
  corpus = corpus,
  vocabSize = 20000,
  continuingSubwordPrefix = "@@",
  maxInputCharsPerWord = 50
)
```

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞

- –†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤ (—Å–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ –ø—Ä–æ–±–µ–ª–∞–º)
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∂–∞–¥–Ω—ã–π longest-match-first –∞–ª–≥–æ—Ä–∏—Ç–º
- –ü—Ä–µ—Ñ–∏–∫—Å "##" –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞–∑–ª–∏—á–∞—Ç—å –Ω–∞—á–∞–ª–æ —Å–ª–æ–≤–∞ –æ—Ç –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è
- –û–ø—Ç–∏–º–∞–ª–µ–Ω –¥–ª—è —è–∑—ã–∫–æ–≤ —Å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–µ–π

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –î–ª—è BERT-–ø–æ–¥–æ–±–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
- ‚úÖ –ö–æ–≥–¥–∞ –≤–∞–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≥—Ä–∞–Ω–∏—Ü—ã —Å–ª–æ–≤
- ‚úÖ –î–ª—è —è–∑—ã–∫–æ–≤ —Å –±–æ–≥–∞—Ç–æ–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–µ–π
- ‚ùå –ù–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —è–∑—ã–∫–æ–≤ –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤ (–∫–∏—Ç–∞–π—Å–∫–∏–π, —è–ø–æ–Ω—Å–∫–∏–π)

---

## trainSentencePiece

```nim
proc trainSentencePiece*(corpus: seq[string], 
                          vocabSize: int = 8000,
                          characterCoverage: float = 0.9995): Tokenizer
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–û–±—É—á–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä SentencePiece - –∞–ª–≥–æ—Ä–∏—Ç–º, –Ω–µ –∑–∞–≤–∏—Å—è—â–∏–π –æ—Ç –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç unigram language model –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏. –ò–¥–µ–∞–ª–µ–Ω –¥–ª—è –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `corpus` | `seq[string]` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –ö–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–æ–≤. SentencePiece —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–ø—Ä—è–º—É—é —Å —Å—ã—Ä—ã–º —Ç–µ–∫—Å—Ç–æ–º –±–µ–∑ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–±–µ–ª–∞–º. |
| `vocabSize` | `int` | **–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: 8000**. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è. –¢–∏–ø–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: 4000-32000. T5 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 32000. |
| `characterCoverage` | `float` | **–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.9995**. –ü—Ä–æ—Ü–µ–Ω—Ç —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ –∫–æ—Ä–ø—É—Å–∞, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ–∫—Ä—ã—Ç—ã —Å–ª–æ–≤–∞—Ä–µ–º. –ó–Ω–∞—á–µ–Ω–∏–µ 0.9995 –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ 99.95% —Å–∏–º–≤–æ–ª–æ–≤ –±—É–¥—É—Ç –∏–º–µ—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ. –†–µ–¥–∫–∏–µ —Å–∏–º–≤–æ–ª—ã (0.05%) –±—É–¥—É—Ç –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ byte-fallback. –î–ª—è CJK —è–∑—ã–∫–æ–≤ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 0.9995+. –î–ª—è –ª–∞—Ç–∏–Ω–∏—Ü—ã –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 0.995. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `Tokenizer`

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å —Ç–∏–ø–æ–º `tkSentencePiece`, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:
- Unigram language model —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
- –°–ª–æ–≤–∞—Ä—å –ø–æ–¥—Å–ª–æ–≤ –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–æ–±–µ–ª–æ–≤
- –°–∏–º–≤–æ–ª ‚ñÅ (U+2581) –¥–ª—è –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
- –¢–∞–±–ª–∏—Ü—É scores –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π SentencePiece
var spTok = trainSentencePiece(
  corpus = multilingualCorpus,
  vocabSize = 16000,
  characterCoverage = 0.9995
)

# –î–ª—è –ª–∞—Ç–∏–Ω–∏—Ü—ã (–º–æ–∂–Ω–æ —Å–Ω–∏–∑–∏—Ç—å coverage)
var latinSP = trainSentencePiece(
  corpus = englishCorpus,
  vocabSize = 8000,
  characterCoverage = 0.995
)

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–±–µ–ª—ã –∫–∞–∫ ‚ñÅ
let tokens = tokenize("–ü—Ä–∏–≤–µ—Ç –º–∏—Ä", spTok)
# –†–µ–∑—É–ª—å—Ç–∞—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å: ["‚ñÅ–ü—Ä–∏", "–≤–µ—Ç", "‚ñÅ–º–∏—Ä"]
```

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞

- –ù–µ —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Ä–∞–±–æ—Ç–∞–µ—Ç —Å raw text)
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏–º–≤–æ–ª ‚ñÅ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
- Unigram language model –¥–ª—è –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
- –ü–æ–ª–Ω–æ—Å—Ç—å—é –æ–±—Ä–∞—Ç–∏–º–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (lossless)

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –î–ª—è –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (T5, mT5, XLM-R)
- ‚úÖ –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–∞ –æ–±—Ä–∞—Ç–∏–º–æ—Å—Ç—å (decode(encode(text)) == text)
- ‚úÖ –î–ª—è —è–∑—ã–∫–æ–≤ –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤ (CJK)
- ‚úÖ –ö–æ–≥–¥–∞ –≤–∞–∂–Ω–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
- ‚ùå –ú–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω–µ–µ BPE/WordPiece

---

## trainByteLevelBPE

```nim
proc trainByteLevelBPE*(corpus: seq[string], 
                         vocabSize: int = 50000,
                         minFrequency: int = 2): Tokenizer
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–û–±—É—á–∞–µ—Ç Byte-Level BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –≤ GPT-2, GPT-3 –∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª—è—Ö OpenAI. –†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ –±–∞–π—Ç–æ–≤, –º–∞–ø—è –∫–∞–∂–¥—ã–π –±–∞–π—Ç –Ω–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã–π Unicode —Å–∏–º–≤–æ–ª, —á—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –ª—é–±–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ UNK —Ç–æ–∫–µ–Ω–æ–≤.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `corpus` | `seq[string]` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –ö–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. Byte-Level BPE –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ª—é–±–æ–π —Ç–µ–∫—Å—Ç, –≤–∫–ª—é—á–∞—è –Ω–µ-Unicode –∏ –±–∏–Ω–∞—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. |
| `vocabSize` | `int` | **–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: 50000**. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è. GPT-2 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 50257 —Ç–æ–∫–µ–Ω–æ–≤. –ë–æ–ª—å—à–∏–π —Å–ª–æ–≤–∞—Ä—å –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª—É—á—à–µ —Å–∂–∏–º–∞—Ç—å —Ç–µ–∫—Å—Ç. –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω: 30000-100000. |
| `minFrequency` | `int` | **–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: 2**. –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –ø–∞—Ä—ã –±–∞–π—Ç–æ–≤ –¥–ª—è —Å–ª–∏—è–Ω–∏—è. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `Tokenizer`

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å —Ç–∏–ø–æ–º `tkByteLevelBPE`, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:
- –ë–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å –∏–∑ 256 –±–∞–π—Ç–æ–≤, –∑–∞–º–∞–ø–ª–µ–Ω–Ω—ã—Ö –Ω–∞ Unicode U+0100-U+01FF
- BPE merges –ø–æ–≤–µ—Ä—Ö –±–∞–π—Ç–æ–≤
- `byteEncoder` –∏ `byteDecoder` —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
- –ì–∞—Ä–∞–Ω—Ç–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è UNK —Ç–æ–∫–µ–Ω–æ–≤ (–ª—é–±–æ–π —Ç–µ–∫—Å—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω)

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
# GPT-style —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
var gptTok = trainByteLevelBPE(
  corpus = largeWebCorpus,
  vocabSize = 50257  # –ö–∞–∫ –≤ GPT-2
)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã —Å –ª—é–±—ã–º —Ç–µ–∫—Å—Ç–æ–º
let text = "Hello –º–∏—Ä ‰Ω†Â•Ω üåç \x00\xFF"  # –ú–∏–∫—Å —è–∑—ã–∫–æ–≤ –∏ –±–∞–π—Ç–æ–≤
let tokens = tokenize(text, gptTok)
let decoded = decode(gptTok, tokens)
assert text == decoded  # –í—Å–µ–≥–¥–∞ true!

# –° –º–µ–Ω—å—à–∏–º —Å–ª–æ–≤–∞—Ä–µ–º
var smallGPT = trainByteLevelBPE(
  corpus = corpus,
  vocabSize = 30000,
  minFrequency = 5
)
```

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞

- –ö–∞–∂–¥—ã–π –±–∞–π—Ç (0-255) –º–∞–ø–∏—Ç—Å—è –Ω–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã–π Unicode —Å–∏–º–≤–æ–ª
- BPE —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ–≤–µ—Ä—Ö —ç—Ç–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
- –ü–æ–ª–Ω–æ—Å—Ç—å—é –æ–±—Ä–∞—Ç–∏–º–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
- –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç UNK —Ç–æ–∫–µ–Ω
- –ú–æ–∂–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ª—é–±—ã–µ –¥–∞–Ω–Ω—ã–µ (–Ω–µ —Ç–æ–ª—å–∫–æ UTF-8 —Ç–µ–∫—Å—Ç)

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –î–ª—è GPT-–ø–æ–¥–æ–±–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
- ‚úÖ –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–∞ –≥–∞—Ä–∞–Ω—Ç–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª—é–±–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
- ‚úÖ –î–ª—è –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
- ‚úÖ –ö–æ–≥–¥–∞ –≤–∞–∂–Ω–∞ –ø–æ–ª–Ω–∞—è –æ–±—Ä–∞—Ç–∏–º–æ—Å—Ç—å
- ‚úÖ –î–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–¥–∞ (–º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ª—é–±—ã–µ –±–∞–π—Ç—ã)
- ‚ùå –¢—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏ —á–µ–º –æ–±—ã—á–Ω—ã–π BPE

---

## createCharacterTokenizer

```nim
proc createCharacterTokenizer*(corpus: seq[string]): Tokenizer
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ—Å—Ç–æ–π —Å–∏–º–≤–æ–ª—å–Ω—ã–π (character-level) —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –≥–¥–µ –∫–∞–∂–¥—ã–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Å–∏–º–≤–æ–ª –∏–∑ –∫–æ—Ä–ø—É—Å–∞ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω—ã–º —Ç–æ–∫–µ–Ω–æ–º.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `corpus` | `seq[string]` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –ö–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤. –í—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏–∑ –∫–æ—Ä–ø—É—Å–∞ –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ —Å–ª–æ–≤–∞—Ä—å. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `Tokenizer`

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:
- –°–ª–æ–≤–∞—Ä—å —Å–æ –≤—Å–µ–º–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏ –∏–∑ –∫–æ—Ä–ø—É—Å–∞
- –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã ([PAD], [UNK], [BOS], [EOS])
- –ë–∞–∑–æ–≤—ã–µ ASCII —Å–∏–º–≤–æ–ª—ã (32-126)

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
let corpus = @["abc", "–∞–±–≤", "123"]
var charTok = createCharacterTokenizer(corpus)

# –ö–∞–∂–¥—ã–π —Å–∏–º–≤–æ–ª = 1 —Ç–æ–∫–µ–Ω
let text = "a1–±"
let tokens = tokenize(text, charTok)
# –†–µ–∑—É–ª—å—Ç–∞—Ç: [ID('a'), ID('1'), ID('–±')]
echo tokens.len  # 3 (–±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤)
```

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –î–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –∞–ª—Ñ–∞–≤–∏—Ç–æ–≤
- ‚úÖ –î–ª—è –∑–∞–¥–∞—á character-level —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è
- ‚úÖ –î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –Ω—É–ª—è –Ω–∞ –Ω–æ–≤–æ–º —è–∑—ã–∫–µ
- ‚ùå –ù–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ (–æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)

---

## createWhitespaceTokenizer

```nim
proc createWhitespaceTokenizer*(): Tokenizer
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ—Å—Ç–µ–π—à–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, —Ä–∞–∑–±–∏–≤–∞—é—â–∏–π —Ç–µ–∫—Å—Ç –ø–æ –ø—Ä–æ–±–µ–ª—å–Ω—ã–º —Å–∏–º–≤–æ–ª–∞–º (–ø—Ä–æ–±–µ–ª—ã, —Ç–∞–±—É–ª—è—Ü–∏—è, –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫).

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

–ù–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `Tokenizer`

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å –±–∞–∑–æ–≤—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏. –°–ª–æ–≤–∞—Ä—å –±—É–¥–µ—Ç —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø—Ä–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
var wsTok = createWhitespaceTokenizer()

let text = "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä\n–ù–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞"
let words = splitIntoWords(text)  # ["–ü—Ä–∏–≤–µ—Ç", "–º–∏—Ä", "–ù–æ–≤–∞—è", "—Å—Ç—Ä–æ–∫–∞"]
# –ö–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –ø–æ–ª—É—á–∏—Ç —Å–≤–æ–π ID
```

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è
- ‚úÖ –ö–æ–≥–¥–∞ –Ω–µ –Ω—É–∂–Ω–∞ subword —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
- ‚úÖ –î–ª—è —è–∑—ã–∫–æ–≤ —Å —á–µ—Ç–∫–∏–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ —Å–ª–æ–≤
- ‚ùå –ë–æ–ª—å—à–æ–π —Å–ª–æ–≤–∞—Ä—å (–∫–∞–∂–¥–æ–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ = —Ç–æ–∫–µ–Ω)
- ‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å OOV (out-of-vocabulary) —Å–ª–æ–≤–∞–º–∏

---

## createRegexTokenizer

```nim
proc createRegexTokenizer*(pattern: string): Tokenizer
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–°–æ–∑–¥–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –≥–∏–±–∫–æ–π –∫–∞—Å—Ç–æ–º–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `pattern` | `string` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤. –ü–∞—Ç—Ç–µ—Ä–Ω –¥–æ–ª–∂–µ–Ω —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Ç–æ–º—É, —á—Ç–æ —Å—á–∏—Ç–∞–µ—Ç—Å—è —Ç–æ–∫–µ–Ω–æ–º. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `Tokenizer`

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —É–∫–∞–∑–∞–Ω–Ω–æ–µ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞.

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
# –¢–æ–ª—å–∫–æ —Å–ª–æ–≤–∞ (–±—É–∫–≤—ã –∏ —Ü–∏—Ñ—Ä—ã)
var wordTok = createRegexTokenizer(r"\w+")

# –°–ª–æ–≤–∞ –∏ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ
var punctTok = createRegexTokenizer(r"\w+|[^\w\s]")

# Email –∞–¥—Ä–µ—Å–∞
var emailTok = createRegexTokenizer(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}")
```

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –î–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á (email, URL, —Ç–µ–ª–µ—Ñ–æ–Ω—ã)
- ‚úÖ –ö–æ–≥–¥–∞ –Ω—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π
- ‚úÖ –î–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–µ—Ä–µ–¥ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π

---

## fromPretrainedModel

```nim
proc fromPretrainedModel*(modelName: string): Tokenizer
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –±–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ GPT-2 –∏ BERT.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `modelName` | `string` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: "gpt2", "gpt-2" (Byte-Level BPE), "bert", "bert-base" (WordPiece). –†–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `Tokenizer`

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:
- **GPT-2**: Byte-Level BPE, —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω `<|endoftext|>`
- **BERT**: WordPiece —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º "##", —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã [CLS], [SEP], [MASK]

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
# GPT-2 style
var gpt = fromPretrainedModel("gpt2")
echo gpt.kind  # tkByteLevelBPE
echo gpt.specialTokens.eosToken  # "<|endoftext|>"

# BERT style
var bert = fromPretrainedModel("bert")
echo bert.kind  # tkWordPiece
echo bert.continuingSubwordPrefix  # "##"
echo bert.specialTokens.clsToken  # "[CLS]"

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
try:
  var unknown = fromPretrainedModel("unknown-model")
except ValidationError as e:
  echo "–û—à–∏–±–∫–∞: ", e.msg  # "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: unknown-model"
```

### –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏

| –ú–æ–¥–µ–ª—å | –¢–∏–ø | –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã |
|--------|-----|-------------------|
| gpt2, gpt-2 | Byte-Level BPE | `<|endoftext|>` |
| bert, bert-base | WordPiece | [PAD], [UNK], [CLS], [SEP], [MASK] |

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞ —Å –∏–∑–≤–µ—Å—Ç–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
- ‚úÖ –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
- ‚úÖ –ö–∞–∫ –±–∞–∑—É –¥–ª—è fine-tuning
- ‚ùå –ù–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–µ–∞–ª—å–Ω—ã—Ö –æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤ (—Ç–æ–ª—å–∫–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞)

---

## initSpecialTokens

```nim
proc initSpecialTokens*(padToken: string = "[PAD]",
                        unkToken: string = "[UNK]",
                        bosToken: string = "[BOS]",
                        eosToken: string = "[EOS]",
                        sepToken: string = "[SEP]",
                        clsToken: string = "[CLS]",
                        maskToken: string = "[MASK]"): SpecialTokens
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|--------------|----------|
| `padToken` | `string` | `"[PAD]"` | –¢–æ–∫–µ–Ω –¥–ª—è padding (–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–æ –æ–¥–Ω–æ–π –¥–ª–∏–Ω—ã). –û–±—ã—á–Ω–æ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å—é —á–µ—Ä–µ–∑ attention mask. |
| `unkToken` | `string` | `"[UNK]"` | –¢–æ–∫–µ–Ω –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤ (UNK = unknown). –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–≥–¥–∞ —Å–ª–æ–≤–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ. |
| `bosToken` | `string` | `"[BOS]"` | –¢–æ–∫–µ–Ω –Ω–∞—á–∞–ª–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (BOS = beginning of sequence). –î–æ–±–∞–≤–ª—è–µ—Ç—Å—è –≤ –Ω–∞—á–∞–ª–æ —Ç–µ–∫—Å—Ç–∞. |
| `eosToken` | `string` | `"[EOS]"` | –¢–æ–∫–µ–Ω –∫–æ–Ω—Ü–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (EOS = end of sequence). –î–æ–±–∞–≤–ª—è–µ—Ç—Å—è –≤ –∫–æ–Ω–µ—Ü —Ç–µ–∫—Å—Ç–∞. |
| `sepToken` | `string` | `"[SEP]"` | –¢–æ–∫–µ–Ω —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è (SEP = separator). –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤–æ–ø—Ä–æ—Å –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ BERT). |
| `clsToken` | `string` | `"[CLS]"` | –¢–æ–∫–µ–Ω –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (CLS = classification). –í BERT –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –≤ –Ω–∞—á–∞–ª–æ –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. |
| `maskToken` | `string` | `"[MASK]"` | –¢–æ–∫–µ–Ω –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è Masked Language Modeling (MLM). –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ BERT-style –æ–±—É—á–µ–Ω–∏–∏. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `SpecialTokens`

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏, –∫–æ—Ç–æ—Ä–∞—è –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–∏—Å–≤–æ–µ–Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É.

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
# BERT-style —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
let bertTokens = initSpecialTokens(
  padToken = "[PAD]",
  unkToken = "[UNK]",
  bosToken = "[CLS]",  # BERT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç [CLS] –∫–∞–∫ BOS
  eosToken = "[SEP]",  # BERT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç [SEP] –∫–∞–∫ EOS
  sepToken = "[SEP]",
  clsToken = "[CLS]",
  maskToken = "[MASK]"
)

# GPT-style (–æ–¥–∏–Ω —Ç–æ–∫–µ–Ω –¥–ª—è –≤—Å–µ–≥–æ)
let gptTokens = initSpecialTokens(
  padToken = "<|endoftext|>",
  unkToken = "<|endoftext|>",
  bosToken = "<|endoftext|>",
  eosToken = "<|endoftext|>"
)

# –ö–∞—Å—Ç–æ–º–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
let customTokens = initSpecialTokens(
  padToken = "<PAD>",
  unkToken = "<UNK>",
  bosToken = "<START>",
  eosToken = "<END>"
)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º
var tok = trainBPE(corpus)
tok.specialTokens = customTokens
addSpecialTokensToVocab(tok)
```

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

- –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã (–∏–º–µ—é—Ç –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
- –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ü–µ–ª–µ–π
- –¢–æ–∫–µ–Ω—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –æ–±—ã—á–Ω—ã—Ö —Å–ª–æ–≤
- –ü–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ —Å–ª–æ–≤–∞—Ä—å —á–µ—Ä–µ–∑ `addSpecialTokensToVocab`

---

## addSpecialTokensToVocab

```nim
proc addSpecialTokensToVocab*(tokenizer: var Tokenizer)
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–î–æ–±–∞–≤–ª—è–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∏–∑ `tokenizer.specialTokens` –≤ —Å–ª–æ–≤–∞—Ä—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ —Ç–∞–±–ª–∏—Ü—É `specialTokenIds`. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –ø–æ–ª—É—á–∞—é—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∏–∑–∫–∏–µ ID (–æ–±—ã—á–Ω–æ 0-6).

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `tokenizer` | `var Tokenizer` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π** (–∏–∑–º–µ–Ω—è–µ–º–∞—è —Å—Å—ã–ª–∫–∞). –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –≤ –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã. –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç—Å—è in-place. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

–ù–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è (–ø—Ä–æ—Ü–µ–¥—É—Ä–∞ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞–ø—Ä—è–º—É—é).

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
var tok = trainBPE(corpus)

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
tok.specialTokens = initSpecialTokens(
  padToken = "<PAD>",
  unkToken = "<UNK>"
)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ä—å
addSpecialTokensToVocab(tok)

# –ü—Ä–æ–≤–µ—Ä–∫–∞
echo tok.vocab["<PAD>"]  # 0
echo tok.vocab["<UNK>"]  # 1
echo tok.specialTokenIds["<PAD>"]  # 0

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
let tokens = tokenize("—Ç–µ–∫—Å—Ç", tok, addSpecialTokens = true)
# –†–µ–∑—É–ª—å—Ç–∞—Ç: [BOS_ID, ...—Ç–æ–∫–µ–Ω—ã —Ç–µ–∫—Å—Ç–∞..., EOS_ID]
```

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

- –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –ø–æ–ª—É—á–∞—é—Ç ID –≤ –Ω–∞—á–∞–ª–µ —Å–ª–æ–≤–∞—Ä—è (0, 1, 2, ...)
- –ï—Å–ª–∏ —Ç–æ–∫–µ–Ω —É–∂–µ –µ—Å—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ, –µ–≥–æ ID –Ω–µ –∏–∑–º–µ–Ω–∏—Ç—Å—è
- –û–±–Ω–æ–≤–ª—è–µ—Ç –∫–∞–∫ `vocab`, —Ç–∞–∫ –∏ `specialTokenIds` —Ç–∞–±–ª–∏—Ü—ã
- –û–±—ã—á–Ω–æ –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏, –Ω–æ –º–æ–∂–µ—Ç –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞

---

# 2. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ

## tokenize

```nim
proc tokenize*(text: string, 
               tokenizer: var Tokenizer,
               flag: int = 0,
               addSpecialTokens: bool = true): seq[int]
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö ID —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `text` | `string` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –¢–µ–∫—Å—Ç –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. –ú–æ–∂–µ—Ç –±—ã—Ç—å –ª—é–±–æ–π –¥–ª–∏–Ω—ã (—Å —É—á–µ—Ç–æ–º –ª–∏–º–∏—Ç–∞ `MAX_INPUT_LENGTH` = 1M —Å–∏–º–≤–æ–ª–æ–≤). –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Unicode. |
| `tokenizer` | `var Tokenizer` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π** (–∏–∑–º–µ–Ω—è–µ–º–∞—è —Å—Å—ã–ª–∫–∞). –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –ú–æ–∂–µ—Ç –±—ã—Ç—å –∏–∑–º–µ–Ω–µ–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ–±–Ω–æ–≤–ª–µ–Ω –∫—ç—à). |
| `flag` | `int` | **–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0**. –§–ª–∞–≥–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è. –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –±—É–¥—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –í —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è. |
| `addSpecialTokens` | `bool` | **–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: true**. –ï—Å–ª–∏ `true`, –¥–æ–±–∞–≤–ª—è–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã BOS (beginning of sequence) –≤ –Ω–∞—á–∞–ª–æ –∏ EOS (end of sequence) –≤ –∫–æ–Ω–µ—Ü –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ï—Å–ª–∏ `false`, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω—ã —Ç–µ–∫—Å—Ç–∞. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `seq[int]`

–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö ID —Ç–æ–∫–µ–Ω–æ–≤. –ö–∞–∂–¥–æ–µ —á–∏—Å–ª–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç–æ–∫–µ–Ω—É –≤ —Å–ª–æ–≤–∞—Ä–µ:
- –ï—Å–ª–∏ `addSpecialTokens = true`: `[BOS_ID, token1_ID, token2_ID, ..., EOS_ID]`
- –ï—Å–ª–∏ `addSpecialTokens = false`: `[token1_ID, token2_ID, ...]`
- –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∑–∞–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ UNK_ID

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
var tok = trainBPE(corpus)

# –ë–∞–∑–æ–≤–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
let text = "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä"
let tokens = tokenize(text, tok)
echo tokens  # –ù–∞–ø—Ä–∏–º–µ—Ä: [2, 145, 67, 890, 3] (–≥–¥–µ 2=BOS, 3=EOS)

# –ë–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
let tokensNoSpecial = tokenize(text, tok, addSpecialTokens = false)
echo tokensNoSpecial  # –ù–∞–ø—Ä–∏–º–µ—Ä: [145, 67, 890]

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã
echo "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: ", tokens.len

# –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ (–ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º –≤—ã–∑–æ–≤–µ –±—É–¥–µ—Ç –±—ã—Å—Ç—Ä–µ–µ)
let tokens2 = tokenize(text, tok)  # –í–æ–∑—å–º–µ—Ç –∏–∑ –∫—ç—à–∞
```

### –ü–æ–≤–µ–¥–µ–Ω–∏–µ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤

**BPE:**
- –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ –ø—Ä–æ–±–µ–ª–∞–º
- –ü—Ä–∏–º–µ–Ω—è–µ—Ç BPE merges –∫ –∫–∞–∂–¥–æ–º—É —Å–ª–æ–≤—É
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–≥–∏—Å—Ç—Ä (–µ—Å–ª–∏ `preserveCase = true`)

**WordPiece:**
- –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ –ø—Ä–æ–±–µ–ª–∞–º
- –ü—Ä–∏–º–µ–Ω—è–µ—Ç –∂–∞–¥–Ω—ã–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π matching
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å "##" –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–π

**SentencePiece:**
- –†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–ø—Ä—è–º—É—é —Å —Ç–µ–∫—Å—Ç–æ–º (–Ω–µ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –ø–æ –ø—Ä–æ–±–µ–ª–∞–º)
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç unigram language model
- –ö–æ–¥–∏—Ä—É–µ—Ç –ø—Ä–æ–±–µ–ª—ã –∫–∞–∫ ‚ñÅ

**Byte-Level BPE:**
- –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ –±–∞–π—Ç—ã
- –ú–∞–ø–∏—Ç –±–∞–π—Ç—ã –Ω–∞ Unicode —Å–∏–º–≤–æ–ª—ã
- –ü—Ä–∏–º–µ–Ω—è–µ—Ç BPE –∫ –∑–∞–º–∞–ø–ª–µ–Ω–Ω—ã–º —Å–∏–º–≤–æ–ª–∞–º

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫—ç—à –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–π
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç Unicode
- –ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞ (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `tokenizeThreadSafe` –¥–ª—è –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏)
- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞: 1,000,000 —Å–∏–º–≤–æ–ª–æ–≤

---

## tokenizeWithOffsets

```nim
proc tokenizeWithOffsets*(text: string,
                           tokenizer: var Tokenizer,
                           addSpecialTokens: bool = true): seq[TokenOffset]
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ–∑–∏—Ü–∏—è—Ö –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞ –¥–ª—è –∑–∞–¥–∞—á Named Entity Recognition (NER) –∏ Question Answering (QA), –≥–¥–µ –Ω—É–∂–Ω–æ –∑–Ω–∞—Ç—å —Ç–æ—á–Ω–æ–µ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `text` | `string` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –¢–µ–∫—Å—Ç –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. |
| `tokenizer` | `var Tokenizer` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. |
| `addSpecialTokens` | `bool` | **–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: true**. –î–æ–±–∞–≤–ª—è—Ç—å –ª–∏ BOS/EOS —Ç–æ–∫–µ–Ω—ã. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –ø–æ–ª—É—á–∞—é—Ç –ø–æ–∑–∏—Ü–∏–∏ -1 (–Ω–µ –ø—Ä–∏–≤—è–∑–∞–Ω—ã –∫ —Ç–µ–∫—Å—Ç—É). |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `seq[TokenOffset]`

–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä `TokenOffset`, –≥–¥–µ –∫–∞–∂–¥–∞—è —Å–æ–¥–µ—Ä–∂–∏—Ç:

```nim
type TokenOffset* = object
  token*: string        # –°—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞
  tokenId*: int         # ID —Ç–æ–∫–µ–Ω–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ
  startChar*: int       # –ü–æ–∑–∏—Ü–∏—è –Ω–∞—á–∞–ª–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö (Unicode runes)
  endChar*: int         # –ü–æ–∑–∏—Ü–∏—è –∫–æ–Ω—Ü–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö (—ç–∫—Å–∫–ª—é–∑–∏–≤–Ω–∞—è)
  startByte*: int       # –ü–æ–∑–∏—Ü–∏—è –Ω–∞—á–∞–ª–∞ –≤ –±–∞–π—Ç–∞—Ö (UTF-8)
  endByte*: int         # –ü–æ–∑–∏—Ü–∏—è –∫–æ–Ω—Ü–∞ –≤ –±–∞–π—Ç–∞—Ö (—ç–∫—Å–∫–ª—é–∑–∏–≤–Ω–∞—è)
```

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
var tok = trainBPE(corpus)
let text = "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä"

let offsets = tokenizeWithOffsets(text, tok, addSpecialTokens = false)

for offset in offsets:
  echo "–¢–æ–∫–µ–Ω: '", offset.token, "'"
  echo "  ID: ", offset.tokenId
  echo "  –°–∏–º–≤–æ–ª—ã: [", offset.startChar, ":", offset.endChar, "]"
  echo "  –ë–∞–π—Ç—ã: [", offset.startByte, ":", offset.endByte, "]"
  echo "  –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ: '", text[offset.startByte..<offset.endByte], "'"
  echo ""

# –ü—Ä–∏–º–µ—Ä –≤—ã–≤–æ–¥–∞:
# –¢–æ–∫–µ–Ω: '–ü—Ä–∏'
#   ID: 145
#   –°–∏–º–≤–æ–ª—ã: [0:3]
#   –ë–∞–π—Ç—ã: [0:6]
#   –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ: '–ü—Ä–∏'
# 
# –¢–æ–∫–µ–Ω: '–≤–µ—Ç'
#   ID: 67
#   –°–∏–º–≤–æ–ª—ã: [3:6]
#   –ë–∞–π—Ç—ã: [6:12]
#   –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ: '–≤–µ—Ç'
```

### –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ NER

```nim
# –ü–æ–∏—Å–∫ —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
let text = "–ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤ –∂–∏–≤–µ—Ç –≤ –ú–æ—Å–∫–≤–µ"
let offsets = tokenizeWithOffsets(text, tok)

# –ú–æ–¥–µ–ª—å NER –≤–µ—Ä–Ω—É–ª–∞, —á—Ç–æ —Ç–æ–∫–µ–Ω—ã [0:2] - —ç—Ç–æ PERSON
let personTokens = offsets[0..1]
let personStart = personTokens[0].startChar
let personEnd = personTokens[^1].endChar
let personName = text.runeSubStr(personStart, personEnd - personStart)
echo "–ù–∞–π–¥–µ–Ω–∞ –ø–µ—Ä—Å–æ–Ω–∞: ", personName  # "–ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤"
```

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

- **startChar/endChar**: –ø–æ–∑–∏—Ü–∏–∏ –≤ Unicode —Å–∏–º–≤–æ–ª–∞—Ö (runes), –ø–æ–ª–µ–∑–Ω—ã –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–∏–º–≤–æ–ª–æ–≤
- **startByte/endByte**: –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –±–∞–π—Ç–æ–≤—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –≤ UTF-8, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–æ–¥—Å—Ç—Ä–æ–∫
- –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (BOS/EOS) –∏–º–µ—é—Ç –ø–æ–∑–∏—Ü–∏–∏ -1 (–Ω–µ –ø—Ä–∏–≤—è–∑–∞–Ω—ã –∫ —Ç–µ–∫—Å—Ç—É)
- –î–ª—è Byte-Level BPE –±–∞–π—Ç–æ–≤—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –º–æ–≥—É—Ç –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –Ω–∞–ø—Ä—è–º—É—é —Å–∏–º–≤–æ–ª–∞–º
- –í–∞–∂–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–∞–π—Ç–æ–≤—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–æ–¥—Å—Ç—Ä–æ–∫ (`text[startByte..<endByte]`)

---

## streamTokenize

```nim
iterator streamTokenize*(filePath: string,
                          tokenizer: var Tokenizer,
                          chunkSize: int = 8192,
                          addSpecialTokens: bool = true): seq[int]
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–ò—Ç–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤. –ß–∏—Ç–∞–µ—Ç —Ñ–∞–π–ª —á–∞–Ω–∫–∞–º–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç –∏—Ö –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ, –º–∏–Ω–∏–º–∏–∑–∏—Ä—É—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏. –ò–¥–µ–∞–ª–µ–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ —Ä–∞–∑–º–µ—Ä–æ–º –≤ –≥–∏–≥–∞–±–∞–π—Ç—ã.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `filePath` | `string` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. –§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤–æ–≤–∞—Ç—å –∏ –±—ã—Ç—å —á–∏—Ç–∞–µ–º—ã–º. |
| `tokenizer` | `var Tokenizer` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. |
| `chunkSize` | `int` | **–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: 8192**. –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è –≤ –±–∞–π—Ç–∞—Ö. –ë–æ–ª—å—à–∏–π —Ä–∞–∑–º–µ—Ä = –º–µ–Ω—å—à–µ I/O –æ–ø–µ—Ä–∞—Ü–∏–π, –Ω–æ –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω: 4096-65536. |
| `addSpecialTokens` | `bool` | **–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: true**. –ï—Å–ª–∏ `true`, –¥–æ–±–∞–≤–ª—è–µ—Ç BOS —Ç–æ–∫–µ–Ω –ø–µ—Ä–µ–¥ –ø–µ—Ä–≤—ã–º —á–∞–Ω–∫–æ–º –∏ EOS –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** –∏—Ç–µ—Ä–∞—Ç–æ—Ä `seq[int]`

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É —á–∞–Ω–∫—É –∑–∞ —Ä–∞–∑ —á–µ—Ä–µ–∑ `yield`. –ö–∞–∂–¥—ã–π –≤—ã–∑–æ–≤ `yield` –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —á–∞–Ω–∫–∞.

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
var tok = trainBPE(corpus)

# –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
var totalTokens = 0
for tokenChunk in streamTokenize("large_file.txt", tok):
  totalTokens += tokenChunk.len
  # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–ø–∏—Å—å –≤ —Ñ–∞–π–ª –∏–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏)
  processChunk(tokenChunk)

echo "–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: ", totalTokens

# –° –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º —Ä–∞–∑–º–µ—Ä–æ–º —á–∞–Ω–∫–∞
for tokenChunk in streamTokenize("huge_file.txt", tok, chunkSize = 32768):
  saveToDatabase(tokenChunk)

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
for tokenChunk in streamTokenize("file.txt", tok, addSpecialTokens = false):
  # –ö–∞–∂–¥—ã–π —á–∞–Ω–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω—ã —Ç–µ–∫—Å—Ç–∞
  echo "–ß–∞–Ω–∫ –∏–∑ ", tokenChunk.len, " —Ç–æ–∫–µ–Ω–æ–≤"
```

### –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø—Ä–∏–º–µ—Ä: –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

```nim
# –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
proc trainModelOnLargeFile(modelPath: string, filePath: string) =
  var tok = loadTokenizer("tokenizer.json")
  var model = loadModel(modelPath)
  
  var batchTokens: seq[seq[int]] = @[]
  const BATCH_SIZE = 32
  
  for tokenChunk in streamTokenize(filePath, tok, chunkSize = 16384):
    batchTokens.add(tokenChunk)
    
    if batchTokens.len >= BATCH_SIZE:
      # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –±–∞—Ç—á–µ
      model.trainBatch(batchTokens)
      batchTokens = @[]
      
  # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Ç–æ–∫–µ–Ω–æ–≤
  if batchTokens.len > 0:
    model.trainBatch(batchTokens)
```

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

- –ß–∏—Ç–∞–µ—Ç —Ñ–∞–π–ª –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –Ω–µ –∑–∞–≥—Ä—É–∂–∞—è –≤ –ø–∞–º—è—Ç—å —Ü–µ–ª–∏–∫–æ–º
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã —Å–ª–æ–≤ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç overflow –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
- –ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Ñ–∞–π–ª –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –∏—Ç–µ—Ä–∞—Ü–∏–∏
- BOS –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∫ –ø–µ—Ä–≤–æ–º—É —á–∞–Ω–∫—É, EOS - –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É
- –ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞

### –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

```nim
try:
  for chunk in streamTokenize("file.txt", tok):
    processChunk(chunk)
except IOError as e:
  echo "–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: ", e.msg
except:
  echo "–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞"
```

---

## tokenizeWithDropout

```nim
proc tokenizeWithDropout*(text: string,
                           tokenizer: var Tokenizer,
                           dropoutProb: float = 0.1,
                           seed: int = -1,
                           minDropped: int = 1): seq[int]
```

### –û–ø–∏—Å–∞–Ω–∏–µ
BPE-dropout (subword regularization) –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏. –°–ª—É—á–∞–π–Ω–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ BPE merge –æ–ø–µ—Ä–∞—Ü–∏–∏, —Å–æ–∑–¥–∞–≤–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ —Ç–µ–∫—Å—Ç–∞. –£–ª—É—á—à–∞–µ—Ç —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `text` | `string` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –¢–µ–∫—Å—Ç –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. |
| `tokenizer` | `var Tokenizer` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. **–¢–æ–ª—å–∫–æ BPE –∏–ª–∏ Byte-Level BPE**. –î–ª—è –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ã—á–Ω—É—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é. |
| `dropoutProb` | `float` | **–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.1**. –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–æ–ø—É—Å–∫–∞ –∫–∞–∂–¥–æ–π merge –æ–ø–µ—Ä–∞—Ü–∏–∏. –î–∏–∞–ø–∞–∑–æ–Ω: 0.0-1.0. –¢–∏–ø–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: 0.05-0.3. –ë–æ–ª—å—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ = –±–æ–ª—å—à–µ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏, –Ω–æ –º–µ–Ω–µ–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è. |
| `seed` | `int` | **–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: -1**. Random seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏. –ï—Å–ª–∏ -1, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π seed (—Ä–∞–∑–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ –∫–∞–∂–¥–æ–º –≤—ã–∑–æ–≤–µ). |
| `minDropped` | `int` | **–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1**. –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ merge –æ–ø–µ—Ä–∞—Ü–∏–π –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞. –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±—É–¥–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `seq[int]`

–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–º dropout. –î–ª–∏–Ω–∞ –æ–±—ã—á–Ω–æ –±–æ–ª—å—à–µ —á–µ–º –ø—Ä–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (—Ç–∞–∫ –∫–∞–∫ –ø—Ä–æ–ø—É—Å–∫ merges –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –±–æ–ª—å—à–µ–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏).

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
var bpeTok = trainBPE(corpus)
let text = "—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"

# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (–±–µ–∑ dropout)
let standard = tokenize(text, bpeTok, addSpecialTokens = false)
echo "–°—Ç–∞–Ω–¥–∞—Ä—Ç: ", standard  # –ù–∞–ø—Ä–∏–º–µ—Ä: [123, 456]

# –° dropout - —Ä–∞–∑–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
let drop1 = tokenizeWithDropout(text, bpeTok, dropoutProb = 0.2)
let drop2 = tokenizeWithDropout(text, bpeTok, dropoutProb = 0.2)
let drop3 = tokenizeWithDropout(text, bpeTok, dropoutProb = 0.2)

echo "Dropout 1: ", drop1  # –ù–∞–ø—Ä–∏–º–µ—Ä: [123, 78, 90]
echo "Dropout 2: ", drop2  # –ù–∞–ø—Ä–∏–º–µ—Ä: [12, 456]
echo "Dropout 3: ", drop3  # –ù–∞–ø—Ä–∏–º–µ—Ä: [12, 34, 56, 78]
# –í—Å–µ —Ä–∞–∑–Ω—ã–µ –∏–∑-–∑–∞ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏!

# –° —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º seed (–≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ)
let repro1 = tokenizeWithDropout(text, bpeTok, dropoutProb = 0.2, seed = 42)
let repro2 = tokenizeWithDropout(text, bpeTok, dropoutProb = 0.2, seed = 42)
assert repro1 == repro2  # –í—Å–µ–≥–¥–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Å –æ–¥–Ω–∏–º seed

# –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π dropout
let aggressive = tokenizeWithDropout(text, bpeTok, dropoutProb = 0.5)
echo "–ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π: ", aggressive  # –ù–∞–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
```

### –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –¥–ª—è data augmentation

```nim
# –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
proc augmentTrainingData(texts: seq[string], tok: var Tokenizer, 
                          augFactor: int = 3): seq[seq[int]] =
  ## –°–æ–∑–¥–∞–µ—Ç augFactor –≤–µ—Ä—Å–∏–π –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
  result = @[]
  
  for text in texts:
    # –û—Ä–∏–≥–∏–Ω–∞–ª
    result.add(tokenize(text, tok))
    
    # Augmented –≤–µ—Ä—Å–∏–∏
    for i in 1..augFactor:
      result.add(tokenizeWithDropout(text, tok, dropoutProb = 0.15))
  
  # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º
  shuffle(result)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
let trainTexts = @["—Ç–µ–∫—Å—Ç 1", "—Ç–µ–∫—Å—Ç 2", "—Ç–µ–∫—Å—Ç 3"]
let augmentedData = augmentTrainingData(trainTexts, bpeTok, augFactor = 2)
# –ü–æ–ª—É—á–∏–º 9 –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (3 –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ + 6 augmented)
```

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞

1. –°–æ–∑–¥–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—É—é –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫—É –∏–Ω–¥–µ–∫—Å–æ–≤ –≤—Å–µ—Ö BPE merges
2. –í—ã–±–∏—Ä–∞–µ—Ç –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ merges –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è (–æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç)
3. –ü—Ä–∏–º–µ–Ω—è–µ—Ç –≤—ã–±—Ä–∞–Ω–Ω—ã–µ merges –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ (–ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É)
4. –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –º–∏–Ω–∏–º—É–º `minDropped` –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ seq2seq –º–æ–¥–µ–ª–µ–π (–º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥)
- ‚úÖ –î–ª—è data augmentation –≤ NLU –∑–∞–¥–∞—á–∞—Ö
- ‚úÖ –ß—Ç–æ–±—ã –º–æ–¥–µ–ª—å –±—ã–ª–∞ —Ä–æ–±–∞—Å—Ç–Ω–æ–π –∫ —Ä–∞–∑–Ω—ã–º —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è–º
- ‚úÖ –î–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è overfitting –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é
- ‚ùå –ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)
- ‚ùå –†–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å BPE/Byte-Level BPE

---

## tokenizeCode

```nim
proc tokenizeCode*(code: string,
                    tokenizer: var Tokenizer,
                    language: ProgrammingLanguage = plPython): seq[int]
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –∫–æ–¥–∞ —Å —É—á–µ—Ç–æ–º —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ —è–∑—ã–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –î–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –ª—É—á—à–µ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `code` | `string` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. |
| `tokenizer` | `var Tokenizer` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –ë–∞–∑–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. |
| `language` | `ProgrammingLanguage` | **–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: plPython**. –Ø–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: `plPython`, `plJavaScript`, `plJava`, `plCpp`, `plRust`, `plGo`, `plNim`. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `seq[int]`

–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ –∫–æ–¥–∞. –û–ø–µ—Ä–∞—Ç–æ—Ä—ã –∏ —Å–∫–æ–±–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ.

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
var tok = trainBPE(codeCorpus)

# Python –∫–æ–¥
let pythonCode = """
def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n-1)
"""

let tokens = tokenizeCode(pythonCode, tok, language = plPython)

# JavaScript
let jsCode = """
function add(a, b) {
    return a + b;
}
"""
let jsTokens = tokenizeCode(jsCode, tok, language = plJavaScript)

# Rust
let rustCode = """
fn main() {
    let x = 5;
    println!("x = {}", x);
}
"""
let rustTokens = tokenizeCode(rustCode, tok, language = plRust)
```

### –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —è–∑—ã–∫–∞

**Python** (`plPython`):
- –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: def, class, if, else, elif, for, while, return, import, from, etc.

**JavaScript** (`plJavaScript`):
- –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: function, const, let, var, if, else, for, while, return, etc.

**Java** (`plJava`):
- –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: public, private, class, if, else, for, while, return, etc.

**C++** (`plCpp`):
- –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: int, float, class, struct, if, else, for, while, etc.

**Rust** (`plRust`):
- –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: fn, let, mut, if, else, for, while, return, etc.

**Go** (`plGo`):
- –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: func, var, if, else, for, range, return, etc.

**Nim** (`plNim`):
- –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: proc, var, let, if, else, for, while, return, etc.

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤: `( ) { } [ ] ; , : .`
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—Å—Ç—É–ø—ã –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–æ–¥–∞
- –£–ª—É—á—à–∞–µ—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤
- –ù–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (–ø—Ä–æ—Å—Ç–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞)

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –î–ª—è –æ–±—É—á–µ–Ω–∏—è code generation –º–æ–¥–µ–ª–µ–π (CodeGPT, CodeBERT)
- ‚úÖ –ü—Ä–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∫–æ–¥–æ–≤—ã—Ö –±–∞–∑
- ‚úÖ –î–ª—è –∑–∞–¥–∞—á code completion
- ‚úÖ –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞ –∏ code search
- ‚ùå –ù–µ –∑–∞–º–µ–Ω—è–µ—Ç –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä/lexer

---

## tokenizeMath

```nim
proc tokenizeMath*(mathExpr: string,
                    tokenizer: var Tokenizer): seq[int]
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π. –†–∞–∑–¥–µ–ª—è–µ—Ç –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã, —Ñ—É–Ω–∫—Ü–∏–∏ –∏ —á–∏—Å–ª–∞ –¥–ª—è –ª—É—á—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –≤ –º–æ–¥–µ–ª—è—Ö.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `mathExpr` | `string` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. |
| `tokenizer` | `var Tokenizer` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –ë–∞–∑–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `seq[int]`

–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è.

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
var tok = trainBPE(mathCorpus)

# –ü—Ä–æ—Å—Ç–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ
let expr1 = "2 + 3 * 4"
let tokens1 = tokenizeMath(expr1, tok)

# –° —Ñ—É–Ω–∫—Ü–∏—è–º–∏
let expr2 = "sin(x) + cos(y)"
let tokens2 = tokenizeMath(expr2, tok)

# –ò–Ω—Ç–µ–≥—Ä–∞–ª
let expr3 = "‚à´(x¬≤ + 2x + 1)dx"
let tokens3 = tokenizeMath(expr3, tok)

# –°–ª–æ–∂–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ
let expr4 = "(a^2 + b^2) / sqrt(c^2 + d^2)"
let tokens4 = tokenizeMath(expr4, tok)
```

### –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã–µ –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã

- –ê—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏–µ: `+`, `-`, `*`, `/`, `^`, `=`
- –°–∫–æ–±–∫–∏: `(`, `)`, `[`, `]`
- –§—É–Ω–∫—Ü–∏–∏: `sin`, `cos`, `tan`, `log`, `ln`, `exp`, `sqrt`

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

- –î–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∏ —Å–∫–æ–±–æ–∫
- –í—ã–¥–µ–ª—è–µ—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–∏—Å–ª–∞ –∫–∞–∫ –µ–¥–∏–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Unicode –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã

### –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ

```nim
# –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á
proc processMathDataset(problems: seq[string], tok: var Tokenizer): seq[seq[int]] =
  result = @[]
  for problem in problems:
    result.add(tokenizeMath(problem, tok))

# –ü—Ä–∏–º–µ—Ä –¥–ª—è math word problems
let problems = @[
  "–ù–∞–π—Ç–∏ x: 2x + 5 = 15",
  "–í—ã—á–∏—Å–ª–∏—Ç—å: log(100) + ln(e)",
  "–†–µ—à–∏—Ç—å: sin(œÄ/2) + cos(0)"
]
let tokenizedProblems = processMathDataset(problems, tok)
```

---

## tokenizeMarkdown

```nim
proc tokenizeMarkdown*(markdown: string,
                        tokenizer: var Tokenizer): seq[int]
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è Markdown —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç Markdown —Ä–∞–∑–º–µ—Ç–∫—É –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `markdown` | `string` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –¢–µ–∫—Å—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown. |
| `tokenizer` | `var Tokenizer` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –ë–∞–∑–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `seq[int]`

–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã Markdown.

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
var tok = trainBPE(mdCorpus)

let markdown = """
# –ó–∞–≥–æ–ª–æ–≤–æ–∫ 1

## –ó–∞–≥–æ–ª–æ–≤–æ–∫ 2

–û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç —Å **–∂–∏—Ä–Ω—ã–º** –∏ *–∫—É—Ä—Å–∏–≤–æ–º*.

### –ó–∞–≥–æ–ª–æ–≤–æ–∫ 3

–¢–µ–∫—Å—Ç —Å `–∫–æ–¥–æ–º` –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–æ–∫–∏.
"""

let tokens = tokenizeMarkdown(markdown, tok)
```

### –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã

- –ó–∞–≥–æ–ª–æ–≤–∫–∏: `#` ‚Üí `[H1]`, `##` ‚Üí `[H2]`, `###` ‚Üí `[H3]`
- –ñ–∏—Ä–Ω—ã–π: `**—Ç–µ–∫—Å—Ç**` ‚Üí `[BOLD] —Ç–µ–∫—Å—Ç [BOLD]`
- –ö—É—Ä—Å–∏–≤: `*—Ç–µ–∫—Å—Ç*` ‚Üí `[ITALIC] —Ç–µ–∫—Å—Ç [ITALIC]`
- –ö–æ–¥: `` `–∫–æ–¥` `` ‚Üí `[CODE] –∫–æ–¥ [CODE]`

### –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ

```nim
# –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
proc processDocumentation(files: seq[string], tok: var Tokenizer) =
  for file in files:
    let mdContent = readFile(file)
    let tokens = tokenizeMarkdown(mdContent, tok)
    # –î–∞–ª–µ–µ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤...
```

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –î–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ README —Ñ–∞–π–ª–æ–≤
- ‚úÖ –ü—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π
- ‚úÖ –î–ª—è –º–æ–¥–µ–ª–µ–π, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å Markdown (GitHub Copilot)
- ‚úÖ –î–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- ‚ùå –ï—Å–ª–∏ –Ω—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ Markdown (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä)

---

## tokenizeJson

```nim
proc tokenizeJson*(t: var Tokenizer, json: string): seq[int]
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è JSON –¥–∞–Ω–Ω—ã—Ö. –†–∞–∑–¥–µ–ª—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã JSON –¥–ª—è –ª—É—á—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –º–æ–¥–µ–ª—è—Ö.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `t` | `var Tokenizer` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –ë–∞–∑–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. |
| `json` | `string` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. JSON —Å—Ç—Ä–æ–∫–∞ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `seq[int]`

–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
var tok = trainBPE(jsonCorpus)

# –ü—Ä–æ—Å—Ç–æ–π JSON
let json1 = """{"name": "–ò–≤–∞–Ω", "age": 30}"""
let tokens1 = tokenizeJson(tok, json1)

# –í–ª–æ–∂–µ–Ω–Ω—ã–π JSON
let json2 = """{
  "user": {
    "id": 123,
    "profile": {
      "name": "–ê–Ω–Ω–∞",
      "interests": ["–º—É–∑—ã–∫–∞", "—Å–ø–æ—Ä—Ç"]
    }
  }
}"""
let tokens2 = tokenizeJson(tok, json2)

# –ú–∞—Å—Å–∏–≤
let json3 = """[1, 2, 3, {"key": "value"}]"""
let tokens3 = tokenizeJson(tok, json3)
```

### –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã

- –§–∏–≥—É—Ä–Ω—ã–µ —Å–∫–æ–±–∫–∏: `{`, `}`
- –ö–≤–∞–¥—Ä–∞—Ç–Ω—ã–µ —Å–∫–æ–±–∫–∏: `[`, `]`
- –î–≤–æ–µ—Ç–æ—á–∏–µ: `:`
- –ó–∞–ø—è—Ç–∞—è: `,`

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

- –î–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
- –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –º–æ–¥–µ–ª–µ–π, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å API
- –ù–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—é JSON

### –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ

```nim
# –û–±—Ä–∞–±–æ—Ç–∫–∞ API –æ—Ç–≤–µ—Ç–æ–≤
proc processApiResponses(responses: seq[string], tok: var Tokenizer): seq[seq[int]] =
  result = @[]
  for response in responses:
    result.add(tokenizeJson(tok, response))

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ JSON –¥–∞–Ω–Ω—ã—Ö
let apiData = @[
  """{"status": "success", "data": {...}}""",
  """{"error": "not found", "code": 404}"""
]
let tokenizedData = processApiResponses(apiData, tok)
```

---

# 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ, –∑–∞–≥—Ä—É–∑–∫–∞ –∏ —ç–∫—Å–ø–æ—Ä—Ç

## toHuggingFaceFormat

```nim
proc toHuggingFaceFormat*(t: Tokenizer): JsonNode
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ —Ñ–æ—Ä–º–∞—Ç HuggingFace Tokenizers –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —ç–∫–æ—Å–∏—Å—Ç–µ–º–æ–π HuggingFace (Transformers, Datasets).

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `t` | `Tokenizer` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `JsonNode`

JSON –æ–±—ä–µ–∫—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ HuggingFace tokenizer.json, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:
- `version`: –≤–µ—Ä—Å–∏—è —Ñ–æ—Ä–º–∞—Ç–∞
- `model`: –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ (BPE, WordPiece, etc.)
- `vocab`: —Å–ª–æ–≤–∞—Ä—å —Ç–æ–∫–µ–Ω–æ–≤
- `merges`: —Å–ø–∏—Å–æ–∫ BPE —Å–ª–∏—è–Ω–∏–π
- `normalizer`, `pre_tokenizer`, `post_processor`, `decoder`: –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã pipeline

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
var tok = trainBPE(corpus, vocabSize = 10000)

# –≠–∫—Å–ø–æ—Ä—Ç –≤ HuggingFace —Ñ–æ—Ä–º–∞—Ç
let hfJson = toHuggingFaceFormat(tok)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
writeFile("tokenizer.json", $hfJson)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ Python —Å transformers
# from transformers import PreTrainedTokenizerFast
# tokenizer = PreTrainedTokenizerFast(tokenizer_file="tokenizer.json")
```

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ JSON

```json
{
  "version": "1.0",
  "model": {
    "type": "BPE",
    "vocab": {
      "–∞": 0,
      "–±": 1,
      ...
    },
    "merges": [
      "–∞ –±",
      "–≤ –≥",
      ...
    ]
  },
  "normalizer": {...},
  "pre_tokenizer": {...},
  "post_processor": {...},
  "decoder": {...}
}
```

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –î–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å HuggingFace Transformers
- ‚úÖ –ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º HuggingFace
- ‚úÖ –î–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
- ‚úÖ –ü—Ä–∏ –º–∏–≥—Ä–∞—Ü–∏–∏ –º–µ–∂–¥—É —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º–∏

---

## toSentencePieceModel

```nim
proc toSentencePieceModel*(t: Tokenizer, path: string)
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç SentencePiece model –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π SentencePiece.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `t` | `Tokenizer` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞. |
| `path` | `string` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

–ù–∏—á–µ–≥–æ (void). –°–æ–∑–¥–∞–µ—Ç —Ñ–∞–π–ª –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø—É—Ç–∏.

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
var spTok = trainSentencePiece(corpus, vocabSize = 8000)

# –≠–∫—Å–ø–æ—Ä—Ç –≤ SentencePiece —Ñ–æ—Ä–º–∞—Ç
toSentencePieceModel(spTok, "model.sp")

# –ó–∞–≥—Ä—É–∑–∫–∞ –≤ SentencePiece (Python)
# import sentencepiece as spm
# sp = spm.SentencePieceProcessor()
# sp.Load("model.sp")
```

### –§–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞

```
# SentencePiece model
# Vocabulary size: 8000

‚ñÅ	-1.234567
–∞	-2.345678
–±	-2.456789
...
```

–ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç:
- –¢–æ–∫–µ–Ω (—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: —Ç–∞–±—É–ª—è—Ü–∏—è)
- Score (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å/—á–∞—Å—Ç–æ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ float)

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å SentencePiece –≤ –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–∞—Ö
- ‚úÖ –ü—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å T5, mT5, XLM-R –º–æ–¥–µ–ª—è–º–∏
- ‚úÖ –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å TensorFlow Text
- ‚ùå –§–æ—Ä–º–∞—Ç —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π (–±–µ–∑ –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏ .model)

---

## toTikTokenFormat

```nim
proc toTikTokenFormat*(t: Tokenizer): string
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ —Ñ–æ—Ä–º–∞—Ç TikToken (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ OpenAI GPT –º–æ–¥–µ–ª—è—Ö) –≤ –≤–∏–¥–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `t` | `Tokenizer` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `string`

–¢–µ–∫—Å—Ç–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ TikToken, –≥–¥–µ –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–∫–µ–Ω –∏ –µ–≥–æ ID.

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
var tok = trainByteLevelBPE(corpus, vocabSize = 50000)

# –≠–∫—Å–ø–æ—Ä—Ç –≤ TikToken —Ñ–æ—Ä–º–∞—Ç
let tikTokenData = toTikTokenFormat(tok)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
writeFile("tiktoken.txt", tikTokenData)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å tiktoken (Python)
# import tiktoken
# encoding = tiktoken.get_encoding_from_file("tiktoken.txt")
```

### –§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞

```
–∞ 0
–± 1
–≤ 2
...
–ø—Ä–∏–≤–µ—Ç 1234
–º–∏—Ä 1235
```

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å OpenAI tiktoken
- ‚úÖ –ü—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å GPT-–ø–æ–¥–æ–±–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
- ‚úÖ –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ OpenAI –º–æ–¥–µ–ª–µ–π
- ‚ùå –ü—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–∞—Ç (—Ç–æ–ª—å–∫–æ vocab, –±–µ–∑ merges)

---

## fromPretrainedModel

```nim
proc fromPretrainedModel*(modelName: string): Tokenizer
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–°–æ–∑–¥–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (GPT-2, BERT).

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `modelName` | `string` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –ò–º—è –º–æ–¥–µ–ª–∏. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: "gpt2", "gpt-2", "bert", "bert-base". |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `Tokenizer`

–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
# GPT-2 —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
var gpt2Tok = fromPretrainedModel("gpt2")
echo "–¢–∏–ø: ", gpt2Tok.kind  # tkByteLevelBPE
echo "Special token: ", gpt2Tok.specialTokens.eosToken  # <|endoftext|>

# BERT —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
var bertTok = fromPretrainedModel("bert")
echo "–¢–∏–ø: ", bertTok.kind  # tkWordPiece
echo "CLS token: ", bertTok.specialTokens.clsToken  # [CLS]
echo "–ü—Ä–µ—Ñ–∏–∫—Å: ", bertTok.continuingSubwordPrefix  # ##

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
let text = "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä"
let tokens = tokenize(text, gpt2Tok)
```

### –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏

**GPT-2** (`"gpt2"`, `"gpt-2"`):
- –¢–∏–ø: Byte-Level BPE
- Special tokens: `<|endoftext|>` (–≤—Å–µ —Ä–æ–ª–∏)
- –°–ª–æ–≤–∞—Ä—å: –ø—É—Å—Ç–æ–π (—Ç—Ä–µ–±—É–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ –æ–±—É—á–µ–Ω–∏–µ)

**BERT** (`"bert"`, `"bert-base"`):
- –¢–∏–ø: WordPiece
- Special tokens: `[PAD]`, `[UNK]`, `[CLS]`, `[SEP]`, `[MASK]`
- –ü—Ä–µ—Ñ–∏–∫—Å: `##`
- –°–ª–æ–≤–∞—Ä—å: –ø—É—Å—Ç–æ–π (—Ç—Ä–µ–±—É–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ –æ–±—É—á–µ–Ω–∏–µ)

### –í–∞–∂–Ω–æ

‚ö†Ô∏è –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞–µ—Ç **—à–∞–±–ª–æ–Ω** —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏, –Ω–æ **–±–µ–∑ —Å–ª–æ–≤–∞—Ä—è**. –î–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ:
1. –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–ª–æ–≤–∞—Ä—å –∏–∑ —Ñ–∞–π–ª–∞: `tok = loadTokenizer("path/to/vocab")`
2. –ò–ª–∏ –æ–±—É—á–∏—Ç—å –Ω–∞ –∫–æ—Ä–ø—É—Å–µ

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
- ‚úÖ –ü—Ä–∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
- ‚úÖ –ö–∞–∫ —à–∞–±–ª–æ–Ω –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
- ‚ùå –ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞ –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ª–æ–≤–∞—Ä—è

---

# 11. –û—Ç–ª–∞–¥–∫–∞ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è

## visualizeTokenization

```nim
proc visualizeTokenization*(t: var Tokenizer, text: string): string
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é —Å —Ü–≤–µ—Ç–æ–≤—ã–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ. –ö–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –æ–∫—Ä–∞—à–∏–≤–∞–µ—Ç—Å—è –≤ —Å–≤–æ–π —Ü–≤–µ—Ç –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `t` | `var Tokenizer` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏. |
| `text` | `string` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –¢–µ–∫—Å—Ç –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `string`

–°—Ç—Ä–æ–∫–∞ —Å ANSI escape-–∫–æ–¥–∞–º–∏ –¥–ª—è —Ü–≤–µ—Ç–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ –∏ —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤.

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
var tok = trainBPE(corpus)

let text = "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?"
let visualization = visualizeTokenization(tok, text)

echo visualization
# –í—ã–≤–æ–¥ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ —Å —Ü–≤–µ—Ç–∞–º–∏:
# –ü—Ä–∏–≤–µ—Ç , –∫–∞–∫ –¥–µ–ª–∞ ?
#  ^red  ^green ^yellow ^blue...
#
# –¢–æ–∫–µ–Ω—ã:
#   [0] –ü—Ä–∏–≤–µ—Ç (id: 123)
#   [1] , (id: 45)
#   [2] –∫–∞–∫ (id: 678)
#   [3] –¥–µ–ª–∞ (id: 901)
#   [4] ? (id: 23)
```

### –¶–≤–µ—Ç–æ–≤–∞—è —Å—Ö–µ–º–∞

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 6 —Ü–≤–µ—Ç–æ–≤, —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è:
- üî¥ –ö—Ä–∞—Å–Ω—ã–π
- üü¢ –ó–µ–ª–µ–Ω—ã–π
- üü° –ñ–µ–ª—Ç—ã–π
- üîµ –°–∏–Ω–∏–π
- üü£ –§–∏–æ–ª–µ—Ç–æ–≤—ã–π
- üî∑ –ì–æ–ª—É–±–æ–π

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏
- –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ—á–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã —Ç–æ–∫–µ–Ω–æ–≤
- –í—ã–≤–æ–¥–∏—Ç —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ —Å –∏—Ö ID
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç ANSI escape –∫–æ–¥—ã –¥–ª—è —Ü–≤–µ—Ç–æ–≤

### –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ

```nim
# –û—Ç–ª–∞–¥–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
proc debugTokenizerOnExamples(tok: var Tokenizer, examples: seq[string]) =
  for i, example in examples:
    echo "
=== –ü—Ä–∏–º–µ—Ä ", i+1, " ==="
    echo visualizeTokenization(tok, example)

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
let examples = @["–ü—Ä–∏–≤–µ—Ç –º–∏—Ä", "—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"]
echo "BPE:"
echo visualizeTokenization(bpeTok, examples[0])
echo "
WordPiece:"
echo visualizeTokenization(wpTok, examples[0])
```

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –ü—Ä–∏ –æ—Ç–ª–∞–¥–∫–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
- ‚úÖ –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
- ‚úÖ –ü—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Ä–∞–∑–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
- ‚úÖ –î–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
- ‚ùå –ù–µ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞ (—Ç–æ–ª—å–∫–æ –¥–ª—è dev/debug)

---

## debugTokenization

```nim
proc debugTokenization*(t: var Tokenizer, text: string): DebugInfo
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: —Ç–æ–∫–µ–Ω—ã, –∏—Ö ID, –≥—Ä–∞–Ω–∏—Ü—ã, –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `t` | `var Tokenizer` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. |
| `text` | `string` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –¢–µ–∫—Å—Ç –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `DebugInfo`

–û–±—ä–µ–∫—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:
```nim
type DebugInfo* = object
  originalText*: string           # –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
  tokenIds*: seq[int]            # ID —Ç–æ–∫–µ–Ω–æ–≤
  tokens*: seq[string]           # –¢–æ–∫–µ–Ω—ã
  boundaries*: seq[(int, int)]   # –ì—Ä–∞–Ω–∏—Ü—ã (start, end) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
  unknownWords*: seq[string]     # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ ([UNK])
  warnings*: seq[string]         # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
```

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
var tok = trainBPE(smallCorpus, vocabSize = 100)

let text = "–ø—Ä–∏–≤–µ—Ç –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ—Å–ª–æ–≤–æ –º–∏—Ä"
let debug = debugTokenization(tok, text)

echo "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: ", debug.originalText
echo "–¢–æ–∫–µ–Ω—ã: ", debug.tokens
echo "ID: ", debug.tokenIds
echo "–ì—Ä–∞–Ω–∏—Ü—ã: ", debug.boundaries
echo "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞: ", debug.unknownWords
echo "–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:"
for warning in debug.warnings:
  echo "  ‚ö†Ô∏è  ", warning

# –í—ã–≤–æ–¥:
# –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: –ø—Ä–∏–≤–µ—Ç –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ—Å–ª–æ–≤–æ –º–∏—Ä
# –¢–æ–∫–µ–Ω—ã: @["–ø—Ä–∏–≤–µ—Ç", "[UNK]", "–º–∏—Ä"]
# ID: @[123, 0, 456]
# Boundaries: @[(0, 6), (7, 23), (24, 27)]
# –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞: @["[UNK]"]
# –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:
#   ‚ö†Ô∏è  –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ —Å–ª–æ–≤–æ: '[UNK]' –≤ –ø–æ–∑–∏—Ü–∏–∏ 7
```

### –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

```nim
# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
proc analyzeTokenizationQuality(tok: var Tokenizer, testTexts: seq[string]) =
  var totalUnknown = 0
  var totalTokens = 0
  
  for text in testTexts:
    let debug = debugTokenization(tok, text)
    totalUnknown += debug.unknownWords.len
    totalTokens += debug.tokens.len
    
    if debug.unknownWords.len > 0:
      echo "–¢–µ–∫—Å—Ç: ", text[0..min(50, text.len-1)], "..."
      echo "  UNK —Å–ª–æ–≤–∞: ", debug.unknownWords
  
  let unkRate = totalUnknown.float / totalTokens.float * 100
  echo "
–û–±—â–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç UNK: ", unkRate.formatFloat(ffDecimal, 2), "%"
```

### –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è

–§—É–Ω–∫—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:
- ‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ —Å –∏—Ö –ø–æ–∑–∏—Ü–∏—è–º–∏
- ‚ö†Ô∏è –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—Å–ª–∏ >50% —Ç–æ–∫–µ–Ω–æ–≤ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –ü—Ä–∏ –æ—Ç–ª–∞–¥–∫–µ –ø—Ä–æ–±–ª–µ–º —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π
- ‚úÖ –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–∫—Ä—ã—Ç–∏—è —Å–ª–æ–≤–∞—Ä—è
- ‚úÖ –ü—Ä–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è
- ‚úÖ –î–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–∏—á–∏–Ω –ø–ª–æ—Ö–æ–π —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏
- ‚úÖ –ü—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

---

## explainToken

```nim
proc explainToken*(t: Tokenizer, tokenId: int): TokenExplanation
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–û–±—ä—è—Å–Ω—è–µ—Ç –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –∏ —Å–≤–æ–π—Å—Ç–≤–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞: –∫–∞–∫ –æ–Ω –±—ã–ª —Å–æ–∑–¥–∞–Ω (—á–µ—Ä–µ–∑ —Å–ª–∏—è–Ω–∏—è BPE), –µ–≥–æ —á–∞—Å—Ç–æ—Ç—É –∏ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `t` | `Tokenizer` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. |
| `tokenId` | `int` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. ID —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `TokenExplanation`

–û–±—ä–µ–∫—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:
```nim
type TokenExplanation* = object
  token*: string               # –°–∞–º —Ç–æ–∫–µ–Ω
  tokenId*: int               # ID —Ç–æ–∫–µ–Ω–∞
  frequency*: int             # –ß–∞—Å—Ç–æ—Ç–∞ (0 –µ—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ)
  mergeHistory*: seq[string]  # –ò—Å—Ç–æ—Ä–∏—è BPE —Å–ª–∏—è–Ω–∏–π
  exampleContexts*: seq[string] # –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
```

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
var tok = trainBPE(corpus)

# –û–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ "–ø—Ä–∏–≤–µ—Ç"
let tokenId = getIdByToken(tok, "–ø—Ä–∏–≤–µ—Ç")
let explanation = explainToken(tok, tokenId)

echo "–¢–æ–∫–µ–Ω: ", explanation.token
echo "ID: ", explanation.tokenId
echo "–ò—Å—Ç–æ—Ä–∏—è —Å–ª–∏—è–Ω–∏–π:"
for merge in explanation.mergeHistory:
  echo "  ", merge

# –í—ã–≤–æ–¥:
# –¢–æ–∫–µ–Ω: –ø—Ä–∏–≤–µ—Ç
# ID: 1234
# –ò—Å—Ç–æ—Ä–∏—è —Å–ª–∏—è–Ω–∏–π:
#   –ø—Ä–∏ + –≤–µ—Ç
#   –ø—Ä + –∏

# –ê–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
proc explainTopTokens(tok: Tokenizer, count: int = 10) =
  for i in 0..<min(count, tok.inverseVocab.len):
    let exp = explainToken(tok, i)
    echo $i, ": ", exp.token
    if exp.mergeHistory.len > 0:
      echo "  –°–æ–∑–¥–∞–Ω –∏–∑: ", exp.mergeHistory[^1]
```

### –ò—Å—Ç–æ—Ä–∏—è —Å–ª–∏—è–Ω–∏–π (–¥–ª—è BPE)

–î–ª—è —Ç–æ–∫–µ–Ω–æ–≤ BPE/Byte-Level BPE —Ñ—É–Ω–∫—Ü–∏—è –ø—ã—Ç–∞–µ—Ç—Å—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞—Ç—å, –∏–∑ –∫–∞–∫–∏—Ö –±–æ–ª–µ–µ –º–µ–ª–∫–∏—Ö —á–∞—Å—Ç–µ–π –±—ã–ª —Å–æ–∑–¥–∞–Ω —Ç–æ–∫–µ–Ω:

```nim
# –ü—Ä–∏–º–µ—Ä –¥–ª—è —Ç–æ–∫–µ–Ω–∞ "–∏–≥—Ä–∞—Ç—å":
# –ò—Å—Ç–æ—Ä–∏—è –º–æ–∂–µ—Ç –ø–æ–∫–∞–∑–∞—Ç—å:
#   –∏ + –≥—Ä–∞—Ç—å
#   –∏–≥—Ä + –∞—Ç—å
#   –∏–≥—Ä–∞ + —Ç—å
```

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

- –î–ª—è BPE —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—É—Ç–∏ —Å–æ–∑–¥–∞–Ω–∏—è
- –î–ª—è WordPiece/SentencePiece –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç—É—é –∏—Å—Ç–æ—Ä–∏—é
- `frequency` –≤—Å–µ–≥–¥–∞ 0 (—Ç—Ä–µ–±—É–µ—Ç—Å—è –∫–æ—Ä–ø—É—Å –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞)
- `exampleContexts` –∑–∞–ø–æ–ª–Ω–µ–Ω placeholder'–æ–º

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –î–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–ª–æ–≤–∞—Ä—è
- ‚úÖ –ü—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∫–∞—á–µ—Å—Ç–≤–∞ BPE —Å–ª–∏—è–Ω–∏–π
- ‚úÖ –î–ª—è –æ—Ç–ª–∞–¥–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
- ‚úÖ –ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –∏ –∏–∑—É—á–µ–Ω–∏–∏ BPE –∞–ª–≥–æ—Ä–∏—Ç–º–∞
- ‚ùå –ù–µ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞ (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è)

---

## findTokenConflicts

```nim
proc findTokenConflicts*(t: Tokenizer): seq[TokenConflict]
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã –≤ —Å–ª–æ–≤–∞—Ä–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è —Ç–æ–∫–µ–Ω—ã, –ø–æ—Ö–æ–∂–∏–µ —Ç–æ–∫–µ–Ω—ã, –≤–æ–∑–º–æ–∂–Ω—ã–µ –æ—à–∏–±–∫–∏.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `t` | `Tokenizer` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `seq[TokenConflict]`

–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤, –≥–¥–µ –∫–∞–∂–¥—ã–π:
```nim
type TokenConflict* = object
  token1*: string                # –ü–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω
  token2*: string                # –í—Ç–æ—Ä–æ–π —Ç–æ–∫–µ–Ω  
  conflictType*: string          # –¢–∏–ø –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞
  suggestedResolution*: string   # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
```

### –¢–∏–ø—ã –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤

1. **"overlap"** - –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
   - –û–¥–∏–Ω —Ç–æ–∫–µ–Ω —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–µ—Ñ–∏–∫—Å–æ–º –¥—Ä—É–≥–æ–≥–æ
   - –ü—Ä–∏–º–µ—Ä: "–ø—Ä–∏" –∏ "–ø—Ä–∏–≤–µ—Ç"

2. **"ambiguous"** - –ù–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç—å
   - –¢–æ–∫–µ–Ω—ã –æ—Ç–ª–∏—á–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∏–º —Å–∏–º–≤–æ–ª–æ–º
   - –ü—Ä–∏–º–µ—Ä: "–ø—Ä–∏–≤–µ—Ç" –∏ "–ø—Ä—é–≤–µ—Ç" (–≤–æ–∑–º–æ–∂–Ω–∞—è –æ–ø–µ—á–∞—Ç–∫–∞)

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
var tok = trainBPE(corpus, vocabSize = 10000)

let conflicts = findTokenConflicts(tok)

echo "–ù–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤: ", conflicts.len

for conflict in conflicts:
  echo "
–ö–æ–Ω—Ñ–ª–∏–∫—Ç: ", conflict.conflictType
  echo "  –¢–æ–∫–µ–Ω 1: ", conflict.token1
  echo "  –¢–æ–∫–µ–Ω 2: ", conflict.token2
  echo "  –†–µ—à–µ–Ω–∏–µ: ", conflict.suggestedResolution

# –í—ã–≤–æ–¥:
# –ù–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤: 45
#
# –ö–æ–Ω—Ñ–ª–∏–∫—Ç: overlap
#   –¢–æ–∫–µ–Ω 1: –ø—Ä–∏
#   –¢–æ–∫–µ–Ω 2: –ø—Ä–∏–≤–µ—Ç
#   –†–µ—à–µ–Ω–∏–µ: –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å —É–¥–∞–ª–µ–Ω–∏–µ –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
#
# –ö–æ–Ω—Ñ–ª–∏–∫—Ç: ambiguous
#   –¢–æ–∫–µ–Ω 1: –ø—Ä–∏–µ–º
#   –¢–æ–∫–µ–Ω 2: –ø—Ä–∏—ë–º
#   –†–µ—à–µ–Ω–∏–µ: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω–µ –æ—à–∏–±–∫–∞ –ª–∏ —ç—Ç–æ –≤ —Å–ª–æ–≤–∞—Ä–µ
```

### –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤

```nim
# –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø–∞–º
proc analyzeConflicts(tok: Tokenizer) =
  let conflicts = findTokenConflicts(tok)
  
  var byType = initTable[string, int]()
  for conflict in conflicts:
    if conflict.conflictType notin byType:
      byType[conflict.conflictType] = 0
    byType[conflict.conflictType] += 1
  
  echo "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤:"
  for ctype, count in byType:
    echo "  ", ctype, ": ", count
```

### –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

- –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞–∫—Å–∏–º—É–º 100 –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ (–¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
- –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–æ–ª—å–∫–æ –ø–∞—Ä—ã —Ç–æ–∫–µ–Ω–æ–≤ (–Ω–µ —Ç—Ä–æ–π–∫–∏ –∏ —Ç.–¥.)
- –î–ª—è –±–æ–ª—å—à–∏—Ö —Å–ª–æ–≤–∞—Ä–µ–π (>10000) –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
- ‚úÖ –ü—Ä–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–≤–∞—Ä—è
- ‚úÖ –î–ª—è –ø–æ–∏—Å–∫–∞ –æ—à–∏–±–æ–∫ –≤ —Å–ª–æ–≤–∞—Ä–µ
- ‚úÖ –ü—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
- ‚ùå –ù–µ –∑–∞–ø—É—Å–∫–∞—Ç—å –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ (–º–µ–¥–ª–µ–Ω–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Å–ª–æ–≤–∞—Ä–µ–π)

---

## getReadableTokens

```nim
proc getReadableTokens*(t: Tokenizer, tokenIds: seq[int]): seq[string]
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å ID —Ç–æ–∫–µ–Ω–æ–≤ –≤ —á–∏—Ç–∞–µ–º—ã–µ —Å—Ç—Ä–æ–∫–∏. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ ID.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `t` | `Tokenizer` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. |
| `tokenIds` | `seq[int]` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å ID —Ç–æ–∫–µ–Ω–æ–≤. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `seq[string]`

–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫. –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ ID –∑–∞–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ `[INVALID_ID:N]`.

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
var tok = trainBPE(corpus)

let tokenIds = @[0, 1, 2, 100, 9999]
let readable = getReadableTokens(tok, tokenIds)

echo "–¢–æ–∫–µ–Ω—ã: ", readable
# –í—ã–≤–æ–¥: @["[PAD]", "[UNK]", "–∞", "–ø—Ä–∏–≤–µ—Ç", "[INVALID_ID:9999]"]

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å decode
let text = "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä"
let encoded = tokenize(text, tok)
let tokens = getReadableTokens(tok, encoded)
let decoded = decode(encoded, tok)

echo "–ò—Å—Ö–æ–¥–Ω—ã–π: ", text
echo "–¢–æ–∫–µ–Ω—ã: ", tokens
echo "–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π: ", decoded
```

### –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ

```nim
# –û—Ç–ª–∞–¥–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
proc debugSequence(tok: Tokenizer, ids: seq[int]) =
  echo "IDs: ", ids
  echo "–¢–æ–∫–µ–Ω—ã:"
  let tokens = getReadableTokens(tok, ids)
  for i, token in tokens:
    echo "  [", i, "] id=", ids[i], " -> '", token, "'"

# –ê–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–∏
proc inspectModelOutput(tok: Tokenizer, modelOutput: seq[int]) =
  let tokens = getReadableTokens(tok, modelOutput)
  echo "–ú–æ–¥–µ–ª—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞:"
  echo tokens.join(" ")
  
  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ invalid
  for i, token in tokens:
    if token.startsWith("[INVALID"):
      echo "‚ö†Ô∏è  –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π ID –≤ –ø–æ–∑–∏—Ü–∏–∏ ", i, ": ", modelOutput[i]
```

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –î–ª—è –æ—Ç–ª–∞–¥–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Ç–æ–∫–µ–Ω–æ–≤
- ‚úÖ –ü—Ä–∏ –∏–Ω—Å–ø–µ–∫—Ü–∏–∏ –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏
- ‚úÖ –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
- ‚úÖ –ü—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
- ‚úÖ –í –ª—é–±—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö, –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ —É–≤–∏–¥–µ—Ç—å —Ç–æ–∫–µ–Ω—ã –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏

---

## analyzeVocabCoverage

```nim
proc analyzeVocabCoverage*(t: var Tokenizer, texts: seq[string]): 
  tuple[coverage: float, uniqueTokens: int, totalWords: int]
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–∫—Ä—ã—Ç–∏–µ —Å–ª–æ–≤–∞—Ä—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö: –∫–∞–∫–æ–π –ø—Ä–æ—Ü–µ–Ω—Ç —Å–ª–æ–≤ –º–æ–∂–Ω–æ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –±–µ–∑ [UNK] —Ç–æ–∫–µ–Ω–∞.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `t` | `var Tokenizer` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. |
| `texts` | `seq[string]` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–∫—Ä—ã—Ç–∏—è. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `tuple[coverage: float, uniqueTokens: int, totalWords: int]`

–ö–æ—Ä—Ç–µ–∂, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:
- `coverage`: –ø—Ä–æ—Ü–µ–Ω—Ç –ø–æ–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–≤ (0.0 - 1.0)
- `uniqueTokens`: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–∞—Ö
- `totalWords`: –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
var tok = trainBPE(trainCorpus, vocabSize = 5000)

# –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã
let testTexts = @[
  "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä",
  "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ",
  "–ù–æ–≤–æ–µ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ —Å–ª–æ–≤–æ xyzabc"
]

let (coverage, uniqueTokens, totalWords) = analyzeVocabCoverage(tok, testTexts)

echo "–ü–æ–∫—Ä—ã—Ç–∏–µ: ", (coverage * 100).formatFloat(ffDecimal, 2), "%"
echo "–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: ", uniqueTokens
echo "–í—Å–µ–≥–æ —Å–ª–æ–≤: ", totalWords

# –í—ã–≤–æ–¥:
# –ü–æ–∫—Ä—ã—Ç–∏–µ: 85.71%
# –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: 156
# –í—Å–µ–≥–æ —Å–ª–æ–≤: 7

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ —Å–ª–æ–≤–∞—Ä—è
proc compareVocabSizes(corpus, testTexts: seq[string]) =
  for vocabSize in [1000, 5000, 10000, 30000]:
    var tok = trainBPE(corpus, vocabSize = vocabSize)
    let (cov, _, _) = analyzeVocabCoverage(tok, testTexts)
    echo "Vocab size: ", vocabSize, " -> Coverage: ", 
         (cov * 100).formatFloat(ffDecimal, 2), "%"

# –í—ã–≤–æ–¥:
# Vocab size: 1000 -> Coverage: 65.30%
# Vocab size: 5000 -> Coverage: 85.71%
# Vocab size: 10000 -> Coverage: 94.12%
# Vocab size: 30000 -> Coverage: 98.53%
```

### –ú–µ—Ç—Ä–∏–∫–∞ coverage

Coverage —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –∫–∞–∫:
```
coverage = (—Å–ª–æ–≤–∞ –±–µ–∑ UNK) / (–≤—Å–µ–≥–æ —Å–ª–æ–≤)
```

–°–ª–æ–≤–æ —Å—á–∏—Ç–∞–µ—Ç—Å—è **–ø–æ–∫—Ä—ã—Ç—ã–º**, –µ—Å–ª–∏ –ø—Ä–∏ –µ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –Ω–µ –ø–æ—è–≤–ª—è–µ—Ç—Å—è [UNK] —Ç–æ–∫–µ–Ω.

### –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ

```nim
# –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –¥–æ–º–µ–Ω–∞—Ö
proc validateOnDomains(tok: var Tokenizer) =
  let domains = [
    ("–ù–æ–≤–æ—Å—Ç–∏", newsTexts),
    ("–ë–ª–æ–≥–∏", blogTexts),
    ("–ù–∞—É—á–Ω—ã–µ", scientificTexts)
  ]
  
  for (name, texts) in domains:
    let (cov, unique, total) = analyzeVocabCoverage(tok, texts)
    echo name, ":"
    echo "  Coverage: ", (cov*100).formatFloat(ffDecimal, 1), "%"
    echo "  Unique tokens: ", unique
    echo "  Total words: ", total

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è
proc findOptimalVocabSize(trainCorpus, testTexts: seq[string],
                          targetCoverage: float = 0.95): int =
  var lo = 1000
  var hi = 50000
  
  while hi - lo > 1000:
    let mid = (lo + hi) div 2
    var tok = trainBPE(trainCorpus, vocabSize = mid)
    let (cov, _, _) = analyzeVocabCoverage(tok, testTexts)
    
    if cov < targetCoverage:
      lo = mid
    else:
      hi = mid
  
  result = hi
  echo "–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –¥–ª—è ", 
       (targetCoverage*100).int, "% –ø–æ–∫—Ä—ã—Ç–∏—è: ", result
```

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –î–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
- ‚úÖ –ü—Ä–∏ –≤—ã–±–æ—Ä–µ —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è
- ‚úÖ –î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
- ‚úÖ –ü—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–æ–º–µ–Ω–∞—Ö
- ‚úÖ –î–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è

---

# 12. –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏

## truncateToRunes

```nim
proc truncateToRunes*(s: string, maxRunes: int): string
```

### –û–ø–∏—Å–∞–Ω–∏–µ
–û–±—Ä–µ–∑–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –¥–æ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ Unicode —Å–∏–º–≤–æ–ª–æ–≤ (—Ä—É–Ω), –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—è –º–Ω–æ–≥–æ–±–∞–π—Ç–æ–≤—ã–µ —Å–∏–º–≤–æ–ª—ã UTF-8.

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|----------|
| `s` | `string` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –ò—Å—Ö–æ–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞. |
| `maxRunes` | `int` | **–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π**. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ Unicode —Å–∏–º–≤–æ–ª–æ–≤. |

### –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

**–¢–∏–ø:** `string`

–û–±—Ä–µ–∑–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞, —Å–æ–¥–µ—Ä–∂–∞—â–∞—è –Ω–µ –±–æ–ª–µ–µ `maxRunes` Unicode —Å–∏–º–≤–æ–ª–æ–≤.

### –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```nim
# –õ–∞—Ç–∏–Ω–∏—Ü–∞
let text1 = "Hello World"
echo truncateToRunes(text1, 5)  # "Hello"

# –ö–∏—Ä–∏–ª–ª–∏—Ü–∞
let text2 = "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä"
echo truncateToRunes(text2, 6)  # "–ü—Ä–∏–≤–µ—Ç"

# –≠–º–æ–¥–∑–∏ (–∫–∞–∂–¥—ã–π —ç–º–æ–¥–∑–∏ = 1 —Ä—É–Ω–∞)
let text3 = "üòÄüòÉüòÑüòÅ"
echo truncateToRunes(text3, 2)  # "üòÄüòÉ"

# –°–º–µ—à–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
let text4 = "Hello –ø—Ä–∏–≤–µ—Ç ‰∏ñÁïå"
echo truncateToRunes(text4, 10)  # "Hello –ø—Ä–∏–≤"

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
proc summarizeText(text: string, maxLength: int = 100): string =
  let truncated = truncateToRunes(text, maxLength)
  if truncated.len < text.len:
    result = truncated & "..."
  else:
    result = text

let longText = "–û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç " & "–∞".repeat(200)
echo summarizeText(longText, 20)
# "–û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç ..."
```

### –í–∞–∂–Ω—ã–µ –æ—Ç–ª–∏—á–∏—è –æ—Ç –æ–±—ã—á–Ω–æ–≥–æ —Å—Ä–µ–∑–∞

```nim
let text = "–ü—Ä–∏–≤–µ—Ç"  # 6 UTF-8 —Ä—É–Ω, –Ω–æ 12 –±–∞–π—Ç

# –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û (–æ–±—Ä–µ–∑–∞–µ—Ç –ø–æ –±–∞–π—Ç–∞–º):
echo text[0..<6]  # –ú–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–º—É UTF-8!

# –ü–†–ê–í–ò–õ–¨–ù–û (–æ–±—Ä–µ–∑–∞–µ—Ç –ø–æ —Ä—É–Ω–∞–º):
echo truncateToRunes(text, 6)  # "–ü—Ä–∏–≤–µ—Ç"
echo truncateToRunes(text, 3)  # "–ü—Ä–∏"
```

### –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ

```nim
# –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –¥–ª—è UI
proc formatForDisplay(texts: seq[string], maxLen: int = 50): seq[string] =
  result = @[]
  for text in texts:
    result.add(truncateToRunes(text, maxLen))

# –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
proc createPreview(document: string, previewLength: int = 200): string =
  let preview = truncateToRunes(document, previewLength)
  result = preview & " [—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ]"

# –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
proc validateInput(input: string, maxChars: int = 280): bool =
  runeCount(input) <= maxChars
```

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

- ‚úÖ –ü—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å Unicode —Ç–µ–∫—Å—Ç–æ–º
- ‚úÖ –î–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–ª–∏–Ω—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –≤–≤–æ–¥–∞
- ‚úÖ –ü—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø—Ä–µ–≤—å—é/—Å–∞–º–º–∞—Ä–∏
- ‚úÖ –î–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ UI
- ‚úÖ –í–º–µ—Å—Ç–æ –æ–±—ã—á–Ω–æ–≥–æ —Å—Ä–µ–∑–∞ `[0..<n]` –¥–ª—è Unicode
- ‚ùå –ù–µ –Ω—É–∂–Ω–∞ –¥–ª—è ASCII-only —Ç–µ–∫—Å—Ç–æ–≤ (–Ω–æ –±–µ–∑–æ–ø–∞—Å–Ω–∞)

---

## –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞

–≠—Ç–æ –ø–æ–ª–Ω—ã–π —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –ø–æ API –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ tokenization.nim –≤–µ—Ä—Å–∏–∏ 0.7.

### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- **–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥**: tokenization.nim (5026 —Å—Ç—Ä–æ–∫)
- **–í–µ—Ä—Å–∏—è**: 0.7 (2026-02-01)
- **–ê–≤—Ç–æ—Ä**: github.com/Balans097
- **–Ø–∑—ã–∫**: Nim

### –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏

‚úÖ –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã (BPE, WordPiece, SentencePiece, Byte-Level BPE)
‚úÖ –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
‚úÖ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–µ–º –∏ –º–µ—Ç—Ä–∏–∫–∏
‚úÖ –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (–∫–æ–¥, –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞, Markdown, JSON)
‚úÖ –≠–∫—Å–ø–æ—Ä—Ç –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å (HuggingFace, SentencePiece, TikToken)
‚úÖ –û—Ç–ª–∞–¥–∫–∞ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
‚úÖ –ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (20-50x —É–ª—É—á—à–µ–Ω–∏—è)

### –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

```nim
import tokenization

# 1. –û–±—É—á–µ–Ω–∏–µ
let corpus = @["—Ç–µ–∫—Å—Ç 1", "—Ç–µ–∫—Å—Ç 2", "—Ç–µ–∫—Å—Ç 3"]
var tok = trainBPE(corpus, vocabSize = 10000)

# 2. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
let text = "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä"
let tokens = tokenize(text, tok)

# 3. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
let decoded = decode(tokens, tok)

# 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
saveTokenizer(tok, "my_tokenizer.bin")

# 5. –ó–∞–≥—Ä—É–∑–∫–∞
var loadedTok = loadTokenizer("my_tokenizer.bin")
```

---

**–ö–æ–Ω–µ—Ü —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞**