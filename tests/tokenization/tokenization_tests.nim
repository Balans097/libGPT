################################################################
##           ĞšĞĞœĞŸĞ›Ğ•ĞšĞ¡ĞĞ«Ğ• Ğ¢Ğ•Ğ¡Ğ¢Ğ« Ğ¢ĞĞšĞ•ĞĞ˜Ğ—ĞĞ¦Ğ˜Ğ˜
## 
##          Comprehensive tokenization tests
## 
## Ğ’ĞµÑ€ÑĞ¸Ñ:   0.5
## Ğ”Ğ°Ñ‚Ğ°:     2026-01-31
################################################################

import math, times, random, streams
import std/[tables, sequtils, strutils, algorithm, sets, unicode, json, os, re]

# Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
import tokenization


#==============================================================================
# Ğ£Ğ¢Ğ˜Ğ›Ğ˜Ğ¢Ğ« Ğ”Ğ›Ğ¯ Ğ¢Ğ•Ğ¡Ğ¢Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ¯
#==============================================================================

type
  TestResult = object
    name: string
    passed: bool
    message: string
    duration: float

  TestGroup = object
    name: string
    tests: seq[TestResult]
    totalTests: int
    passedTests: int
    failedTests: int
    totalDuration: float

var allGroups: seq[TestGroup] = @[]
var currentGroup: TestGroup

proc startTestGroup(name: string) =
  ## ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñƒ Ñ‚ĞµÑÑ‚Ğ¾Ğ²
  currentGroup = TestGroup(
    name: name,
    tests: @[],
    totalTests: 0,
    passedTests: 0,
    failedTests: 0,
    totalDuration: 0.0
  )
  echo ""
  echo "â•”" & "â•".repeat(70) & "â•—"
  echo "â•‘  ", name.alignLeft(66), "â•‘"
  echo "â•š" & "â•".repeat(70) & "â•"

proc endTestGroup() =
  ## Ğ—Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµÑ‚ Ñ‚ĞµĞºÑƒÑ‰ÑƒÑ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñƒ Ñ‚ĞµÑÑ‚Ğ¾Ğ²
  allGroups.add(currentGroup)
  echo ""
  echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
  echo "Ğ˜Ñ‚Ğ¾Ğ³Ğ¾: ", currentGroup.passedTests, "/", currentGroup.totalTests, 
        " Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ¹Ğ´ĞµĞ½Ğ¾"
  if currentGroup.failedTests > 0:
    echo "âŒ ĞŸÑ€Ğ¾Ğ²Ğ°Ğ»ĞµĞ½Ğ¾: ", currentGroup.failedTests
  else:
    echo "âœ… Ğ’ÑĞµ Ñ‚ĞµÑÑ‚Ñ‹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ¹Ğ´ĞµĞ½Ñ‹!"
  echo "Ğ’Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ: ", currentGroup.totalDuration.formatFloat(ffDecimal, 3), " ÑĞµĞº"
  echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

proc test(name: string, condition: bool, message: string = "") =
  ## Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¾Ğ´Ğ¸Ğ½ Ñ‚ĞµÑÑ‚
  let startTime = cpuTime()
  let passed = condition
  let duration = cpuTime() - startTime
  
  currentGroup.totalTests += 1
  currentGroup.totalDuration += duration
  
  if passed:
    currentGroup.passedTests += 1
    echo "âœ“ ", name
  else:
    currentGroup.failedTests += 1
    echo "âœ— ", name
    if message != "":
      echo "  ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°: ", message
  
  currentGroup.tests.add(TestResult(
    name: name,
    passed: passed,
    message: message,
    duration: duration
  ))

proc testApprox(name: string, actual: float, expected: float, 
                tolerance: float = 0.01, message: string = "") =
  ## Ğ¢ĞµÑÑ‚ Ñ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡Ğ¸ÑĞµĞ»
  let diff = abs(actual - expected)
  let passed = diff <= tolerance
  let msg = if message != "": message 
            else: "ĞĞ¶Ğ¸Ğ´Ğ°Ğ»Ğ¾ÑÑŒ: " & $expected & ", Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¾: " & $actual
  test(name, passed, msg)


#==============================================================================
# Ğ¢Ğ•Ğ¡Ğ¢ĞĞ’Ğ«Ğ• Ğ”ĞĞĞĞ«Ğ•
#==============================================================================

  const FN = "../Ğ¢ĞµĞºÑÑ‚Ñ‹ Ğ¸ ĞºĞ½Ğ¸Ğ³Ğ¸/Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚.txt"
  let corpus = split(readFile(FN), 'n')

const testSentences = @[
  "ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.",
  "Ğ­Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸.",
  "ĞšÑ€Ğ°Ñ‚ĞºĞ¾Ğµ.",
  "Ğ¢ĞµĞºÑÑ‚ ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ°Ğ¼Ğ¸: !@#$%^&*()",
  "Numbers: 123 456 789",
  "UPPERCASE AND lowercase MiXeD",
  "ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ² ÑĞ»Ğ¾Ğ² ÑĞ»Ğ¾Ğ²",
  "ĞºĞ½ÑĞ³Ğ¸Ğ½Ñ Ğ¡Ğ¾Ñ„ÑŒÑ Ğ’Ğ°ÑĞ¸Ğ»ÑŒĞµĞ²Ğ½Ğ° Ğ±Ñ‹Ğ»Ğ° Ñ…ÑƒĞ´Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ°Ñ"
]


#==============================================================================
# Ğ“Ğ Ğ£ĞŸĞŸĞ 1: Ğ¢Ğ•Ğ¡Ğ¢Ğ« BPE (BYTE PAIR ENCODING)
#==============================================================================

proc testBPE() =
  startTestGroup("Ğ“Ğ Ğ£ĞŸĞŸĞ 1: Ğ¢Ğ•Ğ¡Ğ¢Ğ« BPE (BYTE PAIR ENCODING)")
  
  echo "\nâ†’ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ BPE Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°..."
  var bpeTokenizer = trainBPE(testCorpus, vocabSize = 150, minFreq = 1)
  
  # Ğ¢ĞµÑÑ‚ 1.1: Ğ Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ
  test("1.1 Ğ Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ BPE",
       bpeTokenizer.vocab.len > 0 and bpeTokenizer.vocab.len <= 150,
       "Ğ Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ: " & $bpeTokenizer.vocab.len)
  
  # Ğ¢ĞµÑÑ‚ 1.2: ĞĞ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²
  test("1.2 ĞĞ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ PAD Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ² ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğµ",
       bpeTokenizer.specialTokens.padToken in bpeTokenizer.vocab)
  test("1.3 ĞĞ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ UNK Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ² ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğµ",
       bpeTokenizer.specialTokens.unkToken in bpeTokenizer.vocab)
  test("1.4 ĞĞ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ BOS Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ² ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğµ",
       bpeTokenizer.specialTokens.bosToken in bpeTokenizer.vocab)
  test("1.5 ĞĞ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ EOS Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ² ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğµ",
       bpeTokenizer.specialTokens.eosToken in bpeTokenizer.vocab)
  
  # Ğ¢ĞµÑÑ‚ 1.6: Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°
  let testText = "Ğ­Ñ‚Ğ¾ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ"
  let tokens = tokenize(testText, bpeTokenizer)
  test("1.6 Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ½ĞµĞ¿ÑƒÑÑ‚Ğ¾Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚",
       tokens.len > 0,
       "ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²: " & $tokens.len)
  
  # Ğ¢ĞµÑÑ‚ 1.7: Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
  let decoded = bpeTokenizer.decode(tokens, skipSpecialTokens = true)
  test("1.7 Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚",
       decoded.strip() == testText or 
       decoded.replace(" ", "").toLowerAscii() == testText.replace(" ", "").toLowerAscii(),
       "ĞÑ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»: '" & testText & "', Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾: '" & decoded & "'")
  
  # Ğ¢ĞµÑÑ‚ 1.8: Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ vocab Ğ¸ inverseVocab
  var vocabConsistent = true
  for token, id in bpeTokenizer.vocab:
    if id >= bpeTokenizer.inverseVocab.len or bpeTokenizer.inverseVocab[id] != token:
      vocabConsistent = false
      break
  test("1.8 Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ vocab Ğ¸ inverseVocab", vocabConsistent)
  
  # Ğ¢ĞµÑÑ‚ 1.9: ĞĞ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ merges
  test("1.9 ĞĞ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ BPE merges",
       bpeTokenizer.merges.len > 0,
       "ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ merges: " & $bpeTokenizer.merges.len)
  
  # Ğ¢ĞµÑÑ‚ 1.10: Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ°
  let savePath = "/tmp/test_bpe.json"
  saveTokenizer(bpeTokenizer, savePath)
  test("1.10 Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°", fileExists(savePath))
  
  var loadedTokenizer = loadTokenizer(savePath)
  test("1.11 Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°", loadedTokenizer.vocab.len == bpeTokenizer.vocab.len)
  
  # Ğ¢ĞµÑÑ‚ 1.12: Ğ˜Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ÑĞ»Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸
  let tokensOriginal = tokenize("Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚", bpeTokenizer)
  let tokensLoaded = tokenize("Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚", loadedTokenizer)
  test("1.12 Ğ˜Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸",
       tokensOriginal == tokensLoaded)
  
  # Ğ¢ĞµÑÑ‚ 1.13: ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸
  let metrics = getMetrics(bpeTokenizer, testCorpus)
  test("1.13 Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº - Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ",
       metrics.vocabSize > 0)
  test("1.14 Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº - ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ",
       metrics.compressionRatio > 0.0 and metrics.compressionRatio < 100.0)
  
  # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ»
  if fileExists(savePath):
    removeFile(savePath)
  
  endTestGroup()


#==============================================================================
# Ğ“Ğ Ğ£ĞŸĞŸĞ 2: Ğ¢Ğ•Ğ¡Ğ¢Ğ« WORDPIECE
#==============================================================================

proc testWordPiece() =
  startTestGroup("Ğ“Ğ Ğ£ĞŸĞŸĞ 2: Ğ¢Ğ•Ğ¡Ğ¢Ğ« WORDPIECE")
  
  echo "\nâ†’ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ WordPiece Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°..."
  var wpTokenizer = trainWordPiece(testCorpus, vocabSize = 150, minFreq = 1)
  
  # Ğ¢ĞµÑÑ‚ 2.1: Ğ¢Ğ¸Ğ¿ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°
  test("2.1 Ğ¢Ğ¸Ğ¿ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° WordPiece",
       wpTokenizer.kind == tkWordPiece)
  
  # Ğ¢ĞµÑÑ‚ 2.2: Ğ Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ
  test("2.2 Ğ Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ WordPiece",
       wpTokenizer.vocab.len > 0 and wpTokenizer.vocab.len <= 150)
  
  # Ğ¢ĞµÑÑ‚ 2.3: ĞŸÑ€ĞµÑ„Ğ¸ĞºÑ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ÑĞ»Ğ¾Ğ²Ğ°
  test("2.3 ĞĞ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ° Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ñ",
       wpTokenizer.continuingSubwordPrefix == "##")
  
  # Ğ¢ĞµÑÑ‚ 2.4: Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹
  test("2.4 ĞĞ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
       wpTokenizer.specialTokens.padToken in wpTokenizer.vocab and
       wpTokenizer.specialTokens.unkToken in wpTokenizer.vocab)
  
  # Ğ¢ĞµÑÑ‚ 2.5: Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ°Ğ¼Ğ¸
  let testText = "Ğ½ĞµĞ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾Ğµ ÑĞ»Ğ¾Ğ²Ğ¾"
  let tokens = tokenize(testText, wpTokenizer)
  test("2.5 Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚",
       tokens.len > 0)
  
  # Ğ¢ĞµÑÑ‚ 2.6: Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒĞ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑÑ‹ ##
  let decoded = wpTokenizer.decode(tokens, skipSpecialTokens = true)
  test("2.6 Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒĞ±Ğ¸Ñ€Ğ°ĞµÑ‚ ## Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑÑ‹",
       "##" notin decoded,
       "Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾: " & decoded)
  
  # Ğ¢ĞµÑÑ‚ 2.7: ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ²
  let unknownText = "qwertyzxcvb"
  let unknownTokens = tokenize(unknownText, wpTokenizer)
  test("2.7 ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ²",
       unknownTokens.len > 0)
  
  # Ğ¢ĞµÑÑ‚ 2.8: Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ°
  let longWord = "Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾ĞµĞ½ĞµĞ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾ĞµÑĞ»Ğ¾Ğ²Ğ¾"
  let longTokens = tokenize(longWord, wpTokenizer)
  test("2.8 Ğ”Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¿Ğ¾Ğ´ÑĞ»Ğ¾Ğ²Ğ°",
       longTokens.len >= 1)
  
  # Ğ¢ĞµÑÑ‚ 2.9: Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ-Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
  for sentence in testSentences[0..2]:
    let encoded = tokenize(sentence, wpTokenizer)
    let redecoded = wpTokenizer.decode(encoded, skipSpecialTokens = true)
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ ÑĞ¼Ñ‹ÑĞ» ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»ÑÑ (ÑƒĞ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ)
    let normalized1 = sentence.replace(" ", "").toLowerAscii()
    let normalized2 = redecoded.replace(" ", "").toLowerAscii()
    test("2.9 Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ encode-decode Ğ´Ğ»Ñ: " & sentence[0..min(20, sentence.len-1)],
         normalized1 == normalized2 or normalized2.contains(normalized1[0..min(5, normalized1.len-1)]))
  
  # Ğ¢ĞµÑÑ‚ 2.10: ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸
  let metrics = getMetrics(wpTokenizer, testCorpus)
  test("2.10 ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ - ÑƒÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ",
       metrics.vocabUtilization >= 0.0 and metrics.vocabUtilization <= 1.0)
  
  endTestGroup()


#==============================================================================
# Ğ“Ğ Ğ£ĞŸĞŸĞ 3: Ğ¢Ğ•Ğ¡Ğ¢Ğ« SENTENCEPIECE
#==============================================================================

proc testSentencePiece() =
  startTestGroup("Ğ“Ğ Ğ£ĞŸĞŸĞ 3: Ğ¢Ğ•Ğ¡Ğ¢Ğ« SENTENCEPIECE")
  
  echo "\nâ†’ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ SentencePiece Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°..."
  var spTokenizer = trainSentencePiece(testCorpus, vocabSize = 150)
  
  # Ğ¢ĞµÑÑ‚ 3.1: Ğ¢Ğ¸Ğ¿ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°
  test("3.1 Ğ¢Ğ¸Ğ¿ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° SentencePiece",
       spTokenizer.kind == tkSentencePiece)
  
  # Ğ¢ĞµÑÑ‚ 3.2: Ğ Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ
  test("3.2 Ğ Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ SentencePiece",
       spTokenizer.vocab.len > 0)
  
  # Ğ¢ĞµÑÑ‚ 3.3: ĞĞ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ scores
  test("3.3 ĞĞ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ scores Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
       spTokenizer.scores.len > 0)
  
  # Ğ¢ĞµÑÑ‚ 3.4: Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹
  test("3.4 Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğµ",
       spTokenizer.specialTokens.unkToken in spTokenizer.vocab)
  
  # Ğ¢ĞµÑÑ‚ 3.5: Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
  let testText = "Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ"
  let tokens = tokenize(testText, spTokenizer)
  test("3.5 Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚",
       tokens.len > 0,
       "ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²: " & $tokens.len)
  
  # Ğ¢ĞµÑÑ‚ 3.6: Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
  let decoded = spTokenizer.decode(tokens, skipSpecialTokens = true)
  test("3.6 Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚",
       decoded.len > 0)
  
  # Ğ¢ĞµÑÑ‚ 3.7: Ğ’ÑĞµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸Ğ¼ĞµÑÑ‚ scores
  var allHaveScores = true
  for token in spTokenizer.vocab.keys:
    if token notin spTokenizer.scores:
      allHaveScores = false
      break
  test("3.7 Ğ’ÑĞµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ¸Ğ¼ĞµÑÑ‚ scores", allHaveScores)
  
  # Ğ¢ĞµÑÑ‚ 3.8: ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ²
  let textWithSpaces = "ÑĞ»Ğ¾Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» ÑĞ»Ğ¾Ğ²Ğ¾"
  let spacesTokens = tokenize(textWithSpaces, spTokenizer)
  test("3.8 ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ²",
       spacesTokens.len > 0)
  
  # Ğ¢ĞµÑÑ‚ 3.9: ĞšĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ encode-decode
  let original = "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸"
  let encoded = tokenize(original, spTokenizer)
  let redecoded = spTokenizer.decode(encoded, skipSpecialTokens = true)
  let norm1 = original.replace(" ", "").toLowerAscii()
  let norm2 = redecoded.replace(" ", "").replace("â–", "").toLowerAscii()
  test("3.9 ĞšĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ encode-decode",
       norm1 == norm2 or norm2.contains(norm1[0..min(3, norm1.len-1)]))
  
  # Ğ¢ĞµÑÑ‚ 3.10: ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸
  let metrics = getMetrics(spTokenizer, testCorpus)
  test("3.10 ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ - ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ",
       metrics.compressionRatio > 0.0)
  
  endTestGroup()


#==============================================================================
# Ğ“Ğ Ğ£ĞŸĞŸĞ 4: Ğ¢Ğ•Ğ¡Ğ¢Ğ« BYTE-LEVEL BPE
#==============================================================================

proc testByteLevelBPE() =
  startTestGroup("Ğ“Ğ Ğ£ĞŸĞŸĞ 4: Ğ¢Ğ•Ğ¡Ğ¢Ğ« BYTE-LEVEL BPE (GPT-2 STYLE)")
  
  echo "\nâ†’ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ByteLevel BPE Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°..."
  var blbpeTokenizer = trainByteLevelBPE(testCorpus, vocabSize = 200)
  
  # Ğ¢ĞµÑÑ‚ 4.1: Ğ¢Ğ¸Ğ¿ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°
  test("4.1 Ğ¢Ğ¸Ğ¿ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° ByteLevelBPE",
       blbpeTokenizer.kind == tkByteLevelBPE)
  
  # Ğ¢ĞµÑÑ‚ 4.2: ĞĞ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ byte encoder
  test("4.2 ĞĞ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ byte encoder",
       blbpeTokenizer.byteEncoder.len == 256)
  
  # Ğ¢ĞµÑÑ‚ 4.3: ĞĞ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ byte decoder
  test("4.3 ĞĞ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ byte decoder",
       blbpeTokenizer.byteDecoder.len == 256)
  
  # Ğ¢ĞµÑÑ‚ 4.4: ĞšĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ encoder-decoder
  var encoderDecoderConsistent = true
  for b, s in blbpeTokenizer.byteEncoder:
    if blbpeTokenizer.byteDecoder[s] != b:
      encoderDecoderConsistent = false
      break
  test("4.4 ĞšĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ byte encoder/decoder", encoderDecoderConsistent)
  
  # Ğ¢ĞµÑÑ‚ 4.5: Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ UTF-8 Ñ‚ĞµĞºÑÑ‚Ğ°
  let testText = "ĞºĞ½ÑĞ³Ğ¸Ğ½Ñ Ğ¡Ğ¾Ñ„ÑŒÑ Ğ’Ğ°ÑĞ¸Ğ»ÑŒĞµĞ²Ğ½Ğ°"
  let tokens = tokenize(testText, blbpeTokenizer, addSpecialTokens = false)
  test("4.5 Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ UTF-8 Ñ‚ĞµĞºÑÑ‚Ğ°",
       tokens.len > 0,
       "ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²: " & $tokens.len)
  
  # Ğ¢ĞµÑÑ‚ 4.6: Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ‚ĞµĞºÑÑ‚
  let decoded = blbpeTokenizer.decode(tokens, skipSpecialTokens = true)
  test("4.6 Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚",
       decoded == testText,
       "ĞÑ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»: '" & testText & "', Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾: '" & decoded & "'")
  
  # Ğ¢ĞµÑÑ‚ 4.7: ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑĞ¿ĞµÑ†ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²
  let specialChars = "!@#$%^&*()"
  let specialTokens = tokenize(specialChars, blbpeTokenizer, addSpecialTokens = false)
  let specialDecoded = blbpeTokenizer.decode(specialTokens, skipSpecialTokens = true)
  test("4.7 ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²",
       specialDecoded == specialChars,
       "ĞÑ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»: '" & specialChars & "', Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾: '" & specialDecoded & "'")
  
  # Ğ¢ĞµÑÑ‚ 4.8: ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‡Ğ¸ÑĞµĞ»
  let numbers = "123456789"
  let numTokens = tokenize(numbers, blbpeTokenizer, addSpecialTokens = false)
  let numDecoded = blbpeTokenizer.decode(numTokens, skipSpecialTokens = true)
  test("4.8 ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‡Ğ¸ÑĞµĞ»",
       numDecoded == numbers)
  
  # Ğ¢ĞµÑÑ‚ 4.9: Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ²
  let textWithSpaces = "ÑĞ»Ğ¾Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» ÑĞ»Ğ¾Ğ²Ğ¾"
  let spaceTokens = tokenize(textWithSpaces, blbpeTokenizer, addSpecialTokens = false)
  let spaceDecoded = blbpeTokenizer.decode(spaceTokens, skipSpecialTokens = true)
  test("4.9 Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ²",
       spaceDecoded == textWithSpaces,
       "ĞÑ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»: '" & textWithSpaces & "', Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾: '" & spaceDecoded & "'")
  
  # Ğ¢ĞµÑÑ‚ 4.10: Token offsets
  let offsets = tokenizeWithOffsets(testText, blbpeTokenizer, addSpecialTokens = false)
  test("4.10 Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ token offsets",
       offsets.len > 0)
  
  # Ğ¢ĞµÑÑ‚ 4.11: ĞšĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ char offsets
  if offsets.len > 0:
    var offsetsCorrect = true
    for offset in offsets:
      if offset.startChar < 0 or offset.endChar > testText.runeLen or 
         offset.startChar >= offset.endChar:
        offsetsCorrect = false
        break
    test("4.11 ĞšĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ char offsets", offsetsCorrect)
  
  # Ğ¢ĞµÑÑ‚ 4.12: ĞšĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ byte offsets
  if offsets.len > 0:
    var byteOffsetsCorrect = true
    for offset in offsets:
      if offset.startByte < 0 or offset.endByte > testText.len or
         offset.startByte >= offset.endByte:
        byteOffsetsCorrect = false
        break
    test("4.12 ĞšĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ byte offsets", byteOffsetsCorrect)
  
  endTestGroup()


#==============================================================================
# Ğ“Ğ Ğ£ĞŸĞŸĞ 5: Ğ¢Ğ•Ğ¡Ğ¢Ğ« Ğ”ĞĞŸĞĞ›ĞĞ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ¥ Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ™
#==============================================================================

proc testAdditionalFunctions() =
  startTestGroup("Ğ“Ğ Ğ£ĞŸĞŸĞ 5: Ğ¢Ğ•Ğ¡Ğ¢Ğ« Ğ”ĞĞŸĞĞ›ĞĞ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ¥ Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ™")
  
  # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²
  var tokenizer = trainBPE(testCorpus, vocabSize = 150)
  
  # Ğ¢ĞµÑÑ‚ 5.1: cleanText - ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ HTML
  let htmlText = "<div>Ğ¢ĞµĞºÑÑ‚ Ñ <b>HTML</b> Ñ‚ĞµĞ³Ğ°Ğ¼Ğ¸</div>"
  let cleaned = cleanText(htmlText, removeHtml = true)
  test("5.1 cleanText - ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ HTML Ñ‚ĞµĞ³Ğ¾Ğ²",
       "<" notin cleaned and ">" notin cleaned)
  
  # Ğ¢ĞµÑÑ‚ 5.2: cleanText - ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ URL
  let urlText = "Ğ¡ÑÑ‹Ğ»ĞºĞ° https://example.com Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ"
  let noUrls = cleanText(urlText, removeUrls = true)
  test("5.2 cleanText - ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ URLs",
       "https://" notin noUrls)
  
  # Ğ¢ĞµÑÑ‚ 5.3: cleanText - ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ email
  let emailText = "ĞšĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚ test@example.com Ğ·Ğ´ĞµÑÑŒ"
  let noEmails = cleanText(emailText, removeEmails = true)
  test("5.3 cleanText - ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ email",
       "@" notin noEmails or "example.com" notin noEmails)
  
  # Ğ¢ĞµÑÑ‚ 5.4: cleanText - Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ²
  let spacesText = "ĞœĞ½Ğ¾Ğ³Ğ¾    Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ²     Ğ·Ğ´ĞµÑÑŒ"
  let normalizedSpaces = cleanText(spacesText, removeExtraWhitespace = true)
  test("5.4 cleanText - Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ²",
       "    " notin normalizedSpaces)
  
  # Ğ¢ĞµÑÑ‚ 5.5: encodeBatch
  let batchTexts = @["Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹", "Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚", "Ñ‚Ñ€ĞµÑ‚Ğ¸Ğ¹"]
  let batchEncoding = encodeBatch(tokenizer, batchTexts, maxLength = 20, padding = true)
  test("5.5 encodeBatch - ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹",
       batchEncoding.inputIds.len == 3)
  
  # Ğ¢ĞµÑÑ‚ 5.6: encodeBatch - padding
  test("5.6 encodeBatch - Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ° Ğ¿Ğ¾ÑĞ»Ğµ padding",
       batchEncoding.inputIds[0].len == batchEncoding.inputIds[1].len and
       batchEncoding.inputIds[1].len == batchEncoding.inputIds[2].len)
  
  # Ğ¢ĞµÑÑ‚ 5.7: encodeBatch - attention mask
  test("5.7 encodeBatch - ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ°Ñ attention mask",
       batchEncoding.attentionMask.len == 3)
  
  # Ğ¢ĞµÑÑ‚ 5.8: encodeWithPadding
  let paddedTokens = encodeWithPadding(tokenizer, "ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚", maxLength = 20)
  test("5.8 encodeWithPadding - Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ»Ğ¸Ğ½Ñƒ",
       paddedTokens.len == 20)
  
  # Ğ¢ĞµÑÑ‚ 5.9: maskTokens
  let originalTokens = tokenize("Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ", tokenizer)
  let (maskedTokens, labels) = maskTokens(originalTokens, tokenizer, maskProb = 0.15, seed = 42)
  test("5.9 maskTokens - Ğ´Ğ»Ğ¸Ğ½Ñ‹ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°ÑÑ‚",
       maskedTokens.len == labels.len and labels.len == originalTokens.len)
  
  # Ğ¢ĞµÑÑ‚ 5.10: maskTokens - ĞµÑÑ‚ÑŒ Ğ·Ğ°Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹
  var hasMasked = false
  for i in 0..<maskedTokens.len:
    if labels[i] != -100:
      hasMasked = true
      break
  test("5.10 maskTokens - Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ·Ğ°Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹", hasMasked)
  
  # Ğ¢ĞµÑÑ‚ 5.11: getSubwordBreakdown
  let breakdown = getSubwordBreakdown("Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğµ ÑĞ»Ğ¾Ğ²Ğ¾", tokenizer)
  test("5.11 getSubwordBreakdown - Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´ÑĞ»Ğ¾Ğ²Ğ°",
       breakdown.len > 0)
  
  # Ğ¢ĞµÑÑ‚ 5.12: estimateTokenCount
  let estimated = estimateTokenCount("ÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²")
  test("5.12 estimateTokenCount - Ñ€Ğ°Ğ·ÑƒĞ¼Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°",
       estimated > 0 and estimated < 100)
  
  # Ğ¢ĞµÑÑ‚ 5.13: validateTokenizer
  let validationResults = validateTokenizer(tokenizer)
  test("5.13 validateTokenizer - Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚",
       validationResults.len > 0)
  
  # Ğ¢ĞµÑÑ‚ 5.14: compareTokenizers
  var wpTokenizer = trainWordPiece(testCorpus, vocabSize = 150)
  let comparison = compareTokenizers(@[tokenizer, wpTokenizer], "Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚")
  test("5.14 compareTokenizers - ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚",
       comparison.len == 2)
  
  # Ğ¢ĞµÑÑ‚ 5.15: analyzeVocabulary
  let analysis = analyzeVocabulary(tokenizer, testCorpus, topN = 5)
  test("5.15 analyzeVocabulary - Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ",
       analysis.vocabSize > 0)
  test("5.16 analyzeVocabulary - ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ´Ğ»Ğ¸Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ°",
       analysis.avgTokenLength > 0.0)
  test("5.17 analyzeVocabulary - Ñ‚Ğ¾Ğ¿ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹",
       analysis.mostFrequent.len <= 5)
  
  # Ğ¢ĞµÑÑ‚ 5.18: pruneVocabulary
  let originalSize = tokenizer.vocab.len
  pruneVocabulary(tokenizer, minFrequency = 2, corpus = testCorpus)
  test("5.18 pruneVocabulary - ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ",
       tokenizer.vocab.len <= originalSize)
  
  # Ğ¢ĞµÑÑ‚ 5.19: toLowerUnicode
  let mixedCase = "Ğ¢ĞµĞ¡Ñ‚ĞĞ²Ğ«Ğ¹ Ğ¢Ğ•ĞšĞ¡Ğ¢"
  let lowered = toLowerUnicode(mixedCase)
  test("5.19 toLowerUnicode - Ğ¿Ñ€Ğ¸Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğº Ğ½Ğ¸Ğ¶Ğ½ĞµĞ¼Ñƒ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ñƒ",
       lowered == "Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚")
  
  # Ğ¢ĞµÑÑ‚ 5.20: toUpperUnicode
  let upper = toUpperUnicode("Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚")
  test("5.20 toUpperUnicode - Ğ¿Ñ€Ğ¸Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğº Ğ²ĞµÑ€Ñ…Ğ½ĞµĞ¼Ñƒ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ñƒ",
       upper == "Ğ¢Ğ•Ğ¡Ğ¢ĞĞ’Ğ«Ğ™ Ğ¢Ğ•ĞšĞ¡Ğ¢")
  
  endTestGroup()


#==============================================================================
# Ğ“Ğ Ğ£ĞŸĞŸĞ 6: Ğ¢Ğ•Ğ¡Ğ¢Ğ« ĞšĞ­Ğ¨Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ¯ Ğ˜ ĞŸĞ ĞĞ˜Ğ—Ğ’ĞĞ”Ğ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞĞ¡Ğ¢Ğ˜
#==============================================================================

proc testCachingAndPerformance() =
  startTestGroup("Ğ“Ğ Ğ£ĞŸĞŸĞ 6: Ğ¢Ğ•Ğ¡Ğ¢Ğ« ĞšĞ­Ğ¨Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ¯ Ğ˜ ĞŸĞ ĞĞ˜Ğ—Ğ’ĞĞ”Ğ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞĞ¡Ğ¢Ğ˜")
  
  var tokenizer = trainByteLevelBPE(testCorpus, vocabSize = 200)
  tokenizer.cacheMaxSize = 100
  
  # Ğ¢ĞµÑÑ‚ 6.1: ĞšÑÑˆ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿ÑƒÑÑ‚
  test("6.1 ĞšÑÑˆ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿ÑƒÑÑ‚",
       tokenizer.cache.len == 0)
  
  # Ğ¢ĞµÑÑ‚ 6.2: ĞŸĞµÑ€Ğ²Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ - cache miss
  let text1 = "ĞŸĞµÑ€Ğ²Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ"
  discard tokenize(text1, tokenizer)
  test("6.2 ĞŸĞµÑ€Ğ²Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ cache miss",
       tokenizer.cacheMisses == 1)
  
  # Ğ¢ĞµÑÑ‚ 6.3: ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ - cache hit
  discard tokenize(text1, tokenizer)
  test("6.3 ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ cache hit",
       tokenizer.cacheHits == 1)
  
  # Ğ¢ĞµÑÑ‚ 6.4: ĞšÑÑˆ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚
  test("6.4 ĞšÑÑˆ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚",
       text1 in tokenizer.cache)
  
  # Ğ¢ĞµÑÑ‚ 6.5: ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ ĞºÑÑˆĞµĞ¼
  let testText = "Ğ¢ĞµÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºÑÑˆĞ°"
  let startNoCache = cpuTime()
  for i in 1..10:
    discard tokenize(testText & $i, tokenizer)
  let timeNoCache = cpuTime() - startNoCache
  
  # Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ñ‚Ğ¾Ñ‚ Ğ¶Ğµ Ñ‚ĞµĞºÑÑ‚ Ñ ĞºÑÑˆĞµĞ¼
  let startWithCache = cpuTime()
  for i in 1..10:
    discard tokenize(testText, tokenizer)
  let timeWithCache = cpuTime() - startWithCache
  
  test("6.5 ĞšÑÑˆ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
       timeWithCache < timeNoCache or abs(timeWithCache - timeNoCache) < 0.01)
  
  # Ğ¢ĞµÑÑ‚ 6.6: ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° ĞºÑÑˆĞ°
  clearCache(tokenizer)
  test("6.6 ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° ĞºÑÑˆĞ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚",
       tokenizer.cache.len == 0)
  
  # Ğ¢ĞµÑÑ‚ 6.7: Batch processing Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ
  let batchSize = 10
  let batchTexts = newSeq[string](batchSize)
  for i in 0..<batchSize:
    batchTexts[i] = "Ğ¢ĞµĞºÑÑ‚ Ğ½Ğ¾Ğ¼ĞµÑ€ " & $i
  
  let startBatch = cpuTime()
  let batchResult = encodeBatch(tokenizer, batchTexts, maxLength = 50)
  let batchTime = cpuTime() - startBatch
  
  test("6.7 Batch processing Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµÑ‚ÑÑ Ğ·Ğ° Ñ€Ğ°Ğ·ÑƒĞ¼Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ",
       batchTime < 1.0)
  
  # Ğ¢ĞµÑÑ‚ 6.8: ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
  let metrics = getMetrics(tokenizer, testCorpus)
  test("6.8 ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ - ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ°",
       metrics.tokensPerSecond > 0.0)
  
  endTestGroup()


#==============================================================================
# Ğ“Ğ Ğ£ĞŸĞŸĞ 7: Ğ¢Ğ•Ğ¡Ğ¢Ğ« BPE-DROPOUT Ğ˜ Ğ Ğ•Ğ“Ğ£Ğ›Ğ¯Ğ Ğ˜Ğ—ĞĞ¦Ğ˜Ğ˜
#==============================================================================

proc testDropoutAndRegularization() =
  startTestGroup("Ğ“Ğ Ğ£ĞŸĞŸĞ 7: Ğ¢Ğ•Ğ¡Ğ¢Ğ« BPE-DROPOUT Ğ˜ Ğ Ğ•Ğ“Ğ£Ğ›Ğ¯Ğ Ğ˜Ğ—ĞĞ¦Ğ˜Ğ˜")
  
  var tokenizer = trainByteLevelBPE(testCorpus, vocabSize = 200)
  let testText = "Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ dropout"
  
  # Ğ¢ĞµÑÑ‚ 7.1: ĞÑ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
  let originalTokens = tokenize(testText, tokenizer, addSpecialTokens = false)
  test("7.1 ĞÑ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚",
       originalTokens.len > 0)
  
  # Ğ¢ĞµÑÑ‚ 7.2: Dropout Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ
  let noDropout = tokenizeWithDropout(testText, tokenizer, dropoutProb = 0.0, seed = 42)
  test("7.2 Dropout Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ 0.0 Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡ĞµĞ½ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ñƒ",
       noDropout == originalTokens)
  
  # Ğ¢ĞµÑÑ‚ 7.3: Dropout ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ
  let dropout1 = tokenizeWithDropout(testText, tokenizer, dropoutProb = 0.3, seed = 1)
  let dropout2 = tokenizeWithDropout(testText, tokenizer, dropoutProb = 0.3, seed = 2)
  let dropout3 = tokenizeWithDropout(testText, tokenizer, dropoutProb = 0.3, seed = 3)
  
  test("7.3 Dropout ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (1 vs 2)",
       dropout1 != dropout2)
  test("7.4 Dropout ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (2 vs 3)",
       dropout2 != dropout3)
  
  # Ğ¢ĞµÑÑ‚ 7.5: Dropout Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹
  let dropoutMin = tokenizeWithDropout(testText, tokenizer, 
                                        dropoutProb = 0.3, seed = 10, minDropped = 2)
  test("7.5 Dropout Ñ minDropped Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚",
       dropoutMin.len >= originalTokens.len)
  
  # Ğ¢ĞµÑÑ‚ 7.6: Ğ”ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾Ğ¼ seed
  let dropout4a = tokenizeWithDropout(testText, tokenizer, dropoutProb = 0.3, seed = 100)
  let dropout4b = tokenizeWithDropout(testText, tokenizer, dropoutProb = 0.3, seed = 100)
  test("7.6 Dropout Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾Ğ¼ seed",
       dropout4a == dropout4b)
  
  # Ğ¢ĞµÑÑ‚ 7.7: Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ dropout Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²
  let decodedDropout = tokenizer.decode(dropout1, skipSpecialTokens = true)
  test("7.7 Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ dropout Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚",
       decodedDropout.len > 0)
  
  # Ğ¢ĞµÑÑ‚ 7.8: Dropout ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ¼Ñ‹ÑĞ» Ñ‚ĞµĞºÑÑ‚Ğ°
  let normalizedOriginal = testText.replace(" ", "").toLowerAscii()
  let normalizedDropout = decodedDropout.replace(" ", "").toLowerAscii()
  test("7.8 Dropout ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ ÑĞ¼Ñ‹ÑĞ» Ñ‚ĞµĞºÑÑ‚Ğ°",
       normalizedOriginal == normalizedDropout or 
       normalizedDropout.contains(normalizedOriginal[0..min(5, normalizedOriginal.len-1)]))
  
  endTestGroup()


#==============================================================================
# Ğ“Ğ Ğ£ĞŸĞŸĞ 8: Ğ¢Ğ•Ğ¡Ğ¢Ğ« Ğ¡ĞŸĞ•Ğ¦Ğ˜ĞĞ›Ğ¬ĞĞ«Ğ¥ Ğ¡Ğ›Ğ£Ğ§ĞĞ•Ğ’
#==============================================================================

proc testEdgeCases() =
  startTestGroup("Ğ“Ğ Ğ£ĞŸĞŸĞ 8: Ğ¢Ğ•Ğ¡Ğ¢Ğ« Ğ¡ĞŸĞ•Ğ¦Ğ˜ĞĞ›Ğ¬ĞĞ«Ğ¥ Ğ¡Ğ›Ğ£Ğ§ĞĞ•Ğ’")
  
  var tokenizer = trainBPE(testCorpus, vocabSize = 150)
  
  # Ğ¢ĞµÑÑ‚ 8.1: ĞŸÑƒÑÑ‚Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ°
  let emptyTokens = tokenize("", tokenizer, addSpecialTokens = false)
  test("8.1 Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ¾ĞºĞ¸",
       emptyTokens.len == 0)
  
  # Ğ¢ĞµÑÑ‚ 8.2: Ğ¡Ñ‚Ñ€Ğ¾ĞºĞ° Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ²
  let spaceTokens = tokenize("     ", tokenizer, addSpecialTokens = false)
  test("8.2 Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ²",
       spaceTokens.len >= 0)
  
  # Ğ¢ĞµÑÑ‚ 8.3: ĞÑ‡ĞµĞ½ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ°
  let longText = "ÑĞ»Ğ¾Ğ²Ğ¾ ".repeat(1000)
  let longTokens = tokenize(longText, tokenizer)
  test("8.3 Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ¾ĞºĞ¸",
       longTokens.len > 0)
  
  # Ğ¢ĞµÑÑ‚ 8.4: ĞĞ´Ğ½Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ°
  let singleChar = "Ğ°"
  let singleTokens = tokenize(singleChar, tokenizer, addSpecialTokens = false)
  test("8.4 Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ°",
       singleTokens.len > 0)
  
  # Ğ¢ĞµÑÑ‚ 8.5: Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ñ†Ğ¸Ñ„Ñ€Ñ‹
  let onlyNumbers = "1234567890"
  let numberTokens = tokenize(onlyNumbers, tokenizer, addSpecialTokens = false)
  test("8.5 Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ†Ğ¸Ñ„Ñ€",
       numberTokens.len > 0)
  
  # Ğ¢ĞµÑÑ‚ 8.6: Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ¿ĞµÑ†ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹
  let onlySpecial = "!@#$%^&*()"
  let specialTokens = tokenize(onlySpecial, tokenizer, addSpecialTokens = false)
  test("8.6 Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ¿ĞµÑ†ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²",
       specialTokens.len >= 0)
  
  # Ğ¢ĞµÑÑ‚ 8.7: Ğ¡Ğ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸ (ĞºĞ¸Ñ€Ğ¸Ğ»Ğ»Ğ¸Ñ†Ğ° + Ğ»Ğ°Ñ‚Ğ¸Ğ½Ğ¸Ñ†Ğ°)
  let mixed = "Hello Ğ¼Ğ¸Ñ€ World"
  let mixedTokens = tokenize(mixed, tokenizer)
  test("8.7 Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²",
       mixedTokens.len > 0)
  
  # Ğ¢ĞµÑÑ‚ 8.8: Unicode emoji
  let emoji = "Ğ¢ĞµĞºÑÑ‚ Ñ ğŸ˜€ emoji ğŸ‰"
  let emojiTokens = tokenize(emoji, tokenizer)
  test("8.8 Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ emoji",
       emojiTokens.len > 0)
  
  # Ğ¢ĞµÑÑ‚ 8.9: ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸ĞµÑÑ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹
  let repeated = "Ğ°Ğ°Ğ°Ğ°Ğ°Ğ°Ğ°"
  let repeatedTokens = tokenize(repeated, tokenizer, addSpecialTokens = false)
  test("8.9 Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸Ñ…ÑÑ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²",
       repeatedTokens.len > 0)
  
  # Ğ¢ĞµÑÑ‚ 8.10: Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
  let decodedEmpty = tokenizer.decode(@[], skipSpecialTokens = true)
  test("8.10 Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
       decodedEmpty == "")
  
  endTestGroup()


#==============================================================================
# Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ ĞĞĞĞ›Ğ˜Ğ— Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢ĞĞ’
#==============================================================================

proc printStatistics() =
  echo ""
  echo "â•”" & "â•".repeat(70) & "â•—"
  echo "â•‘  Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ ĞĞĞĞ›Ğ˜Ğ— Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢ĞĞ’ Ğ¢Ğ•Ğ¡Ğ¢Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ¯              â•‘"
  echo "â•š" & "â•".repeat(70) & "â•"
  echo ""
  
  var totalTests = 0
  var totalPassed = 0
  var totalFailed = 0
  var totalDuration = 0.0
  
  echo "â”Œ" & "â”€".repeat(70) & "â”"
  echo "â”‚ Ğ“Ğ Ğ£ĞŸĞŸĞ                              â”‚ ĞŸĞ ĞĞ™Ğ”Ğ•ĞĞ â”‚ ĞŸĞ ĞĞ’ĞĞ›Ğ•ĞĞ â”‚ Ğ’Ğ Ğ•ĞœĞ¯  â”‚"
  echo "â”œ" & "â”€".repeat(70) & "â”¤"
  
  for group in allGroups:
    totalTests += group.totalTests
    totalPassed += group.passedTests
    totalFailed += group.failedTests
    totalDuration += group.totalDuration
    
    let groupName = group.name[0..min(34, group.name.len-1)]
    let passedStr = $group.passedTests & "/" & $group.totalTests
    let failedStr = $group.failedTests
    let timeStr = group.totalDuration.formatFloat(ffDecimal, 3) & "s"
    
    echo "â”‚ ", groupName.alignLeft(35), " â”‚ ", 
         passedStr.alignLeft(8), " â”‚ ",
         failedStr.align(9), " â”‚ ",
         timeStr.align(6), " â”‚"
  
  echo "â””" & "â”€".repeat(70) & "â”˜"
  echo ""
  
  # ĞĞ±Ñ‰Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
  echo "ĞĞ‘Ğ©ĞĞ¯ Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ:"
  echo "  Ğ’ÑĞµĞ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¾Ğ²:        ", totalTests
  echo "  Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ¹Ğ´ĞµĞ½Ğ¾:    ", totalPassed, " (", 
       (totalPassed.float / totalTests.float * 100).formatFloat(ffDecimal, 1), "%)"
  echo "  ĞŸÑ€Ğ¾Ğ²Ğ°Ğ»ĞµĞ½Ğ¾:           ", totalFailed
  echo "  ĞĞ±Ñ‰ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ:         ", totalDuration.formatFloat(ffDecimal, 3), " ÑĞµĞº"
  echo "  Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ/Ñ‚ĞµÑÑ‚:  ", 
       (totalDuration / totalTests.float * 1000).formatFloat(ffDecimal, 2), " Ğ¼Ñ"
  echo ""
  
  # ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
  echo "ĞĞĞĞ›Ğ˜Ğ— ĞŸĞ ĞĞ˜Ğ—Ğ’ĞĞ”Ğ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞĞ¡Ğ¢Ğ˜:"
  var fastestGroup = allGroups[0]
  var slowestGroup = allGroups[0]
  
  for group in allGroups:
    let avgTime = group.totalDuration / group.totalTests.float
    let fastestAvg = fastestGroup.totalDuration / fastestGroup.totalTests.float
    let slowestAvg = slowestGroup.totalDuration / slowestGroup.totalTests.float
    
    if avgTime < fastestAvg:
      fastestGroup = group
    if avgTime > slowestAvg:
      slowestGroup = group
  
  echo "  Ğ¡Ğ°Ğ¼Ğ°Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°: ", fastestGroup.name
  echo "    Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ/Ñ‚ĞµÑÑ‚: ", 
       (fastestGroup.totalDuration / fastestGroup.totalTests.float * 1000).formatFloat(ffDecimal, 2), " Ğ¼Ñ"
  echo ""
  echo "  Ğ¡Ğ°Ğ¼Ğ°Ñ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°: ", slowestGroup.name
  echo "    Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ/Ñ‚ĞµÑÑ‚: ", 
       (slowestGroup.totalDuration / slowestGroup.totalTests.float * 1000).formatFloat(ffDecimal, 2), " Ğ¼Ñ"
  echo ""
  
  # Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ‚ÑƒÑ
  if totalFailed == 0:
    echo "â•”" & "â•".repeat(70) & "â•—"
    echo "â•‘  âœ… Ğ’Ğ¡Ğ• Ğ¢Ğ•Ğ¡Ğ¢Ğ« Ğ£Ğ¡ĞŸĞ•Ğ¨ĞĞ ĞŸĞ ĞĞ™Ğ”Ğ•ĞĞ«!                              â•‘"
    echo "â•š" & "â•".repeat(70) & "â•"
  else:
    echo "â•”" & "â•".repeat(70) & "â•—"
    echo "â•‘  âš ï¸  ĞĞ‘ĞĞĞ Ğ£Ğ–Ğ•ĞĞ« ĞŸĞ ĞĞ’ĞĞ›Ğ•ĞĞĞ«Ğ• Ğ¢Ğ•Ğ¡Ğ¢Ğ«: ", totalFailed.align(27), " â•‘"
    echo "â•š" & "â•".repeat(70) & "â•"
  echo ""


#==============================================================================
# Ğ¡Ğ ĞĞ’ĞĞ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ™ ĞĞĞĞ›Ğ˜Ğ— Ğ¢ĞĞšĞ•ĞĞ˜Ğ—ĞĞ¢ĞĞ ĞĞ’
#==============================================================================

proc comparativeAnalysis() =
  echo ""
  echo "â•”" & "â•".repeat(70) & "â•—"
  echo "â•‘  Ğ¡Ğ ĞĞ’ĞĞ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ™ ĞĞĞĞ›Ğ˜Ğ— Ğ¢ĞĞšĞ•ĞĞ˜Ğ—ĞĞ¢ĞĞ ĞĞ’                          â•‘"
  echo "â•š" & "â•".repeat(70) & "â•"
  echo ""
  
  # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ²ÑĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹
  echo "â†’ ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ²..."
  var bpeTokenizer = trainBPE(testCorpus, vocabSize = 150)
  var wpTokenizer = trainWordPiece(testCorpus, vocabSize = 150)
  var spTokenizer = trainSentencePiece(testCorpus, vocabSize = 150)
  var blbpeTokenizer = trainByteLevelBPE(testCorpus, vocabSize = 200)
  
  let testText = "ĞºĞ½ÑĞ³Ğ¸Ğ½Ñ Ğ¡Ğ¾Ñ„ÑŒÑ Ğ’Ğ°ÑĞ¸Ğ»ÑŒĞµĞ²Ğ½Ğ° Ğ±Ñ‹Ğ»Ğ° Ñ…ÑƒĞ´Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ°Ñ Ğ¶ĞµĞ½Ñ‰Ğ¸Ğ½Ğ°"
  
  echo ""
  echo "Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚: ", testText
  echo ""
  
  # Ğ¡Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
  echo "Ğ¡Ğ ĞĞ’ĞĞ•ĞĞ˜Ğ• Ğ¢ĞĞšĞ•ĞĞ˜Ğ—ĞĞ¦Ğ˜Ğ˜:"
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  
  let bpeTokens = tokenize(testText, bpeTokenizer, addSpecialTokens = false)
  echo "BPE:            ", bpeTokens.len, " Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²"
  echo "  Ğ¢Ğ¾ĞºĞµĞ½Ñ‹: ", bpeTokens
  
  let wpTokens = tokenize(testText, wpTokenizer, addSpecialTokens = false)
  echo ""
  echo "WordPiece:      ", wpTokens.len, " Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²"
  echo "  Ğ¢Ğ¾ĞºĞµĞ½Ñ‹: ", wpTokens
  
  let spTokens = tokenize(testText, spTokenizer, addSpecialTokens = false)
  echo ""
  echo "SentencePiece:  ", spTokens.len, " Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²"
  echo "  Ğ¢Ğ¾ĞºĞµĞ½Ñ‹: ", spTokens
  
  let blbpeTokens = tokenize(testText, blbpeTokenizer, addSpecialTokens = false)
  echo ""
  echo "ByteLevel BPE:  ", blbpeTokens.len, " Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²"
  echo "  Ğ¢Ğ¾ĞºĞµĞ½Ñ‹: ", blbpeTokens
  echo ""
  
  # Ğ¡Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  echo "Ğ¡Ğ ĞĞ’ĞĞ•ĞĞ˜Ğ• ĞœĞ•Ğ¢Ğ Ğ˜Ğš:"
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  
  let bpeMetrics = getMetrics(bpeTokenizer, testCorpus)
  let wpMetrics = getMetrics(wpTokenizer, testCorpus)
  let spMetrics = getMetrics(spTokenizer, testCorpus)
  let blbpeMetrics = getMetrics(blbpeTokenizer, testCorpus)
  
  echo "                    â”‚   BPE   â”‚ WordPiece â”‚ SentPiece â”‚ ByteLvlBPE"
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  echo "Ğ Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ      â”‚ ", bpeMetrics.vocabSize.align(7), " â”‚ ",
       wpMetrics.vocabSize.align(9), " â”‚ ",
       spMetrics.vocabSize.align(9), " â”‚ ",
       blbpeMetrics.vocabSize.align(10)
  
  echo "ĞšĞ¾ÑÑ„Ñ„. ÑĞ¶Ğ°Ñ‚Ğ¸Ñ       â”‚ ", 
       bpeMetrics.compressionRatio.formatFloat(ffDecimal, 2).align(7), " â”‚ ",
       wpMetrics.compressionRatio.formatFloat(ffDecimal, 2).align(9), " â”‚ ",
       spMetrics.compressionRatio.formatFloat(ffDecimal, 2).align(9), " â”‚ ",
       blbpeMetrics.compressionRatio.formatFloat(ffDecimal, 2).align(10)
  
  echo "Ğ£Ñ‚Ğ¸Ğ»Ğ¸Ğ·. ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ     â”‚ ",
       (bpeMetrics.vocabUtilization * 100).formatFloat(ffDecimal, 1).align(6), "% â”‚ ",
       (wpMetrics.vocabUtilization * 100).formatFloat(ffDecimal, 1).align(8), "% â”‚ ",
       (spMetrics.vocabUtilization * 100).formatFloat(ffDecimal, 1).align(8), "% â”‚ ",
       (blbpeMetrics.vocabUtilization * 100).formatFloat(ffDecimal, 1).align(9), "%"
  
  echo "UNK Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²         â”‚ ",
       (bpeMetrics.unkTokenRate * 100).formatFloat(ffDecimal, 1).align(6), "% â”‚ ",
       (wpMetrics.unkTokenRate * 100).formatFloat(ffDecimal, 1).align(8), "% â”‚ ",
       (spMetrics.unkTokenRate * 100).formatFloat(ffDecimal, 1).align(8), "% â”‚ ",
       (blbpeMetrics.unkTokenRate * 100).formatFloat(ffDecimal, 1).align(9), "%"
  
  echo ""
  
  # Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  echo "ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ Ğ”Ğ•ĞšĞĞ”Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ¯:"
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  
  echo "ĞÑ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»:       ", testText
  echo ""
  echo "BPE:            ", bpeTokenizer.decode(bpeTokens, skipSpecialTokens = true)
  echo "WordPiece:      ", wpTokenizer.decode(wpTokens, skipSpecialTokens = true)
  echo "SentencePiece:  ", spTokenizer.decode(spTokens, skipSpecialTokens = true)
  echo "ByteLevel BPE:  ", blbpeTokenizer.decode(blbpeTokens, skipSpecialTokens = true)
  echo ""


#==============================================================================
# Ğ“Ğ›ĞĞ’ĞĞĞ¯ Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ¯
#==============================================================================

when isMainModule:
  randomize()
  
  echo "â•”" & "â•".repeat(70) & "â•—"
  echo "â•‘  ĞšĞĞœĞŸĞ›Ğ•ĞšĞ¡ĞĞĞ• Ğ¢Ğ•Ğ¡Ğ¢Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ• Ğ‘Ğ˜Ğ‘Ğ›Ğ˜ĞĞ¢Ğ•ĞšĞ˜ Ğ¢ĞĞšĞ•ĞĞ˜Ğ—ĞĞ¦Ğ˜Ğ˜ v1.0.0      â•‘"
  echo "â•š" & "â•".repeat(70) & "â•"
  echo ""
  echo "Ğ”Ğ°Ñ‚Ğ° Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°: ", now().format("yyyy-MM-dd HH:mm:ss")
  echo "ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ: ", testCorpus.len
  echo ""
  
  let overallStart = cpuTime()
  
  # Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ²ÑĞµÑ… Ğ³Ñ€ÑƒĞ¿Ğ¿ Ñ‚ĞµÑÑ‚Ğ¾Ğ²
  testBPE()
  testWordPiece()
  testSentencePiece()
  testByteLevelBPE()
  testAdditionalFunctions()
  testCachingAndPerformance()
  testDropoutAndRegularization()
  testEdgeCases()
  
  let overallTime = cpuTime() - overallStart
  
  # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·
  printStatistics()
  
  # Ğ¡Ñ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·
  comparativeAnalysis()
  
  # Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ²Ğ¾Ğ´ĞºĞ°
  echo "â•”" & "â•".repeat(70) & "â•—"
  echo "â•‘  Ğ¢Ğ•Ğ¡Ğ¢Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ• Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•ĞĞ                                      â•‘"
  echo "â•š" & "â•".repeat(70) & "â•"
  echo ""
  echo "ĞĞ±Ñ‰ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ²ÑĞµÑ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²: ", 
       overallTime.formatFloat(ffDecimal, 3), " ÑĞµĞº"
  echo ""
