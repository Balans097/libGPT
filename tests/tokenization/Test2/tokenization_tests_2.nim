################################################################
##           –ö–û–ú–ü–õ–ï–ö–°–ù–´–ï –¢–ï–°–¢–´ –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–ò
## 
##          Comprehensive tokenization tests
## 
## –í–µ—Ä—Å–∏—è:   0.7
## –î–∞—Ç–∞:     2026-02-02
################################################################




import times, random
import std/[os, tables, strutils, unicode, json, options]
import tokenization



# nim c -d:release tokenization_tests_2.nim




#============================================================================
# –ö–û–ú–ü–õ–ï–ö–°–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ë–ò–ë–õ–ò–û–¢–ï–ö–ò –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–ò
#============================================================================

randomize()

echo "‚ïî" & "‚ïê".repeat(70) & "‚ïó"
echo "‚ïë           –ö–û–ú–ü–õ–ï–ö–°–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ë–ò–ë–õ–ò–û–¢–ï–ö–ò –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–ò            ‚ïë"
echo "‚ïö" & "‚ïê".repeat(70) & "‚ïù"
echo ""
echo "–î–∞—Ç–∞ –∑–∞–ø—É—Å–∫–∞: ", now().format("yyyy-MM-dd HH:mm:ss")
echo ""

#============================================================================
# –£–¢–ò–õ–ò–¢–´ –î–õ–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø
#============================================================================

type
  TestResult = object
    name: string
    passed: bool
    message: string
    duration: float

  TestGroup = object
    name: string
    tests: seq[TestResult]
    totalTests: int
    passedTests: int
    failedTests: int
    totalDuration: float

var allGroups: seq[TestGroup] = @[]
var currentGroup: TestGroup

proc startTestGroup(name: string) =
  currentGroup = TestGroup(
    name: name,
    tests: @[],
    totalTests: 0,
    passedTests: 0,
    failedTests: 0,
    totalDuration: 0.0
  )
  currentGroup.totalDuration = -epochTime()  # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ)
  echo ""
  echo "‚ïî" & "‚ïê".repeat(70) & "‚ïó"
  echo "‚ïë  ", alignLeft(name, 68), "‚ïë"
  echo "‚ïö" & "‚ïê".repeat(70) & "‚ïù"

proc endTestGroup() =
  currentGroup.totalDuration += epochTime()  # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è
  allGroups.add(currentGroup)
  echo ""
  echo repeat("‚îÅ", 72)
  echo "–ò—Ç–æ–≥–æ: ", currentGroup.passedTests, "/", currentGroup.totalTests, " —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–π–¥–µ–Ω–æ"
  if currentGroup.failedTests > 0:
    echo "‚ùå –ü—Ä–æ–≤–∞–ª–µ–Ω–æ: ", currentGroup.failedTests
  else:
    echo "‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–π–¥–µ–Ω—ã!"
  echo "–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: ", currentGroup.totalDuration.formatFloat(ffDecimal, 3), " —Å–µ–∫"
  echo repeat("‚îÅ", 72)

proc test(name: string, condition: bool, message: string = "") =
  let passed = condition
  
  currentGroup.totalTests += 1
  
  if passed:
    currentGroup.passedTests += 1
    echo "‚úì ", name
  else:
    currentGroup.failedTests += 1
    echo "‚úó ", name
    if message != "":
      echo "  –ü—Ä–∏—á–∏–Ω–∞: ", message
  
  currentGroup.tests.add(TestResult(
    name: name,
    passed: passed,
    message: message,
    duration: 0.0  # –ù–µ –∏–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤
  ))



#============================================================================
# –¢–ï–°–¢–û–í–´–ï –î–ê–ù–ù–´–ï
#============================================================================
const FN = "../../../examples/–¢–µ–∫—Å—Ç—ã –∏ –∫–Ω–∏–≥–∏/–ë–∞–∑–æ–≤—ã–π —Ç–µ–∫—Å—Ç.txt"
let corpus = split(readFile(FN), '\n')

const testSentences = @[
  "–°–Ω–∞—á–∞–ª–∞ –æ–Ω –≤—Å—ë-—Ç–∞–∫–∏ —Ö–æ—Ç–µ–ª —Ä–∞–∑—ã—Å–∫–∞—Ç—å –µ—ë –∏ —Ä–µ–±—ë–Ω–∫–∞.",
  """–†–µ—á—å —Ç–æ–≤–∞—Ä–∏—â–∞ –ø—Ä–æ–∫—É—Ä–æ—Ä–∞, –ø–æ –µ–≥–æ –º–Ω–µ–Ω–∏—é, –¥–æ–ª–∂–Ω–∞ –±—ã–ª–∞ –∏–º–µ—Ç—å 
–æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, –ø–æ–¥–æ–±–Ω–æ —Ç–µ–º –∑–Ω–∞–º–µ–Ω–∏—Ç—ã–º —Ä–µ—á–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –≥–æ–≤–æ—Ä–∏–ª–∏ 
—Å–¥–µ–ª–∞–≤—à–∏–µ—Å—è –∑–Ω–∞–º–µ–Ω–∏—Ç—ã–º–∏ –∞–¥–≤–æ–∫–∞—Ç—ã.""",
  "–í–µ—Å—ë–ª—ã–π –∫—É–ø–µ—Ü.",
  "–¢–µ–∫—Å—Ç —Å–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏: !@#$%^&*()",
  "Numbers: 123 456 789",
  "–í–ï–†–•–ù–ò–ô –†–ï–ì–ò–°–¢–† –ò –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä –°–º–ï—à–ê–Ω–ù—ã–ô",
  "–ü–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —Å–ª–æ–≤ —Å–ª–æ–≤ —Å–ª–æ–≤",
  "–∫–Ω—è–≥–∏–Ω—è –°–æ—Ñ—å—è –í–∞—Å–∏–ª—å–µ–≤–Ω–∞ –±—ã–ª–∞ —Ö—É–¥–∞—è –¥–ª–∏–Ω–Ω–∞—è –∂–µ–Ω—â–∏–Ω–∞"
]

#============================================================================
# –ì–†–£–ü–ü–ê 1: –¢–ï–°–¢–´ BPE
#============================================================================

proc testBPE() =
  startTestGroup("–ì–†–£–ü–ü–ê 1: –¢–ï–°–¢–´ BPE (BYTE PAIR ENCODING)")
  
  echo "\n‚Üí –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞..."
  var bpeTokenizer = trainBPE(corpus, vocabSize = 1500, minFrequency = 1)
  
  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–∞—Ä—å –≤ JSON
  exportTokenizerToJson(bpeTokenizer, "bpe_vocab.json")
  echo "‚úì –°–ª–æ–≤–∞—Ä—å BPE —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: bpe_vocab.json"
  
  test("1.1 –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è BPE",
        bpeTokenizer.vocab.len > 0 and bpeTokenizer.vocab.len <= 1500,
        "–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: " & $bpeTokenizer.vocab.len)
  
  test("1.2 –ù–∞–ª–∏—á–∏–µ PAD —Ç–æ–∫–µ–Ω–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ",
        bpeTokenizer.specialTokens.padToken in bpeTokenizer.vocab)
  test("1.3 –ù–∞–ª–∏—á–∏–µ UNK —Ç–æ–∫–µ–Ω–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ",
        bpeTokenizer.specialTokens.unkToken in bpeTokenizer.vocab)
  test("1.4 –ù–∞–ª–∏—á–∏–µ BOS —Ç–æ–∫–µ–Ω–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ",
        bpeTokenizer.specialTokens.bosToken in bpeTokenizer.vocab)
  test("1.5 –ù–∞–ª–∏—á–∏–µ EOS —Ç–æ–∫–µ–Ω–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ",
        bpeTokenizer.specialTokens.eosToken in bpeTokenizer.vocab)
  
  let testText = testSentences[0]
  let tokens = tokenize(testText, bpeTokenizer)
  test("1.6 –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–µ–ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç",
        tokens.len > 0,
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: " & $tokens.len)
  
  let decoded = bpeTokenizer.decode(tokens, skipSpecialTokens = true)
  test("1.7 –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç",
        decoded.strip() == testText or 
        decoded.replace(" ", "").toLowerAscii() == testText.replace(" ", "").toLowerAscii(),
        "–û—Ä–∏–≥–∏–Ω–∞–ª: '" & testText & "', –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: '" & decoded & "'")
  
  var vocabConsistent = true
  for token, id in bpeTokenizer.vocab:
    if id >= bpeTokenizer.inverseVocab.len or bpeTokenizer.inverseVocab[id] != token:
      vocabConsistent = false
      break
  test("1.8 –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å vocab –∏ inverseVocab", vocabConsistent)
  
  test("1.9 –ù–∞–ª–∏—á–∏–µ BPE merges",
        bpeTokenizer.merges.len > 0,
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ merges: " & $bpeTokenizer.merges.len)
  
  let savePath = "test_bpe.json"
  saveTokenizer(bpeTokenizer, savePath)
  test("1.10 –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞", fileExists(savePath))
  
  var loadedTokenizer = loadTokenizer(savePath)
  test("1.11 –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞", loadedTokenizer.vocab.len == bpeTokenizer.vocab.len)
  
  let tokensOriginal = tokenize("—Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç", bpeTokenizer)
  let tokensLoaded = tokenize("—Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç", loadedTokenizer)
  test("1.12 –ò–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏",
        tokensOriginal == tokensLoaded)
  
  let metrics = getMetrics(bpeTokenizer, corpus)
  test("1.13 –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ - —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è",
        metrics.vocabSize > 0)
  test("1.14 –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ - –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è",
        metrics.compressionRatio > 0.0 and metrics.compressionRatio < 100.0)
  
  if fileExists(savePath):
    removeFile(savePath)
  
  endTestGroup()

#============================================================================
# –ì–†–£–ü–ü–ê 2: –¢–ï–°–¢–´ WORDPIECE
#============================================================================

proc testWordPiece() =
  startTestGroup("–ì–†–£–ü–ü–ê 2: –¢–ï–°–¢–´ WORDPIECE")
  
  echo "\n‚Üí –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ WordPiece —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞..."
  var wpTokenizer = trainWordPiece(corpus, vocabSize = 1500, minFrequency = 1, preserveCase = true)
  
  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–∞—Ä—å –≤ JSON
  exportTokenizerToJson(wpTokenizer, "wordpiece_vocab.json")
  echo "‚úì –°–ª–æ–≤–∞—Ä—å WordPiece —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: wordpiece_vocab.json"
  
  test("2.1 –¢–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ WordPiece",
        wpTokenizer.kind == tkWordPiece)
  
  test("2.2 –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è WordPiece",
        wpTokenizer.vocab.len > 0 and wpTokenizer.vocab.len <= 1500)
  
  test("2.3 –ù–∞–ª–∏—á–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å–∞ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è",
        wpTokenizer.continuingSubwordPrefix == "##")
  
  test("2.4 –ù–∞–ª–∏—á–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤",
        wpTokenizer.specialTokens.padToken in wpTokenizer.vocab and
        wpTokenizer.specialTokens.unkToken in wpTokenizer.vocab)
  
  let testText = "–Ω–µ–ø–æ–Ω—è—Ç–Ω–æ–µ —Å–ª–æ–≤–æ"
  let tokens = tokenize(testText, wpTokenizer)
  test("2.5 –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç",
        tokens.len > 0)
  
  let decoded = wpTokenizer.decode(tokens, skipSpecialTokens = true)
  test("2.6 –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —É–±–∏—Ä–∞–µ—Ç ## –ø—Ä–µ—Ñ–∏–∫—Å—ã",
        "##" notin decoded,
        "–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: " & decoded)
  
  let unknownText = "qwertyzxcvb"
  let unknownTokens = tokenize(unknownText, wpTokenizer)
  test("2.7 –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤",
        unknownTokens.len > 0)
  
  let longWord = "–¥–ª–∏–Ω–Ω–æ–µ–Ω–µ–ø–æ–Ω—è—Ç–Ω–æ–µ—Å–ª–æ–≤–æ"
  let longTokens = tokenize(longWord, wpTokenizer)
  test("2.8 –î–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ —Ä–∞–∑–±–∏–≤–∞—é—Ç—Å—è –Ω–∞ –ø–æ–¥—Å–ª–æ–≤–∞",
        longTokens.len >= 1)
  
  for sentence in testSentences[0..2]:
    let encoded = tokenize(sentence, wpTokenizer)
    let redecoded = wpTokenizer.decode(encoded, skipSpecialTokens = true)
    let normalized1 = sentence.replace(" ", "").toLowerAscii()
    let normalized2 = redecoded.replace(" ", "").toLowerAscii()
    test("2.9 –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å encode-decode –¥–ª—è: " & sentence[0..min(20, sentence.len-1)],
          normalized1 == normalized2 or normalized2.contains(normalized1[0..min(5, normalized1.len-1)]))
  
  let metrics = getMetrics(wpTokenizer, corpus)
  test("2.10 –ú–µ—Ç—Ä–∏–∫–∏ - —É—Ç–∏–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤–∞—Ä—è",
        metrics.vocabUtilization >= 0.0 and metrics.vocabUtilization <= 1.0)
  
  endTestGroup()

#============================================================================
# –ì–†–£–ü–ü–ê 3: –¢–ï–°–¢–´ SENTENCEPIECE
#============================================================================

proc testSentencePiece() =
  startTestGroup("–ì–†–£–ü–ü–ê 3: –¢–ï–°–¢–´ SENTENCEPIECE")
  
  echo "\n‚Üí –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ SentencePiece —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞..."
  var spTokenizer = trainSentencePiece(corpus, vocabSize = 1500)
  
  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–∞—Ä—å –≤ JSON
  exportTokenizerToJson(spTokenizer, "sentencepiece_vocab.json")
  echo "‚úì –°–ª–æ–≤–∞—Ä—å SentencePiece —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: sentencepiece_vocab.json"
  
  test("3.1 –¢–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ SentencePiece",
        spTokenizer.kind == tkSentencePiece)
  
  test("3.2 –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è SentencePiece",
        spTokenizer.vocab.len > 0)
  
  test("3.3 –ù–∞–ª–∏—á–∏–µ scores –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤",
        spTokenizer.scores.len > 0)
  
  test("3.4 –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ —Å–ª–æ–≤–∞—Ä–µ",
        spTokenizer.specialTokens.unkToken in spTokenizer.vocab)
  
  let testText = "–¢–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ"
  let tokens = tokenize(testText, spTokenizer)
  test("3.5 –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç",
        tokens.len > 0,
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: " & $tokens.len)
  
  let decoded = spTokenizer.decode(tokens, skipSpecialTokens = true)
  test("3.6 –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç",
        decoded.len > 0)
  
  var allHaveScores = true
  for token in spTokenizer.vocab.keys:
    if token notin spTokenizer.scores:
      allHaveScores = false
      break
  test("3.7 –í—Å–µ —Ç–æ–∫–µ–Ω—ã —Å–ª–æ–≤–∞—Ä—è –∏–º–µ—é—Ç scores", allHaveScores)
  
  let textWithSpaces = "—Å–ª–æ–≤–æ –ø—Ä–æ–±–µ–ª —Å–ª–æ–≤–æ"
  let spacesTokens = tokenize(textWithSpaces, spTokenizer)
  test("3.8 –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–±–µ–ª–æ–≤",
        spacesTokens.len > 0)
  
  let original = "–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏"
  let encoded = tokenize(original, spTokenizer)
  let redecoded = spTokenizer.decode(encoded, skipSpecialTokens = true)
  let norm1 = original.replace(" ", "").toLowerAscii()
  let norm2 = redecoded.replace(" ", "").replace("‚ñÅ", "").toLowerAscii()
  test("3.9 –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å encode-decode",
        norm1 == norm2 or norm2.contains(norm1[0..min(3, norm1.len-1)]))
  
  let metrics = getMetrics(spTokenizer, corpus)
  test("3.10 –ú–µ—Ç—Ä–∏–∫–∏ - –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è",
        metrics.compressionRatio > 0.0)
  
  endTestGroup()

#============================================================================
# –ì–†–£–ü–ü–ê 4: –¢–ï–°–¢–´ BYTE-LEVEL BPE
#============================================================================

proc testByteLevelBPE() =
  startTestGroup("–ì–†–£–ü–ü–ê 4: –¢–ï–°–¢–´ BYTE-LEVEL BPE (GPT-2 STYLE)")
  
  echo "\n‚Üí –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ ByteLevel BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞..."
  var blbpeTokenizer = trainByteLevelBPE(corpus, vocabSize = 2000)
  
  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–∞—Ä—å –≤ JSON
  exportTokenizerToJson(blbpeTokenizer, "bytelevelbpe_vocab.json")
  echo "‚úì –°–ª–æ–≤–∞—Ä—å ByteLevelBPE —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: bytelevelbpe_vocab.json"
  
  test("4.1 –¢–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ByteLevelBPE",
        blbpeTokenizer.kind == tkByteLevelBPE)
  
  test("4.2 –ù–∞–ª–∏—á–∏–µ byte encoder",
        blbpeTokenizer.byteEncoder.len == 256)
  
  test("4.3 –ù–∞–ª–∏—á–∏–µ byte decoder",
        blbpeTokenizer.byteDecoder.len == 256)
  
  var encoderDecoderConsistent = true
  for b, s in blbpeTokenizer.byteEncoder:
    if blbpeTokenizer.byteDecoder[s] != b:
      encoderDecoderConsistent = false
      break
  test("4.4 –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å byte encoder/decoder", encoderDecoderConsistent)
  
  let testText = "–∫–Ω—è–≥–∏–Ω—è –°–æ—Ñ—å—è –í–∞—Å–∏–ª—å–µ–≤–Ω–∞"
  let tokens = tokenize(testText, blbpeTokenizer, addSpecialTokens = false)
  test("4.5 –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è UTF-8 —Ç–µ–∫—Å—Ç–∞",
        tokens.len > 0,
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: " & $tokens.len)
  
  let decoded = blbpeTokenizer.decode(tokens, skipSpecialTokens = true)
  test("4.6 –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç",
        decoded == testText,
        "–û—Ä–∏–≥–∏–Ω–∞–ª: '" & testText & "', –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: '" & decoded & "'")
  
  let specialChars = "!@#$%^&*()"
  let specialTokens = tokenize(specialChars, blbpeTokenizer, addSpecialTokens = false)
  let specialDecoded = blbpeTokenizer.decode(specialTokens, skipSpecialTokens = true)
  test("4.7 –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤",
        specialDecoded == specialChars,
        "–û—Ä–∏–≥–∏–Ω–∞–ª: '" & specialChars & "', –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: '" & specialDecoded & "'")
  
  let numbers = "123456789"
  let numTokens = tokenize(numbers, blbpeTokenizer, addSpecialTokens = false)
  let numDecoded = blbpeTokenizer.decode(numTokens, skipSpecialTokens = true)
  test("4.8 –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∏—Å–µ–ª",
        numDecoded == numbers)
  
  let textWithSpaces = "—Å–ª–æ–≤–æ –ø—Ä–æ–±–µ–ª —Å–ª–æ–≤–æ"
  let spaceTokens = tokenize(textWithSpaces, blbpeTokenizer, addSpecialTokens = false)
  let spaceDecoded = blbpeTokenizer.decode(spaceTokens, skipSpecialTokens = true)
  test("4.9 –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤",
        spaceDecoded == textWithSpaces,
        "–û—Ä–∏–≥–∏–Ω–∞–ª: '" & textWithSpaces & "', –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: '" & spaceDecoded & "'")
  
  let offsets = tokenizeWithOffsets(testText, blbpeTokenizer, addSpecialTokens = false)
  test("4.10 –ì–µ–Ω–µ—Ä–∞—Ü–∏—è token offsets",
        offsets.len > 0)
  
  if offsets.len > 0:
    var offsetsCorrect = true
    for offset in offsets:
      if offset.startChar < 0 or offset.endChar > testText.runeLen or 
          offset.startChar >= offset.endChar:
        offsetsCorrect = false
        break
    test("4.11 –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å char offsets", offsetsCorrect)
  
  if offsets.len > 0:
    var byteOffsetsCorrect = true
    for offset in offsets:
      if offset.startByte < 0 or offset.endByte > testText.len or
          offset.startByte >= offset.endByte:
        byteOffsetsCorrect = false
        break
    test("4.12 –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å byte offsets", byteOffsetsCorrect)
  
  endTestGroup()

#============================================================================
# –ì–†–£–ü–ü–ê 5: –¢–ï–°–¢–´ –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –§–£–ù–ö–¶–ò–ô
#============================================================================

proc testAdditionalFunctions() =
  startTestGroup("–ì–†–£–ü–ü–ê 5: –¢–ï–°–¢–´ –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –§–£–ù–ö–¶–ò–ô")
  
  var tokenizer = trainBPE(corpus, vocabSize = 1500)
  
  let htmlText = "<div>–¢–µ–∫—Å—Ç —Å <b>HTML</b> —Ç–µ–≥–∞–º–∏</div>"
  let cleaned = cleanText(htmlText, removeHtml = true)
  test("5.1 cleanText - —É–¥–∞–ª–µ–Ω–∏–µ HTML —Ç–µ–≥–æ–≤",
        "<" notin cleaned and ">" notin cleaned)
  
  let urlText = "–°—Å—ã–ª–∫–∞ https://example.com –≤ —Ç–µ–∫—Å—Ç–µ"
  let noUrls = cleanText(urlText, removeUrls = true)
  test("5.2 cleanText - —É–¥–∞–ª–µ–Ω–∏–µ URLs",
        "https://" notin noUrls)
  
  let emailText = "–ö–æ–Ω—Ç–∞–∫—Ç test@example.com –∑–¥–µ—Å—å"
  let noEmails = cleanText(emailText, removeEmails = true)
  test("5.3 cleanText - —É–¥–∞–ª–µ–Ω–∏–µ email",
        "@" notin noEmails or "example.com" notin noEmails)
  
  let spacesText = "–ú–Ω–æ–≥–æ    –ø—Ä–æ–±–µ–ª–æ–≤     –∑–¥–µ—Å—å"
  let normalizedSpaces = cleanText(spacesText, removeExtraWhitespace = true)
  test("5.4 cleanText - –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤",
        "    " notin normalizedSpaces)
  
  let batchTexts = @["–ø–µ—Ä–≤—ã–π", "–≤—Ç–æ—Ä–æ–π —Ç–µ–∫—Å—Ç", "—Ç—Ä–µ—Ç–∏–π"]
  let batchEncoding = encodeBatch(tokenizer, batchTexts, maxLength = 20, padding = true)
  test("5.5 encodeBatch - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π",
        batchEncoding.inputIds.len == 3)
  
  test("5.6 encodeBatch - –æ–¥–∏–Ω–∞–∫–æ–≤–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ padding",
        batchEncoding.inputIds[0].len == batchEncoding.inputIds[1].len and
        batchEncoding.inputIds[1].len == batchEncoding.inputIds[2].len)
  
  test("5.7 encodeBatch - –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è attention mask",
        batchEncoding.attentionMask.len == 3)
  
  let paddedTokens = encodeWithPadding(tokenizer, "–∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç", maxLength = 20)
  test("5.8 encodeWithPadding - —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–º–µ–µ—Ç –∑–∞–¥–∞–Ω–Ω—É—é –¥–ª–∏–Ω—É",
        paddedTokens.len == 20)
  
  let originalTokens = tokenize("—Ç–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è", tokenizer)
  let (maskedTokens, labels) = maskTokens(originalTokens, tokenizer, maskProb = 0.15, seed = 42)
  test("5.9 maskTokens - –¥–ª–∏–Ω—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç",
        maskedTokens.len == labels.len and labels.len == originalTokens.len)
  
  var hasMasked = false
  for i in 0..<maskedTokens.len:
    if labels[i] != -100:
      hasMasked = true
      break
  test("5.10 maskTokens - –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã", hasMasked)
  
  let breakdown = getSubwordBreakdown("—Ç–µ—Å—Ç–æ–≤–æ–µ —Å–ª–æ–≤–æ", tokenizer)
  test("5.11 getSubwordBreakdown - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥—Å–ª–æ–≤–∞",
        breakdown.len > 0)
  
  let estimated = estimateTokenCount("—ç—Ç–æ —Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤")
  test("5.12 estimateTokenCount - —Ä–∞–∑—É–º–Ω–∞—è –æ—Ü–µ–Ω–∫–∞",
        estimated > 0 and estimated < 100)
  
  let validationResults = validateTokenizerDetailed(tokenizer)
  test("5.13 validateTokenizerDetailed - –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ—Ö–æ–¥–∏—Ç",
        validationResults.len > 0)
  
  var wpTokenizer = trainWordPiece(corpus, vocabSize = 1500)
  let comparison = compareTokenizers("–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç", @[tokenizer, wpTokenizer])
  test("5.14 compareTokenizers - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç",
        comparison.len == 2)
  
  let analysis = analyzeVocabulary(tokenizer, corpus, topN = 5)
  test("5.15 analyzeVocabulary - —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è",
        analysis.vocabSize > 0)
  test("5.16 analyzeVocabulary - —Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∞",
        analysis.avgTokenLength > 0.0)
  test("5.17 analyzeVocabulary - —Ç–æ–ø —Ç–æ–∫–µ–Ω—ã",
        analysis.mostFrequent.len <= 5)
  
  let originalSize = tokenizer.vocab.len
  discard pruneVocabulary(tokenizer, minFrequency = 2, corpus = corpus)
  test("5.18 pruneVocabulary - —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è",
        tokenizer.vocab.len <= originalSize)
  
  let mixedCase = "–¢–µ–°—Ç–û–≤–´–π –¢–ï–ö–°–¢"
  let lowered = toLowerUnicode(mixedCase)
  test("5.19 toLowerUnicode - –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É",
        lowered == "—Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç")
  
  let upper = toUpperUnicode("—Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç")
  test("5.20 toUpperUnicode - –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –≤–µ—Ä—Ö–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É",
        upper == "–¢–ï–°–¢–û–í–´–ô –¢–ï–ö–°–¢")
  
  endTestGroup()

#============================================================================
# –ì–†–£–ü–ü–ê 6: –¢–ï–°–¢–´ –ö–≠–®–ò–†–û–í–ê–ù–ò–Ø –ò –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò
#============================================================================

proc testCachingAndPerformance() =
  startTestGroup("–ì–†–£–ü–ü–ê 6: –¢–ï–°–¢–´ –ö–≠–®–ò–†–û–í–ê–ù–ò–Ø –ò –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò")
  
  var tokenizer = trainByteLevelBPE(corpus, vocabSize = 2000)
  tokenizer.cacheMaxSize = 100
  
  test("6.1 –ö—ç—à –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –ø—É—Å—Ç",
        tokenizer.cache.len == 0)
  
  let text1 = "–ü–µ—Ä–≤–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"
  discard tokenize(text1, tokenizer)
  test("6.2 –ü–µ—Ä–≤–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞—ë—Ç cache miss",
        tokenizer.cacheMisses == 1)
  
  discard tokenize(text1, tokenizer)
  test("6.3 –ü–æ–≤—Ç–æ—Ä–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞—ë—Ç cache hit",
        tokenizer.cacheHits == 1)
  
  test("6.4 –ö—ç—à —Å–æ–¥–µ—Ä–∂–∏—Ç —ç–ª–µ–º–µ–Ω—Ç",
        (text1 & "0" & "false") in tokenizer.cache)
  
  let testText = "–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫—ç—à–∞"
  let startNoCache = cpuTime()
  for i in 1..10:
    discard tokenize(testText & $i, tokenizer)
  let timeNoCache = cpuTime() - startNoCache
  
  let startWithCache = cpuTime()
  for i in 1..10:
    discard tokenize(testText, tokenizer)
  let timeWithCache = cpuTime() - startWithCache
  
  test("6.5 –ö—ç—à —É—Å–∫–æ—Ä—è–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏",
        timeWithCache < timeNoCache or abs(timeWithCache - timeNoCache) < 0.01)
  
  clearCache(tokenizer)
  test("6.6 –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç",
        tokenizer.cache.len == 0)
  
  let batchSize = 10
  var batchTexts = newSeq[string](batchSize)
  for i in 0..<batchSize:
    batchTexts[i] = "–¢–µ–∫—Å—Ç –Ω–æ–º–µ—Ä " & $i
  
  let startBatch = cpuTime()
  let batchResult = encodeBatch(tokenizer, batchTexts, maxLength = 50)
  let batchTime = cpuTime() - startBatch
  
  test("6.7 Batch processing –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è –∑–∞ —Ä–∞–∑—É–º–Ω–æ–µ –≤—Ä–µ–º—è",
        batchTime < 1.0)
  
  let metrics = getMetrics(tokenizer, corpus)
  test("6.8 –ú–µ—Ç—Ä–∏–∫–∏ - —Å–∫–æ—Ä–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑–º–µ—Ä–µ–Ω–∞",
        metrics.tokensPerSecond > 0.0)
  
  endTestGroup()

#============================================================================
# –ì–†–£–ü–ü–ê 7: –¢–ï–°–¢–´ BPE-DROPOUT –ò –†–ï–ì–£–õ–Ø–†–ò–ó–ê–¶–ò–ò
#============================================================================

proc testDropoutAndRegularization() =
  startTestGroup("–ì–†–£–ü–ü–ê 7: –¢–ï–°–¢–´ BPE-DROPOUT –ò –†–ï–ì–£–õ–Ø–†–ò–ó–ê–¶–ò–ò")
  
  var tokenizer = trainByteLevelBPE(corpus, vocabSize = 2000)
  let testText = "–¢–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ dropout"
  
  let originalTokens = tokenize(testText, tokenizer, addSpecialTokens = false)
  test("7.1 –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç",
        originalTokens.len > 0)
  
  let noDropout = tokenizeWithDropout(testText, tokenizer, dropoutProb = 0.0, seed = 42)
  test("7.2 Dropout —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é 0.0 –∏–¥–µ–Ω—Ç–∏—á–µ–Ω –æ—Ä–∏–≥–∏–Ω–∞–ª—É",
        noDropout == originalTokens)
  
  let dropout1 = tokenizeWithDropout(testText, tokenizer, dropoutProb = 0.3, seed = 1)
  let dropout2 = tokenizeWithDropout(testText, tokenizer, dropoutProb = 0.3, seed = 2)
  let dropout3 = tokenizeWithDropout(testText, tokenizer, dropoutProb = 0.3, seed = 3)
  
  test("7.3 Dropout —Å–æ–∑–¥–∞—ë—Ç —Ä–∞–∑–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (1 vs 2)",
        dropout1 != dropout2)
  test("7.4 Dropout —Å–æ–∑–¥–∞—ë—Ç —Ä–∞–∑–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (2 vs 3)",
        dropout2 != dropout3)
  
  let dropoutMin = tokenizeWithDropout(testText, tokenizer, 
                                        dropoutProb = 0.3, seed = 10, minDropped = 2)
  test("7.5 Dropout —Å minDropped —Ä–∞–±–æ—Ç–∞–µ—Ç",
        dropoutMin.len >= originalTokens.len)
  
  let dropout4a = tokenizeWithDropout(testText, tokenizer, dropoutProb = 0.3, seed = 100)
  let dropout4b = tokenizeWithDropout(testText, tokenizer, dropoutProb = 0.3, seed = 100)
  test("7.6 Dropout –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω –ø—Ä–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–º seed",
        dropout4a == dropout4b)
  
  let decodedDropout = tokenizer.decode(dropout1, skipSpecialTokens = true)
  test("7.7 –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ dropout —Ç–æ–∫–µ–Ω–æ–≤ —Ä–∞–±–æ—Ç–∞–µ—Ç",
        decodedDropout.len > 0)
  
  let normalizedOriginal = testText.replace(" ", "").toLowerAscii()
  let normalizedDropout = decodedDropout.replace(" ", "").toLowerAscii()
  test("7.8 Dropout —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Å–º—ã—Å–ª —Ç–µ–∫—Å—Ç–∞",
        normalizedOriginal == normalizedDropout or 
        normalizedDropout.contains(normalizedOriginal[0..min(5, normalizedOriginal.len-1)]))
  
  endTestGroup()

#============================================================================
# –ì–†–£–ü–ü–ê 8: –¢–ï–°–¢–´ –°–ü–ï–¶–ò–ê–õ–¨–ù–´–• –°–õ–£–ß–ê–ï–í
#============================================================================

proc testEdgeCases() =
  startTestGroup("–ì–†–£–ü–ü–ê 8: –¢–ï–°–¢–´ –°–ü–ï–¶–ò–ê–õ–¨–ù–´–• –°–õ–£–ß–ê–ï–í")
  
  var tokenizer = trainBPE(corpus, vocabSize = 1500)
  
  let emptyTokens = tokenize("", tokenizer, addSpecialTokens = false)
  test("8.1 –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–∏",
        emptyTokens.len == 0)
  
  let spaceTokens = tokenize("     ", tokenizer, addSpecialTokens = false)
  test("8.2 –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–æ–∫–∏ –∏–∑ –ø—Ä–æ–±–µ–ª–æ–≤",
        spaceTokens.len >= 0)
  
  let longText = "—Å–ª–æ–≤–æ ".repeat(1000)
  let longTokens = tokenize(longText, tokenizer)
  test("8.3 –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω–æ–π —Å—Ç—Ä–æ–∫–∏",
        longTokens.len > 0)
  
  let singleChar = "–∞"
  let singleTokens = tokenize(singleChar, tokenizer, addSpecialTokens = false)
  test("8.4 –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞",
        singleTokens.len > 0)
  
  let onlyNumbers = "1234567890"
  let numberTokens = tokenize(onlyNumbers, tokenizer, addSpecialTokens = false)
  test("8.5 –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä",
        numberTokens.len > 0)
  
  let onlySpecial = "!@#$%^&*()"
  let specialTokens = tokenize(onlySpecial, tokenizer, addSpecialTokens = false)
  test("8.6 –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤",
        specialTokens.len >= 0)
  
  let mixed = "Hello –º–∏—Ä World"
  let mixedTokens = tokenize(mixed, tokenizer)
  test("8.7 –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å–º–µ—à–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤",
        mixedTokens.len > 0)
  
  let emoji = "–¢–µ–∫—Å—Ç —Å üòÄ emoji üéâ"
  let emojiTokens = tokenize(emoji, tokenizer)
  test("8.8 –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å emoji",
        emojiTokens.len > 0)
  
  let repeated = "–∞–∞–∞–∞–∞–∞–∞"
  let repeatedTokens = tokenize(repeated, tokenizer, addSpecialTokens = false)
  test("8.9 –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å–∏–º–≤–æ–ª–æ–≤",
        repeatedTokens.len > 0)
  
  let decodedEmpty = tokenizer.decode(@[], skipSpecialTokens = true)
  test("8.10 –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—É—Å—Ç–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏",
        decodedEmpty == "")
  
  endTestGroup()



#============================================================================
# –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
#============================================================================

proc printStatistics() =
  echo ""
  echo "‚ïî" & "‚ïê".repeat(70) & "‚ïó"
  echo "‚ïë          –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø              ‚ïë"
  echo "‚ïö" & "‚ïê".repeat(70) & "‚ïù"
  echo ""
  
  var totalTests = 0
  var totalPassed = 0
  var totalFailed = 0
  var totalDuration = 0.0
  
  echo "‚îå" & "‚îÄ".repeat(70) & "‚îê"
  echo "‚îÇ –ì–†–£–ü–ü–ê                              ‚îÇ –ü–†–û–ô–î–ï–ù–û ‚îÇ –ü–†–û–í–ê–õ–ï–ù–û ‚îÇ  –í–†–ï–ú–Ø  ‚îÇ"
  echo "‚îú" & "‚îÄ".repeat(70) & "‚î§"
  
  for group in allGroups:
    totalTests += group.totalTests
    totalPassed += group.passedTests
    totalFailed += group.failedTests
    totalDuration += group.totalDuration
    
    let groupName = group.name[0..min(34, group.name.len-1)]
    let passedStr = $group.passedTests & "/" & $group.totalTests
    let failedStr = $group.failedTests
    let timeStr = group.totalDuration.formatFloat(ffDecimal, 3) & "s"
    
    echo "‚îÇ ", groupName.alignLeft(35), " ‚îÇ ", 
          passedStr.alignLeft(8), " ‚îÇ ",
          failedStr.align(9), " ‚îÇ ",
          timeStr.align(7), " ‚îÇ"
  
  echo "‚îî" & "‚îÄ".repeat(70) & "‚îò"
  echo ""
  
  echo "–û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:"
  echo "  –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤:        ", totalTests
  echo "  –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ–π–¥–µ–Ω–æ:    ", totalPassed, " (", 
        (totalPassed.float / totalTests.float * 100).formatFloat(ffDecimal, 1), "%)"
  echo "  –ü—Ä–æ–≤–∞–ª–µ–Ω–æ:           ", totalFailed
  echo "  –û–±—â–µ–µ –≤—Ä–µ–º—è:         ", totalDuration.formatFloat(ffDecimal, 3), " —Å–µ–∫"
  echo "  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è/—Ç–µ—Å—Ç:  ", 
        (totalDuration / totalTests.float * 1000).formatFloat(ffDecimal, 2), " –º—Å"
  echo ""
  
  echo "–ê–ù–ê–õ–ò–ó –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò:"
  var fastestGroup = allGroups[0]
  var slowestGroup = allGroups[0]
  
  for group in allGroups:
    let avgTime = group.totalDuration / group.totalTests.float
    let fastestAvg = fastestGroup.totalDuration / fastestGroup.totalTests.float
    let slowestAvg = slowestGroup.totalDuration / slowestGroup.totalTests.float
    
    if avgTime < fastestAvg:
      fastestGroup = group
    if avgTime > slowestAvg:
      slowestGroup = group
  
  echo "  –°–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è –≥—Ä—É–ø–ø–∞: ", fastestGroup.name
  echo "    –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è/—Ç–µ—Å—Ç: ", 
        (fastestGroup.totalDuration / fastestGroup.totalTests.float * 1000).formatFloat(ffDecimal, 2), " –º—Å"
  echo ""
  echo "  –°–∞–º–∞—è –º–µ–¥–ª–µ–Ω–Ω–∞—è –≥—Ä—É–ø–ø–∞: ", slowestGroup.name
  echo "    –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è/—Ç–µ—Å—Ç: ", 
        (slowestGroup.totalDuration / slowestGroup.totalTests.float * 1000).formatFloat(ffDecimal, 2), " –º—Å"
  echo ""
  
  if totalFailed == 0:
    echo "‚ïî" & "‚ïê".repeat(70) & "‚ïó"
    echo "‚ïë  ‚úÖ –í–°–ï –¢–ï–°–¢–´ –£–°–ü–ï–®–ù–û –ü–†–û–ô–î–ï–ù–´!                              ‚ïë"
    echo "‚ïö" & "‚ïê".repeat(70) & "‚ïù"
  else:
    echo "‚ïî" & "‚ïê".repeat(70) & "‚ïó"
    echo "‚ïë      ‚ö†Ô∏è  –û–ë–ù–ê–†–£–ñ–ï–ù–´ –ü–†–û–í–ê–õ–ï–ù–ù–´–ï –¢–ï–°–¢–´: ", align($totalFailed, 30), " ‚ïë"
    echo "‚ïö" & "‚ïê".repeat(70) & "‚ïù"
  echo ""

#============================================================================
# –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–û–í
#============================================================================

proc comparativeAnalysis() =
  echo ""
  echo "‚ïî" & "‚ïê".repeat(70) & "‚ïó"
  echo "‚ïë          –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–û–í                          ‚ïë"
  echo "‚ïö" & "‚ïê".repeat(70) & "‚ïù"
  echo ""
  
  echo "‚Üí –û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤..."
  var bpeTokenizer = trainBPE(corpus, vocabSize = 1500)
  var wpTokenizer = trainWordPiece(corpus, vocabSize = 1500, preserveCase = true)
  var spTokenizer = trainSentencePiece(corpus, vocabSize = 1500)
  var blbpeTokenizer = trainByteLevelBPE(corpus, vocabSize = 2000)
  
  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
  exportTokenizerToJson(bpeTokenizer, "comparative_bpe.json")
  exportTokenizerToJson(wpTokenizer, "comparative_wordpiece.json")
  exportTokenizerToJson(spTokenizer, "comparative_sentencepiece.json")
  exportTokenizerToJson(blbpeTokenizer, "comparative_bytelevelbpe.json")
  echo "‚úì –í—Å–µ —Å–ª–æ–≤–∞—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª–∞—Ö comparative_*.json"
  echo ""
  
  let testText = "–∫–Ω—è–≥–∏–Ω—è –°–æ—Ñ—å—è –í–∞—Å–∏–ª—å–µ–≤–Ω–∞ –±—ã–ª–∞ —Ö—É–¥–∞—è –¥–ª–∏–Ω–Ω–∞—è –∂–µ–Ω—â–∏–Ω–∞"
  
  echo ""
  echo "–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç: ", testText
  echo ""
  
  echo "–°–†–ê–í–ù–ï–ù–ò–ï –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–ò:"
  echo "‚îÄ" & "‚îÄ".repeat(69)
  
  let bpeTokens = tokenize(testText, bpeTokenizer, addSpecialTokens = false)
  echo "BPE:            ", bpeTokens.len, " —Ç–æ–∫–µ–Ω–æ–≤"
  echo "  –¢–æ–∫–µ–Ω—ã: ", bpeTokens
  
  let wpTokens = tokenize(testText, wpTokenizer, addSpecialTokens = false)
  echo ""
  echo "WordPiece:      ", wpTokens.len, " —Ç–æ–∫–µ–Ω–æ–≤"
  echo "  –¢–æ–∫–µ–Ω—ã: ", wpTokens
  
  let spTokens = tokenize(testText, spTokenizer, addSpecialTokens = false)
  echo ""
  echo "SentencePiece:  ", spTokens.len, " —Ç–æ–∫–µ–Ω–æ–≤"
  echo "  –¢–æ–∫–µ–Ω—ã: ", spTokens
  
  let blbpeTokens = tokenize(testText, blbpeTokenizer, addSpecialTokens = false)
  echo ""
  echo "ByteLevel BPE:  ", blbpeTokens.len, " —Ç–æ–∫–µ–Ω–æ–≤"
  echo "  –¢–æ–∫–µ–Ω—ã: ", blbpeTokens
  echo ""
  
  echo "‚îÄ" & "‚îÄ".repeat(69)
  echo "–°–†–ê–í–ù–ï–ù–ò–ï –ú–ï–¢–†–ò–ö:"
  echo "‚îÄ" & "‚îÄ".repeat(69)
  
  let bpeMetrics = getMetrics(bpeTokenizer, corpus)
  let wpMetrics = getMetrics(wpTokenizer, corpus)
  let spMetrics = getMetrics(spTokenizer, corpus)
  let blbpeMetrics = getMetrics(blbpeTokenizer, corpus)
  
  echo "                    ‚îÇ   BPE   ‚îÇ WordPiece ‚îÇ SentPiece ‚îÇ ByteLvlBPE"
  echo "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"
  echo "–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è      ‚îÇ ", align($bpeMetrics.vocabSize, 7), " ‚îÇ ",
        align($wpMetrics.vocabSize, 9), " ‚îÇ ",
        align($spMetrics.vocabSize, 9), " ‚îÇ ",
        align($blbpeMetrics.vocabSize, 10)
  
  echo "–ö–æ—ç—Ñ—Ñ. —Å–∂–∞—Ç–∏—è       ‚îÇ ", 
        bpeMetrics.compressionRatio.formatFloat(ffDecimal, 2).align(7), " ‚îÇ ",
        wpMetrics.compressionRatio.formatFloat(ffDecimal, 2).align(9), " ‚îÇ ",
        spMetrics.compressionRatio.formatFloat(ffDecimal, 2).align(9), " ‚îÇ ",
        blbpeMetrics.compressionRatio.formatFloat(ffDecimal, 2).align(10)
  
  echo "–£—Ç–∏–ª–∏–∑. —Å–ª–æ–≤–∞—Ä—è     ‚îÇ ",
        (bpeMetrics.vocabUtilization * 100).formatFloat(ffDecimal, 1).align(6), "% ‚îÇ ",
        (wpMetrics.vocabUtilization * 100).formatFloat(ffDecimal, 1).align(8), "% ‚îÇ ",
        (spMetrics.vocabUtilization * 100).formatFloat(ffDecimal, 1).align(8), "% ‚îÇ ",
        (blbpeMetrics.vocabUtilization * 100).formatFloat(ffDecimal, 1).align(9), "%"
  
  echo "UNK —Ç–æ–∫–µ–Ω–æ–≤         ‚îÇ ",
        (bpeMetrics.unkTokenRate * 100).formatFloat(ffDecimal, 1).align(6), "% ‚îÇ ",
        (wpMetrics.unkTokenRate * 100).formatFloat(ffDecimal, 1).align(8), "% ‚îÇ ",
        (spMetrics.unkTokenRate * 100).formatFloat(ffDecimal, 1).align(8), "% ‚îÇ ",
        (blbpeMetrics.unkTokenRate * 100).formatFloat(ffDecimal, 1).align(9), "%"
  
  echo ""
  
  echo "‚îÄ" & "‚îÄ".repeat(69)
  echo "–ü–†–û–í–ï–†–ö–ê –î–ï–ö–û–î–ò–†–û–í–ê–ù–ò–Ø:"
  echo "‚îÄ" & "‚îÄ".repeat(69)
  
  echo "–û—Ä–∏–≥–∏–Ω–∞–ª:       ", testText
  echo ""
  echo "BPE:            ", bpeTokenizer.decode(bpeTokens, skipSpecialTokens = true)
  echo "WordPiece:      ", wpTokenizer.decode(wpTokens, skipSpecialTokens = true)
  echo "SentencePiece:  ", spTokenizer.decode(spTokens, skipSpecialTokens = true)
  echo "ByteLevel BPE:  ", blbpeTokenizer.decode(blbpeTokens, skipSpecialTokens = true)
  echo ""

#============================================================================
# –ó–ê–ü–£–°–ö –í–°–ï–• –¢–ï–°–¢–û–í
#============================================================================

let overallStart = epochTime()

testBPE()
testWordPiece()
testSentencePiece()
testByteLevelBPE()
testAdditionalFunctions()
testCachingAndPerformance()
testDropoutAndRegularization()
testEdgeCases()

#============================================================================
# –ù–û–í–ê–Ø –ì–†–£–ü–ü–ê: –¢–ï–°–¢–´ v0.6 FEATURES
#============================================================================

proc testNewFeaturesV06() =
  startTestGroup("–ì–†–£–ü–ü–ê 9: –ù–û–í–´–ï –§–£–ù–ö–¶–ò–ò v0.6")
  
  echo "\n‚Üí –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫..."
  
  # –í–∞–ª–∏–¥–∞—Ü–∏—è
  test("9.1 –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—É—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞",
        isSome(validateInput("")),
        "–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –≤—ã–∑—ã–≤–∞—Ç—å –æ—à–∏–±–∫—É")
  
  test("9.2 –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞",
        validateInput("–Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç").isNone,
        "–ù–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é")
  
  test("9.3 –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞",
        isSome(validateInput("x".repeat(MAX_INPUT_LENGTH + 1))),
        "–°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –≤—ã–∑—ã–≤–∞—Ç—å –æ—à–∏–±–∫—É")
  
  # Advanced normalization
  echo "\n‚Üí –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏..."
  
  let textWithZeroWidth = "Hello\u200BWorld"
  let cleaned = handleZeroWidthChars(textWithZeroWidth)
  test("9.4 –£–¥–∞–ª–µ–Ω–∏–µ zero-width —Å–∏–º–≤–æ–ª–æ–≤",
        cleaned == "HelloWorld",
        "–†–µ–∑—É–ª—å—Ç–∞—Ç: " & cleaned)
  
  let textWithWhitespace = "test\t\ttext  \n\n  end"
  let normalized = normalizeWhitespaceAdvanced(textWithWhitespace)
  test("9.5 –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è whitespace (advanced)",
        normalized == "test text end",
        "–†–µ–∑—É–ª—å—Ç–∞—Ç: " & normalized)
  
  let fullNorm = fullNormalization("test\u200B\t\ttext")
  test("9.6 –ü–æ–ª–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è",
        fullNorm.len > 0 and "\u200B" notin fullNorm)
  
  # LRU Cache
  echo "\n‚Üí –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ LRU Cache..."
  
  var lruCache = newLRUCache(maxSize = 3)
  lruCache.put("key1", @[1, 2, 3])
  lruCache.put("key2", @[4, 5, 6])
  lruCache.put("key3", @[7, 8, 9])
  
  test("9.7 LRU Cache —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è",
        isSome(lruCache.get("key1")),
        "key1 –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –∫—ç—à–µ")
  
  lruCache.put("key4", @[10, 11, 12])  # –¥–æ–ª–∂–µ–Ω –≤—ã—Ç–µ—Å–Ω–∏—Ç—å —Å—Ç–∞—Ä–µ–π—à–∏–π
  
  test("9.8 LRU Cache eviction —Ä–∞–±–æ—Ç–∞–µ—Ç",
        lruCache.entries.len <= 3,
        "–†–∞–∑–º–µ—Ä –∫—ç—à–∞: " & $lruCache.entries.len)
  
  let stats = lruCache.getStats()
  test("9.9 LRU Cache —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞",
        stats.hits > 0 or stats.misses >= 0,
        "Hits: " & $stats.hits & ", Misses: " & $stats.misses)
  
  # Language detection
  echo "\n‚Üí –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–∞..."
  
  test("9.10 –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã",
        detectLanguage("–ü—Ä–∏–≤–µ—Ç –º–∏—Ä") == "cyrillic",
        "–†–µ–∑—É–ª—å—Ç–∞—Ç: " & detectLanguage("–ü—Ä–∏–≤–µ—Ç –º–∏—Ä"))
  
  test("9.11 –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª–∞—Ç–∏–Ω–∏—Ü—ã",
        detectLanguage("Hello world") == "latin",
        "–†–µ–∑—É–ª—å—Ç–∞—Ç: " & detectLanguage("Hello world"))
  
  # Incremental vocabulary
  echo "\n‚Üí –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è..."
  
  var testTokenizer = trainBPE(corpus[0..99], vocabSize = 500)
  let initialSize = testTokenizer.vocab.len
  
  testTokenizer.addTokens(@["–Ω–æ–≤—ã–π—Ç–æ–∫–µ–Ω1", "–Ω–æ–≤—ã–π—Ç–æ–∫–µ–Ω2"])
  test("9.12 –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤",
        testTokenizer.vocab.len == initialSize + 2,
        "–ë—ã–ª–æ: " & $initialSize & ", —Å—Ç–∞–ª–æ: " & $testTokenizer.vocab.len)
  
  test("9.13 –ù–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ —Å–ª–æ–≤–∞—Ä–µ",
        "–Ω–æ–≤—ã–π—Ç–æ–∫–µ–Ω1" in testTokenizer.vocab and "–Ω–æ–≤—ã–π—Ç–æ–∫–µ–Ω2" in testTokenizer.vocab)
  
  # Vocabulary alignment
  echo "\n‚Üí –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Å–ª–æ–≤–∞—Ä–µ–π..."
  
  var tokenizer1 = trainBPE(corpus[0..49], vocabSize = 300)
  var tokenizer2 = trainBPE(corpus[50..99], vocabSize = 300)
  
  let commonTokens = findCommonTokens(@[tokenizer1, tokenizer2])
  test("9.14 –ü–æ–∏—Å–∫ –æ–±—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤",
        commonTokens.len > 0,
        "–ù–∞–π–¥–µ–Ω–æ –æ–±—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤: " & $commonTokens.len)
  
  let aligned = alignVocabularies(tokenizer1, tokenizer2)
  test("9.15 –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–µ–π",
        aligned.vocab.len >= tokenizer1.vocab.len,
        "–†–∞–∑–º–µ—Ä –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è: " & $aligned.vocab.len)
  
  # OOV detection
  echo "\n‚Üí –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ OOV —Å–ª–æ–≤..."
  
  let oovWords = analyzeOOVWords(tokenizer1, "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ—Å–ª–æ–≤–æ —Ç–µ—Å—Ç")
  test("9.16 –î–µ—Ç–µ–∫—Ü–∏—è OOV —Ä–∞–±–æ—Ç–∞–µ—Ç",
        oovWords.len >= 0,
        "–ù–∞–π–¥–µ–Ω–æ OOV —Å–ª–æ–≤: " & $oovWords.len)

  # Enhanced Unicode
  echo "\n‚Üí –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö Unicode –æ–ø–µ—Ä–∞—Ü–∏–π..."
  
  test("9.17 –ü–æ–¥—Å—á—ë—Ç —Ä—É–Ω",
        runeCount("–ü—Ä–∏–≤–µ—Ç") == 6,
        "–ü–æ–¥—Å—á–∏—Ç–∞–Ω–æ —Ä—É–Ω: " & $runeCount("–ü—Ä–∏–≤–µ—Ç"))
  
  let truncated = truncateToRunes("–î–ª–∏–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Ç–µ–∫—Å—Ç–∞", 7)
  test("9.18 –û–±—Ä–µ–∑–∫–∞ –¥–æ —Ä—É–Ω",
        runeCount(truncated) <= 7,
        "–û–±—Ä–µ–∑–∞–Ω–æ –¥–æ: " & $runeCount(truncated) & " —Ä—É–Ω")
  
  # Token statistics
  echo "\n‚Üí –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤..."
  
  let tokenStats = getTokenStatistics(tokenizer1, corpus[0..49])
  test("9.19 –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤",
        tokenStats.totalTokens > 0,
        "–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: " & $tokenStats.totalTokens)
  
  test("9.20 –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤",
        tokenStats.avgLength > 0,
        "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: " & tokenStats.avgLength.formatFloat(ffDecimal, 2))
  
  endTestGroup()

testNewFeaturesV06()

let overallTime = epochTime() - overallStart

printStatistics()
comparativeAnalysis()

echo "‚ïî" & "‚ïê".repeat(70) & "‚ïó"
echo "‚ïë          –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û                                      ‚ïë"
echo "‚ïö" & "‚ïê".repeat(70) & "‚ïù"
echo ""
echo "–û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤: ", 
      overallTime.formatFloat(ffDecimal, 3), " —Å–µ–∫"
echo ""





# nim c -d:release tokenization_tests_2.nim

