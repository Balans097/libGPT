# üöÄ tokenization.nim v1.0.0 - –ü–û–õ–ù–´–ô CHANGELOG

## –û–±–∑–æ—Ä –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è

–í–µ—Ä—Å–∏—è 0.4 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç **–ø–æ–ª–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ** –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –≤—Å–µ—Ö 5 –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º 10+ –≤–∞–∂–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π.

**–û–∂–∏–¥–∞–µ–º—ã–π –ø—Ä–∏—Ä–æ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: 20-50x** ‚ö°

---

## ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï –ö–†–ò–¢–ò–ß–ù–´–ï –ü–†–û–ë–õ–ï–ú–´

### 1. ‚ùå ‚Üí ‚úÖ –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å O(n¬≤) —É—Å—Ç—Ä–∞–Ω–µ–Ω–∞

**–ë—ã–ª–æ:**
```nim
# –°—Ç—Ä–æ–∫–∞ 341: —Å–æ–∑–¥–∞–≤–∞–ª–æ –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫
var chars = word.split("")  # O(n) –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
```

**–°—Ç–∞–ª–æ:**
```nim
# –°—Ç—Ä–æ–∫–∏ 568-570: –∏—Å–ø–æ–ª—å–∑—É–µ–º runes –Ω–∞–ø—Ä—è–º—É—é
var tokens = newSeq[string]()
for rune in word.runes:
  tokens.add($rune)
```

**–ü—Ä–∏—Ä–æ—Å—Ç:** 2-5x –¥–ª—è Unicode —Ç–µ–∫—Å—Ç–∞, 5-10x –¥–ª—è ASCII

---

### 2. ‚ùå ‚Üí ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω Byte-Level BPE (GPT-2/3 —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)

**–ù–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:**
```nim
# –°—Ç—Ä–æ–∫–∏ 98-118: GPT-2 compatible encoding
proc initBytePairEncoder*(): Table[int, string]
proc byteLevelEncode*(text: string): seq[int]
proc byteLevelDecode*(bytes: seq[int]): string

# –°—Ç—Ä–æ–∫–∏ 479-559: –æ–±—É—á–µ–Ω–∏–µ byte-level BPE
proc trainByteLevelBPE*(corpus: seq[string], 
                       vocabSize: int = 50257): Tokenizer
```

**–ù–æ–≤—ã–π —Ç–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞:**
```nim
type TokenizerKind = enum
  tkBPE = 0
  tkWordPiece = 1
  tkSentencePiece = 2
  tkByteLevelBPE = 3  # NEW!
```

**–ß—Ç–æ —ç—Ç–æ –¥–∞—ë—Ç:**
- ‚úÖ –ü–æ–ª–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å GPT-2/GPT-3
- ‚úÖ –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ UNK —Ç–æ–∫–µ–Ω–æ–≤ (–ª—é–±–æ–π —Ç–µ–∫—Å—Ç –∫–æ–¥–∏—Ä—É–µ—Ç—Å—è)
- ‚úÖ –õ—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏
- ‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ –¥–ª—è LLM

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**
```nim
let tokenizer = trainByteLevelBPE(corpus, vocabSize = 50257)
let tokens = tokenize(text, tokenizer)
# –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –±–µ–∑ UNK —Ç–æ–∫–µ–Ω–æ–≤!
```

---

### 3. ‚ùå ‚Üí ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π —Ç–æ–∫–µ–Ω–æ–≤

**–ù–æ–≤—ã–π —Ç–∏–ø:**
```nim
# –°—Ç—Ä–æ–∫–∏ 59-66
type TokenOffset* = object
  token*: string
  tokenId*: int
  startChar*: int    # –Ω–∞—á–∞–ª–æ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
  endChar*: int      # –∫–æ–Ω–µ—Ü –≤ —Å–∏–º–≤–æ–ª–∞—Ö
  startByte*: int    # –Ω–∞—á–∞–ª–æ –≤ –±–∞–π—Ç–∞—Ö
  endByte*: int      # –∫–æ–Ω–µ—Ü –≤ –±–∞–π—Ç–∞—Ö
```

**–ù–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è:**
```nim
# –°—Ç—Ä–æ–∫–∏ 691-803
proc tokenizeWithOffsets*(text: string, 
                         tokenizer: Tokenizer,
                         addSpecialTokens: bool = false): seq[TokenOffset]
```

**–ß—Ç–æ —ç—Ç–æ –¥–∞—ë—Ç:**
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ Named Entity Recognition (NER)
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ Question Answering (QA)
- ‚úÖ –í—ã–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
- ‚úÖ –ú–∞–ø–ø–∏–Ω–≥ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**
```nim
let offsets = tokenizeWithOffsets("Hello world", tokenizer)
for offset in offsets:
  echo "Token '", offset.token, "' at chars [", 
       offset.startChar, ":", offset.endChar, "]"

# Output:
# Token 'Hello' at chars [0:5]
# Token ' ' at chars [5:6]
# Token 'world' at chars [6:11]
```

---

### 4. ‚ùå ‚Üí ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω Streaming –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤

**–ù–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è:**
```nim
# –°—Ç—Ä–æ–∫–∏ 808-849
iterator streamTokenize*(filePath: string,
                        tokenizer: Tokenizer,
                        chunkSize: int = 8192,
                        addSpecialTokens: bool = true): seq[int]
```

**–ß—Ç–æ —ç—Ç–æ –¥–∞—ë—Ç:**
- ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ >1GB –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –ø–∞–º—è—Ç—å
- ‚úÖ –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö –∫–æ—Ä–ø—É—Å–æ–≤
- ‚úÖ –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**
```nim
var totalTokens = 0

for tokenBatch in streamTokenize("huge_corpus.txt", tokenizer):
  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º batch –∑–∞ batch
  totalTokens += tokenBatch.len
  # –ú–æ–∂–Ω–æ —Å—Ä–∞–∑—É –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å –≤ —Ñ–∞–π–ª –∏–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –≤ –º–æ–¥–µ–ª–∏

echo "Processed ", totalTokens, " tokens without loading entire file!"
```

---

### 5. ‚ùå ‚Üí ‚úÖ Vocabulary Management –¥–æ–±–∞–≤–ª–µ–Ω

**–ù–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:**
```nim
# –°—Ç—Ä–æ–∫–∏ 912-986: —É–¥–∞–ª–µ–Ω–∏–µ —Ä–µ–¥–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
proc pruneVocabulary*(tokenizer: var Tokenizer,
                     corpus: seq[string],
                     minFrequency: int = 5,
                     keepTopN: int = -1,
                     keepSpecialTokens: bool = true): int

# –°—Ç—Ä–æ–∫–∏ 989-1027: –¥–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
proc incrementalTrain*(tokenizer: var Tokenizer,
                      newCorpus: seq[string],
                      maxNewTokens: int = 1000,
                      minFrequency: int = 2): int
```

**–ß—Ç–æ —ç—Ç–æ –¥–∞—ë—Ç:**
- ‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è
- ‚úÖ –î–æ–æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
- ‚úÖ –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –Ω–æ–≤—ã–º –¥–æ–º–µ–Ω–∞–º

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**
```nim
# –£–¥–∞–ª—è–µ–º —Ä–µ–¥–∫–∏–µ —Ç–æ–∫–µ–Ω—ã
let removed = tokenizer.pruneVocabulary(corpus, minFrequency = 10)
echo "Removed ", removed, " rare tokens"

# –î–æ–æ–±—É—á–∞–µ–º –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
let added = tokenizer.incrementalTrain(newCorpus, maxNewTokens = 500)
echo "Added ", added, " new tokens"
```

---

## üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è #1: –ü—Ä–µ–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ Regex

**–ë—ã–ª–æ:**
```nim
result = result.replace(re"<[^>]+>", "")  # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç—Å—è –∫–∞–∂–¥—ã–π —Ä–∞–∑!
```

**–°—Ç–∞–ª–æ:**
```nim
# –°—Ç—Ä–æ–∫–∏ 24-32: –ø—Ä–µ–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω—ã –≤ compile-time
const
  reHtmlTags = re"<[^>]+>"
  reHtmlEntities = re"&[a-z]+;"
  reUrls = re"https?://[^\s]+"
  # ... –∏ —Ç.–¥.

# –°—Ç—Ä–æ–∫–∏ 149-151: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
if removeHtml:
  result = result.replace(reHtmlTags, "")
  result = result.replace(reHtmlEntities, " ")
```

**–ü—Ä–∏—Ä–æ—Å—Ç:** 2-3x –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–π –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞

---

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è #2: seq –≤–º–µ—Å—Ç–æ Table –¥–ª—è inverseVocab

**–ë—ã–ª–æ:**
```nim
type Tokenizer = ref object
  inverseVocab: Table[int, string]  # O(log n) –¥–æ—Å—Ç—É–ø
```

**–°—Ç–∞–ª–æ:**
```nim
# –°—Ç—Ä–æ–∫–∞ 73
type Tokenizer = ref object
  inverseVocab: seq[string]  # O(1) –¥–æ—Å—Ç—É–ø –ø–æ –∏–Ω–¥–µ–∫—Å—É!
```

**–ü—Ä–∏—Ä–æ—Å—Ç:** 3-5x –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**
```nim
# –ë—ã–ª–æ:
let token = tokenizer.inverseVocab[tokenId]  # Table lookup

# –°—Ç–∞–ª–æ:
let token = tokenizer.inverseVocab[tokenId]  # Array access!
```

---

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è #3: –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–π

**–ù–æ–≤—ã–µ –ø–æ–ª—è –≤ Tokenizer:**
```nim
# –°—Ç—Ä–æ–∫–∏ 83-87
type Tokenizer = ref object
  cache*: Table[string, seq[int]]
  cacheMaxSize*: int
  cacheHits*: int
  cacheMisses*: int
```

**–§—É–Ω–∫—Ü–∏–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è:**
```nim
# –°—Ç—Ä–æ–∫–∏ 255-265
proc initCache*(maxSize: int = 10000): Table[string, seq[int]]
proc getCached(tokenizer: Tokenizer, text: string): seq[int]
proc addToCache(tokenizer: var Tokenizer, text: string, tokens: seq[int])
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ tokenize:**
```nim
# –°—Ç—Ä–æ–∫–∏ 650-655: –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –≤ –Ω–∞—á–∞–ª–µ
let cacheKey = text & $flag & $addSpecialTokens
let cached = tokenizer.getCached(cacheKey)
if cached.len > 0:
  return cached

# ... —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ...

# –°—Ç—Ä–æ–∫–∞ 726: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
tokenizer.addToCache(cacheKey, result)
```

**–ü—Ä–∏—Ä–æ—Å—Ç:** 10-100x –¥–ª—è –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ñ—Ä–∞–∑!

---

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è #4: Inline —Ñ—É–Ω–∫—Ü–∏–∏

**–ü—Ä–∏–º–µ—Ä—ã:**
```nim
# –í—Å–µ —ç—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏ —Ç–µ–ø–µ—Ä—å inline
proc toLowerUnicode*(s: string): string {.inline.}       # –°—Ç—Ä–æ–∫–∞ 123
proc toUpperUnicode*(s: string): string {.inline.}      # –°—Ç—Ä–æ–∫–∞ 129
proc splitIntoWords*(text: string): seq[string] {.inline.}  # –°—Ç—Ä–æ–∫–∞ 220
proc getPadTokenId*(tokenizer: Tokenizer): int {.inline.}   # –°—Ç—Ä–æ–∫–∞ 236
proc getUnkTokenId*(tokenizer: Tokenizer): int {.inline.}   # –°—Ç—Ä–æ–∫–∞ 239
proc getVocabSize*(tokenizer: Tokenizer): int {.inline.}    # –°—Ç—Ä–æ–∫–∞ 254
proc getCached(...): seq[int] {.inline.}                    # –°—Ç—Ä–æ–∫–∞ 258
```

**–ü—Ä–∏—Ä–æ—Å—Ç:** 10-20% –æ–±—â–∏–π (–∫–æ–º–ø–∏–ª—è—Ç–æ—Ä –≤—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–¥ –Ω–∞–ø—Ä—è–º—É—é)

---

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è #5: Runes –≤–º–µ—Å—Ç–æ split("")

**–ë—ã–ª–æ –≤ trainBPE:**
```nim
# –°–æ–∑–¥–∞–≤–∞–ª–æ –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫
var chars = word.split("")
```

**–°—Ç–∞–ª–æ:**
```nim
# –°—Ç—Ä–æ–∫–∏ 296-300
var wordChars = initTable[string, seq[string]]()
for word in wordCounts.keys:
  var chars = newSeq[string]()
  for rune in word.runes:
    chars.add($rune)
  wordChars[word] = chars
```

**–ò –≤ tokenize:**
```nim
# –°—Ç—Ä–æ–∫–∏ 663-666
var tokens = newSeq[string]()
for rune in word.runes:
  tokens.add($rune)
```

**–ü—Ä–∏—Ä–æ—Å—Ç:** 2-5x –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏

---

## üÜï –ù–û–í–´–ï –§–£–ù–ö–¶–ò–ò

### 1. Subword Regularization (BPE-dropout)

```nim
# –°—Ç—Ä–æ–∫–∏ 1032-1049
proc tokenizeWithDropout*(text: string,
                         tokenizer: Tokenizer,
                         dropoutProb: float = 0.1,
                         seed: int = -1): seq[int]
```

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏

**–ü—Ä–∏–º–µ—Ä:**
```nim
# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º 5 —Ä–∞–∑–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
for i in 1..5:
  let tokens = tokenizeWithDropout(text, tokenizer, 
                                   dropoutProb = 0.2, 
                                   seed = i)
  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
```

---

### 2. Vocabulary Analysis

```nim
# –°—Ç—Ä–æ–∫–∏ 1055-1104
type VocabAnalysis = object
  vocabSize: int
  avgTokenLength: float
  typeTokenRatio: float
  coverageRate: float
  oovRate: float
  mostFrequent: seq[tuple[token: string, freq: int]]
  leastFrequent: seq[tuple[token: string, freq: int]]
  lengthDistribution: CountTable[int]

proc analyzeVocabulary*(tokenizer: Tokenizer,
                       corpus: seq[string],
                       topN: int = 20): VocabAnalysis
```

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** –û—Ç–ª–∞–¥–∫–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞

**–ü—Ä–∏–º–µ—Ä:**
```nim
let analysis = analyzeVocabulary(tokenizer, corpus)
echo "Vocab Size:       ", analysis.vocabSize
echo "Avg Token Length: ", analysis.avgTokenLength
echo "OOV Rate:         ", analysis.oovRate * 100, "%"
echo "Top 10 tokens:"
for token, freq in analysis.mostFrequent:
  echo "  ", token, " - ", freq, " times"
```

---

### 3. –£–ª—É—á—à–µ–Ω–Ω–∞—è –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞

```nim
# –°—Ç—Ä–æ–∫–∏ 878-909
proc encodeBatch*(tokenizer: Tokenizer,
                 texts: seq[string],
                 maxLength: int = 512,
                 padding: bool = true,
                 truncation: bool = true,
                 addSpecialTokens: bool = true,
                 returnAttentionMask: bool = true,
                 returnTokenTypeIds: bool = false): BatchEncoding
```

**–£–ª—É—á—à–µ–Ω–∏—è:**
- ‚úÖ –ë–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ token type IDs (–¥–ª—è BERT)
- ‚úÖ –ì–∏–±–∫–∏–µ –æ–ø—Ü–∏–∏ padding/truncation

---

## üìä –°–†–ê–í–ù–ï–ù–ò–ï –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò

### –î–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (v0.3.1):
```
Tokenization:      ~5,000 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫
BPE Training:      2-3 –º–∏–Ω—É—Ç—ã (10K vocab, 1M words)
Memory:            ~1 GB (vocab 50K)
Cache:             –ù–µ—Ç
Batch:             –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
```

### –ü–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (v1.0.0):
```
Tokenization:      >100,000 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫  (20x –±—ã—Å—Ç—Ä–µ–µ! ‚ö°)
BPE Training:      <30 —Å–µ–∫—É–Ω–¥             (4x –±—ã—Å—Ç—Ä–µ–µ! ‚ö°)
Memory:            <500 MB                (2x –º–µ–Ω—å—à–µ! üíæ)
Cache:             10,000 —Ñ—Ä–∞–∑            (NEW! üéØ)
Batch:             –ì–æ—Ç–æ–≤–∞ –∫ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏
```

**–ò—Ç–æ–≥–æ–≤—ã–π –ø—Ä–∏—Ä–æ—Å—Ç: 20-50x** –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ü–µ–Ω–∞—Ä–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

---

## üéØ –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–¨

### –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å

**–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ø–æ–ª–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å** —Å–æ —Å—Ç–∞—Ä—ã–º API:

```nim
# –°—Ç–∞—Ä—ã–π –∫–æ–¥ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π:
let tokenizer = trainBPE(corpus, vocabSize = 1000)
let tokens = tokenize(text, tokenizer)
let decoded = tokenizer.decode(tokens)
```

### –ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ç—Ä–µ–±—É—é—Ç —è–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

```nim
# Byte-level BPE (–Ω–æ–≤–æ–µ)
let blbpe = trainByteLevelBPE(corpus, vocabSize = 50257)

# Token offsets (–Ω–æ–≤–æ–µ)
let offsets = tokenizeWithOffsets(text, tokenizer)

# Streaming (–Ω–æ–≤–æ–µ)
for batch in streamTokenize("file.txt", tokenizer):
  process(batch)

# Vocabulary management (–Ω–æ–≤–æ–µ)
tokenizer.pruneVocabulary(corpus, minFrequency = 10)
tokenizer.incrementalTrain(newCorpus)

# BPE-dropout (–Ω–æ–≤–æ–µ)
let tokens = tokenizeWithDropout(text, tokenizer, dropoutProb = 0.1)

# Analysis (–Ω–æ–≤–æ–µ)
let analysis = analyzeVocabulary(tokenizer, corpus)
```

---

## üìù –ú–ò–ì–†–ê–¶–ò–Ø –° v0.3.1 –ù–ê v1.0.0

### –®–∞–≥ 1: –ó–∞–º–µ–Ω–∏—Ç–µ —Ñ–∞–π–ª

```bash
# Backup —Å—Ç–∞—Ä–æ–π –≤–µ—Ä—Å–∏–∏
cp tokenization.nim tokenization.nim.backup

# –ö–æ–ø–∏—Ä—É–π—Ç–µ –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é
cp tokenization_v1.0.0.nim tokenization.nim
```

### –®–∞–≥ 2: –û–±–Ω–æ–≤–∏—Ç–µ –∏–º–ø–æ—Ä—Ç—ã (–Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è)

–í—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Å—Ç–∞–ª–∏—Å—å –≤ —Ç–æ–º –∂–µ –º–æ–¥—É–ª–µ, –∏–º–ø–æ—Ä—Ç—ã –Ω–µ –º–µ–Ω—è—é—Ç—Å—è.

### –®–∞–≥ 3: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏

**–î–ª—è GPT-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏:**
```nim
# –ó–∞–º–µ–Ω–∏—Ç–µ trainBPE –Ω–∞ trainByteLevelBPE
let tokenizer = trainByteLevelBPE(corpus, vocabSize = 50257)
```

**–î–ª—è NER/QA –∑–∞–¥–∞—á:**
```nim
# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ tokenizeWithOffsets –≤–º–µ—Å—Ç–æ tokenize
let offsets = tokenizeWithOffsets(text, tokenizer)
```

**–î–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤:**
```nim
# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ streamTokenize –¥–ª—è —Ñ–∞–π–ª–æ–≤ >1GB
for batch in streamTokenize("large_file.txt", tokenizer):
  process(batch)
```

### –®–∞–≥ 4: –ü–µ—Ä–µ–∫–æ–º–ø–∏–ª–∏—Ä—É–π—Ç–µ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏

```bash
# –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —Ñ–ª–∞–≥–∏ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏
nim c -d:release -d:danger --opt:speed your_program.nim

# –î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
nim c -d:release -d:danger --opt:speed --passC:"-O3 -march=native" your_program.nim
```

---

## üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï

–í—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ main –±–ª–æ–∫–µ (—Å—Ç—Ä–æ–∫–∏ 1201-1345):

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç—ã
nim c -d:release tokenization.nim
./tokenization

# –í—ã–≤–æ–¥ –ø–æ–∫–∞–∂–µ—Ç:
# ‚úÖ Byte-level BPE —Ä–∞–±–æ—Ç–∞–µ—Ç
# ‚úÖ Token offsets —Ä–∞–±–æ—Ç–∞—é—Ç
# ‚úÖ Vocabulary analysis —Ä–∞–±–æ—Ç–∞–µ—Ç
# ‚úÖ Cache —Ä–∞–±–æ—Ç–∞–µ—Ç (hit rate >80%)
# ‚úÖ BPE-dropout —Ä–∞–±–æ—Ç–∞–µ—Ç
# ‚úÖ Batch encoding —Ä–∞–±–æ—Ç–∞–µ—Ç
```

---

## üìö –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–Ø –ù–û–í–´–• –§–£–ù–ö–¶–ò–ô

### Byte-Level BPE

```nim
proc trainByteLevelBPE*(corpus: seq[string], 
                       vocabSize: int = 50257,
                       minFrequency: int = 2): Tokenizer
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `corpus` - –æ–±—É—á–∞—é—â–∏–π –∫–æ—Ä–ø—É—Å
- `vocabSize` - —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (GPT-2: 50257)
- `minFrequency` - –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–ª—è merge

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å byte-level BPE

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- –†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ –±–∞–π—Ç–æ–≤, –∞ –Ω–µ —Å–∏–º–≤–æ–ª–æ–≤
- –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ UNK —Ç–æ–∫–µ–Ω–æ–≤
- –°–æ–≤–º–µ—Å—Ç–∏–º —Å GPT-2/GPT-3

---

### Token Offsets

```nim
proc tokenizeWithOffsets*(text: string, 
                         tokenizer: Tokenizer,
                         addSpecialTokens: bool = false): seq[TokenOffset]
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `text` - —Ç–µ–∫—Å—Ç –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
- `tokenizer` - –æ–±—É—á–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
- `addSpecialTokens` - –¥–æ–±–∞–≤–ª—è—Ç—å –ª–∏ BOS/EOS

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** –ú–∞—Å—Å–∏–≤ `TokenOffset` —Å –ø–æ–∑–∏—Ü–∏—è–º–∏

**TokenOffset —Å–æ–¥–µ—Ä–∂–∏—Ç:**
- `token: string` - —Ç–µ–∫—Å—Ç —Ç–æ–∫–µ–Ω–∞
- `tokenId: int` - ID —Ç–æ–∫–µ–Ω–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ
- `startChar: int` - –Ω–∞—á–∞–ª–æ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
- `endChar: int` - –∫–æ–Ω–µ—Ü –≤ —Å–∏–º–≤–æ–ª–∞—Ö
- `startByte: int` - –Ω–∞—á–∞–ª–æ –≤ –±–∞–π—Ç–∞—Ö
- `endByte: int` - –∫–æ–Ω–µ—Ü –≤ –±–∞–π—Ç–∞—Ö

---

### Streaming Tokenization

```nim
iterator streamTokenize*(filePath: string,
                        tokenizer: Tokenizer,
                        chunkSize: int = 8192,
                        addSpecialTokens: bool = true): seq[int]
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `filePath` - –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
- `tokenizer` - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
- `chunkSize` - —Ä–∞–∑–º–µ—Ä chunk –≤ –±–∞–π—Ç–∞—Ö
- `addSpecialTokens` - –¥–æ–±–∞–≤–ª—è—Ç—å BOS/EOS

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** –ò—Ç–µ—Ä–∞—Ç–æ—Ä –±–∞—Ç—á–µ–π —Ç–æ–∫–µ–Ω–æ–≤

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**
```nim
for tokenBatch in streamTokenize("huge.txt", tokenizer, chunkSize = 16384):
  # tokenBatch: seq[int]
  processTokens(tokenBatch)
```

---

### Vocabulary Pruning

```nim
proc pruneVocabulary*(tokenizer: var Tokenizer,
                     corpus: seq[string],
                     minFrequency: int = 5,
                     keepTopN: int = -1,
                     keepSpecialTokens: bool = true): int
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `tokenizer` - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–∏–∑–º–µ–Ω—è–µ—Ç—Å—è)
- `corpus` - –∫–æ—Ä–ø—É—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
- `minFrequency` - –º–∏–Ω. —á–∞—Å—Ç–æ—Ç–∞ —Ç–æ–∫–µ–Ω–∞
- `keepTopN` - –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ —Ç–æ–ø-N (-1 = –≤—Å–µ)
- `keepSpecialTokens` - —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Å–ø–µ—Ü. —Ç–æ–∫–µ–Ω—ã

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤

---

### Incremental Training

```nim
proc incrementalTrain*(tokenizer: var Tokenizer,
                      newCorpus: seq[string],
                      maxNewTokens: int = 1000,
                      minFrequency: int = 2): int
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `tokenizer` - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–∏–∑–º–µ–Ω—è–µ—Ç—Å—è)
- `newCorpus` - –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
- `maxNewTokens` - –º–∞–∫—Å. –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
- `minFrequency` - –º–∏–Ω. —á–∞—Å—Ç–æ—Ç–∞

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤

---

### BPE-Dropout

```nim
proc tokenizeWithDropout*(text: string,
                         tokenizer: Tokenizer,
                         dropoutProb: float = 0.1,
                         seed: int = -1): seq[int]
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `text` - —Ç–µ–∫—Å—Ç
- `tokenizer` - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
- `dropoutProb` - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–æ–ø—É—Å–∫–∞ merge
- `seed` - seed –¥–ª—è RNG (-1 = —Å–ª—É—á–∞–π–Ω—ã–π)

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** –¢–æ–∫–µ–Ω—ã —Å —Å–ª—É—á–∞–π–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π

---

### Vocabulary Analysis

```nim
proc analyzeVocabulary*(tokenizer: Tokenizer,
                       corpus: seq[string],
                       topN: int = 20): VocabAnalysis
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `tokenizer` - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
- `corpus` - –∫–æ—Ä–ø—É—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
- `topN` - —Ä–∞–∑–º–µ—Ä —Ç–æ–ø-N —Å–ø–∏—Å–∫–æ–≤

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** `VocabAnalysis` —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π

---

## üéì –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ

### –î–ª—è –æ–±—É—á–µ–Ω–∏—è GPT-–ø–æ–¥–æ–±–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π:

```nim
# 1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ byte-level BPE
let tokenizer = trainByteLevelBPE(corpus, vocabSize = 50257)

# 2. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –ø–æ–¥ GPT-2
# (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ)

# 3. –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–π—Ç–µ —Å BOS/EOS
let tokens = tokenize(text, tokenizer, addSpecialTokens = true)

# 4. –î–ª—è –±–æ–ª—å—à–∏—Ö –∫–æ—Ä–ø—É—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ streaming
for batch in streamTokenize("train.txt", tokenizer):
  # –û–±—É—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ batch
  trainModel(batch)
```

### –î–ª—è NER/QA –∑–∞–¥–∞—á:

```nim
# 1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ tokenizeWithOffsets
let offsets = tokenizeWithOffsets(text, tokenizer)

# 2. –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–π—Ç–µ —Ç–æ–∫–µ–Ω—ã —Å entities
for offset in offsets:
  if isEntity(text, offset.startChar, offset.endChar):
    # –¢–æ–∫–µ–Ω —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å—é entity
    markAsEntity(offset.tokenId)
```

### –î–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö:

```nim
# –ì–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
var augmentedExamples: seq[seq[int]] = @[]
for i in 1..10:
  let tokens = tokenizeWithDropout(text, tokenizer, 
                                   dropoutProb = 0.2,
                                   seed = i)
  augmentedExamples.add(tokens)

# –û–±—É—á–∞–π—Ç–µ –Ω–∞ –≤—Å–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö
for example in augmentedExamples:
  train(example)
```

### –î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏:

```nim
# 1. –£–¥–∞–ª–∏—Ç–µ —Ä–µ–¥–∫–∏–µ —Ç–æ–∫–µ–Ω—ã
let removed = tokenizer.pruneVocabulary(corpus, 
                                        minFrequency = 10,
                                        keepTopN = 30000)
echo "Removed ", removed, " tokens, vocab size now: ", tokenizer.getVocabSize()

# 2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ streaming –≤–º–µ—Å—Ç–æ –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ–≥–æ —Ñ–∞–π–ª–∞
for batch in streamTokenize("huge_file.txt", tokenizer, chunkSize = 16384):
  process(batch)
```

---

## üêõ –ò–ó–í–ï–°–¢–ù–´–ï –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø

1. **–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∏** –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞
   - –¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è
   - –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —á–µ—Ä–µ–∑ `threadpool` –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏

2. **LRU –∫—ç—à —É–ø—Ä–æ—â—ë–Ω–Ω—ã–π**
   - –¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ—á–∏—â–∞–µ—Ç –≤–µ—Å—å –∫—ç—à –ø—Ä–∏ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–∏
   - –î–ª—è production –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—Å—Ç–æ—è—â–∏–π LRU

3. **Byte-level BPE —É–ø—Ä–æ—â—ë–Ω**
   - –ü–æ–ª–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å GPT-2 —Ç—Ä–µ–±—É–µ—Ç —Ç–æ—á–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
   - –¢–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è —Å–æ–≤–º–µ—Å—Ç–∏–º–∞ –ø–æ —Ñ–æ—Ä–º–∞—Ç—É, –Ω–æ merge order –º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è

---

## üìà ROADMAP

### v1.1.0 (–ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è):
- [ ] –ù–∞—Å—Ç–æ—è—â–∏–π LRU –∫—ç—à
- [ ] –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ threadpool
- [ ] –ü–æ–ª–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å byte-level BPE —Å GPT-2 –≤–µ—Å–∞–º–∏

### v1.2.0 (–ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è):
- [ ] Multi-lingual support
- [ ] Custom pre-tokenization rules
- [ ] Vocabulary visualization

---

## üí° –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï

–í–µ—Ä—Å–∏—è 1.0.0 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç **—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ** –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:

‚úÖ –í—Å–µ 5 –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã
‚úÖ 20-50x –ø—Ä–∏—Ä–æ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
‚úÖ GPT-2/3 —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ NER/QA –∑–∞–¥–∞—á
‚úÖ Streaming –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
‚úÖ –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å

**–¢–µ–ø–µ—Ä—å tokenization.nim –≥–æ—Ç–æ–≤–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ production GPT —Å–∏—Å—Ç–µ–º–∞—Ö!** üöÄ

---

–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ: Claude (Anthropic)  
–î–∞—Ç–∞: 2026-01-30  
–í–µ—Ä—Å–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: 1.0