# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ tokenization.nim v0.3

## –î–∞—Ç–∞: 2026-01-30
## –°—Ç–∞—Ç—É—Å: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –≤–µ—Ä—Å–∏—è (bugfix)

## –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ

–í –º–æ–¥—É–ª—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞, –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–æ–∫–µ–Ω–∞–º–∏ –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.

## ‚ö†Ô∏è –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã

### 1. –≠–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏—è—Ö
**–ü—Ä–æ–±–ª–µ–º–∞:** –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ `removePunctuation`
```nim
# –ë–´–õ–û (–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ):
result = result.replace(re"[!\"#$%&'()*+,\-./:;<=>?@\[\\\]^_`{|}~]", " ")
```

**–†–µ—à–µ–Ω–∏–µ:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π –∏ –Ω–∞–¥—ë–∂–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –±–µ–∑ regex
```nim
# –°–¢–ê–õ–û (–ø—Ä–∞–≤–∏–ª—å–Ω–æ):
const punctuation = "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~"
var cleaned = ""
for ch in result:
  if ch notin punctuation:
    cleaned.add(ch)
  else:
    cleaned.add(' ')
result = cleaned
```

### 2. –û–±—ä—è–≤–ª–µ–Ω–∏–µ Unicode —Å–∏–º–≤–æ–ª–æ–≤ –≤ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞—Ö
**–ü—Ä–æ–±–ª–µ–º–∞:** Nim –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Unicode —Å–∏–º–≤–æ–ª—ã –≤ –ª–∏—Ç–µ—Ä–∞–ª–∞—Ö `char` –Ω–∞–ø—Ä—è–º—É—é
```nim
# –ë–´–õ–û (–æ—à–∏–±–∫–∞ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏):
const accentMap = {
  '√†': 'a', '√°': 'a', ...
}.toTable
```

**–†–µ—à–µ–Ω–∏–µ:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å–æ —Å—Ç—Ä–æ–∫–∞–º–∏
```nim
# –°–¢–ê–õ–û (—Ä–∞–±–æ—Ç–∞–µ—Ç):
let accentPairs = [
  ("√†", "a"), ("√°", "a"), ("√¢", "a"), ...
]
var accentMap = initTable[string, string]()
for (accented, base) in accentPairs:
  accentMap[accented] = base
```

---

## üìã –°–ø–∏—Å–æ–∫ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π

### 1. –û—á–∏—Å—Ç–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞

#### `cleanText*(text: string, ...): string`
**–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –º—É—Å–æ—Ä–∞**

–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
- `removeHtml: bool = true` - —É–¥–∞–ª–∏—Ç—å HTML —Ç–µ–≥–∏
- `removeUrls: bool = true` - —É–¥–∞–ª–∏—Ç—å URL –∞–¥—Ä–µ—Å–∞  
- `removeEmails: bool = true` - —É–¥–∞–ª–∏—Ç—å email –∞–¥—Ä–µ—Å–∞
- `removeExtraWhitespace: bool = true` - —É–¥–∞–ª–∏—Ç—å –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
- `removeEmoji: bool = false` - —É–¥–∞–ª–∏—Ç—å emoji —Å–∏–º–≤–æ–ª—ã
- `removeNumbers: bool = false` - —É–¥–∞–ª–∏—Ç—å —Ü–∏—Ñ—Ä—ã
- `removePunctuation: bool = false` - —É–¥–∞–ª–∏—Ç—å –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
- `normalizeQuotes: bool = true` - –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–∞–≤—ã—á–∫–∏
- `normalizeDashes: bool = true` - –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–∏—Ä–µ
- `removeControlChars: bool = true` - —É–¥–∞–ª–∏—Ç—å —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã

```nim
let dirty = "–ü—Ä–∏–≤–µ—Ç! <b>HTML</b> —Ç–µ–∫—Å—Ç https://site.com üéâ"
let clean = cleanText(dirty, removeHtml = true, removeUrls = true, removeEmoji = true)
# –†–µ–∑—É–ª—å—Ç–∞—Ç: "–ü—Ä–∏–≤–µ—Ç! —Ç–µ–∫—Å—Ç"
```

#### `toUpperUnicode(s: string): string`
–ü—Ä–∏–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –∫ –≤–µ—Ä—Ö–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Unicode.

#### `splitIntoSentences*(text: string): seq[string]`
–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.

```nim
let sentences = splitIntoSentences("–ü–µ—Ä–≤–æ–µ. –í—Ç–æ—Ä–æ–µ! –¢—Ä–µ—Ç—å–µ?")
# –†–µ–∑—É–ª—å—Ç–∞—Ç: @["–ü–µ—Ä–≤–æ–µ.", "–í—Ç–æ—Ä–æ–µ!", "–¢—Ä–µ—Ç—å–µ?"]
```

#### `truncateText*(text: string, maxLength: int, addEllipsis: bool = true): string`
–û–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã.

```nim
let short = truncateText("–û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç", 10)
# –†–µ–∑—É–ª—å—Ç–∞—Ç: "–û—á–µ–Ω—å –¥..."
```

#### `removeAccents*(text: string): string`
–£–¥–∞–ª—è–µ—Ç –∞–∫—Ü–µ–Ω—Ç—ã —Å –±—É–∫–≤ (caf√© ‚Üí cafe).

```nim
echo removeAccents("caf√© r√©sum√©")
# –†–µ–∑—É–ª—å—Ç–∞—Ç: "cafe resume"
```

#### `normalizeWhitespace*(text: string, preserveNewlines: bool = false): string`
–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã.

#### `countWords*(text: string): int`
–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤.

#### `countCharacters*(text: string, excludeWhitespace: bool = false): int`
–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤.

#### `countSentences*(text: string): int`
–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.

---

### 2. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏

#### `getTokenById*(tokenizer: Tokenizer, id: int): string`
–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–∫–µ–Ω –ø–æ –µ–≥–æ ID.

```nim
let token = tokenizer.getTokenById(42)
```

#### `getIdByToken*(tokenizer: Tokenizer, token: string): int`
–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç ID —Ç–æ–∫–µ–Ω–∞.

```nim
let id = tokenizer.getIdByToken("hello")
```

#### `hasToken*(tokenizer: Tokenizer, token: string): bool`
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ç–æ–∫–µ–Ω–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ.

#### `getVocabSize*(tokenizer: Tokenizer): int`
–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è.

#### `getVocabTokens*(tokenizer: Tokenizer): seq[string]`
–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –∏–∑ —Å–ª–æ–≤–∞—Ä—è.

#### `filterTokensByFrequency*(tokenizer, text, minFrequency): seq[(string, int)]`
–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã –∏ –∏—Ö —á–∞—Å—Ç–æ—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —É–±—ã–≤–∞–Ω–∏—é.

```nim
let freqs = filterTokensByFrequency(tokenizer, corpus, minFrequency = 50)
# –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: @[("the", 1234), ("and", 890), ...]
```

#### `compareTokenizers*(text: string, tokenizers: seq[Tokenizer]): seq[...]`
–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤.

```nim
let comparison = compareTokenizers(text, @[bpe, wordpiece, sentencepiece])
# –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ
```

#### `tokenizeWithOffsets*(text, tokenizer): seq[tuple[token, start, stop]]`
–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —Å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏–π –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞.

```nim
let offsets = tokenizeWithOffsets("hello world", tokenizer)
# –†–µ–∑—É–ª—å—Ç–∞—Ç: @[(token: "hello", start: 0, stop: 5), ...]
```

#### `getSubwordBreakdown*(text: string, tokenizer: Tokenizer): seq[string]`
–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –ø–æ–¥—Å–ª–æ–≤–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.

```nim
let subwords = getSubwordBreakdown("–Ω–µ–ø–æ–Ω—è—Ç–Ω—ã–π", tokenizer)
# –†–µ–∑—É–ª—å—Ç–∞—Ç: @["–Ω–µ", "##–ø–æ–Ω—è—Ç–Ω—ã–π"] –∏–ª–∏ @["ƒ†–Ω–µ", "–ø–æ–Ω—è—Ç–Ω—ã–π"]
```

#### `estimateTokenCount*(text: string, avgCharsPerToken: float = 4.0): int`
–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –±–µ–∑ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.

```nim
let estimate = estimateTokenCount(longText, avgCharsPerToken = 3.0)
# –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
```

---

### 3. –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è

#### `encodeWithPadding*(tokenizer, text, maxLength, truncate): seq[int]`
–ö–æ–¥–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —Å –ø–∞–¥–¥–∏–Ω–≥–æ–º –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã.

```nim
let padded = encodeWithPadding(tokenizer, "–∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç", maxLength = 50)
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–∏—Ç PAD —Ç–æ–∫–µ–Ω—ã –¥–æ –¥–ª–∏–Ω—ã 50
```

#### `createAttentionMask*(tokens: seq[int], padTokenId: int): seq[int]`
–°–æ–∑–¥–∞—ë—Ç –º–∞—Å–∫—É –≤–Ω–∏–º–∞–Ω–∏—è (1 –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, 0 –¥–ª—è padding).

```nim
let mask = createAttentionMask(tokens, padTokenId)
# –†–µ–∑—É–ª—å—Ç–∞—Ç: @[1, 1, 1, 1, 0, 0, 0]
```

#### `maskTokens*(tokens, tokenizer, maskProb: float = 0.15): seq[int]`
–ú–∞—Å–∫–∏—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è Masked Language Modeling (MLM).

–°—Ç—Ä–∞—Ç–µ–≥–∏—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è (–∫–∞–∫ –≤ BERT):
- 80% - –∑–∞–º–µ–Ω–∞ –Ω–∞ [MASK]
- 10% - –∑–∞–º–µ–Ω–∞ –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã–π —Ç–æ–∫–µ–Ω
- 10% - –æ—Å—Ç–∞–≤–∏—Ç—å –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π

```nim
let masked = maskTokens(originalTokens, tokenizer, maskProb = 0.15)
# –°–ª—É—á–∞–π–Ω–æ –º–∞—Å–∫–∏—Ä—É–µ—Ç 15% —Ç–æ–∫–µ–Ω–æ–≤
```

#### `validateTokenizer*(tokenizer: Tokenizer): seq[string]`
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –ø—Ä–æ–±–ª–µ–º—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π.

```nim
let warnings = validateTokenizer(tokenizer)
for warning in warnings:
  echo warning
# –ü—Ä–æ–≤–µ—Ä—è–µ—Ç: —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è, —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –æ–±—Ä–∞—Ç–∏–º–æ—Å—Ç—å
```

---

## üéØ –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ (—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –±—É–¥—É—â–∏—Ö –≤–µ—Ä—Å–∏–π)

### –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç:

1. **–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏**
   - `tokenizeParallel()` - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–æ–ª—å—à–∏—Ö –∫–æ—Ä–ø—É—Å–æ–≤
   - `trainTokenizerParallel()` - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

2. **–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è**
   - `TokenizerStream` - –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Ç–æ–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
   - `addToVocab()` - –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è

3. **–°–ª–∏—è–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤**
   - `mergeTokenizers()` - –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
   - `transferVocab()` - –ø–µ—Ä–µ–Ω–æ—Å —Å–ª–æ–≤–∞—Ä—è –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏

4. **–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã**
   - `trainCharTokenizer()` - –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
   - `trainByteTokenizer()` - –±–∞–π—Ç–æ–≤–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (–∫–∞–∫ –≤ GPT-4)

### –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç:

5. **–ê–Ω–∞–ª–∏–∑ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è**
   - `analyzeVocabOverlap()` - –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Å–ª–æ–≤–∞—Ä–µ–π
   - `visualizeTokenDistribution()` - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
   - `findRareTokens()` - –ø–æ–∏—Å–∫ —Ä–µ–¥–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤

6. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤–∞—Ä—è**
   - `pruneVocab()` - —É–¥–∞–ª–µ–Ω–∏–µ —Ä–µ–¥–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
   - `expandVocab()` - —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –Ω–æ–≤—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏
   - `optimizeVocabSize()` - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä —Ä–∞–∑–º–µ—Ä–∞

7. **–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ä–µ–∂–∏–º—ã**
   - `trainDomainSpecific()` - –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–æ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
   - `adaptTokenizer()` - –∞–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É
   - `ensembleTokenizers()` - –∞–Ω—Å–∞–º–±–ª—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤

8. **–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å**
   - `exportToHuggingFace()` - —ç–∫—Å–ø–æ—Ä—Ç –≤ —Ñ–æ—Ä–º–∞—Ç HuggingFace
   - `loadFromSentencePieceModel()` - –∑–∞–≥—Ä—É–∑–∫–∞ SentencePiece –º–æ–¥–µ–ª–µ–π
   - `convertToFastTokenizer()` - –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –±—ã—Å—Ç—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç

### –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç:

9. **–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏**
   - `calculatePerplexity()` - —Ä–∞—Å—á—ë—Ç –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏
   - `measureSegmentationQuality()` - –∫–∞—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
   - `computeVocabEntropy()` - —ç–Ω—Ç—Ä–æ–ø–∏—è —Å–ª–æ–≤–∞—Ä—è

10. **–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏**
    - `handleCodeTokenization()` - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∫–æ–¥–∞
    - `handleMultilingualText()` - –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    - `preserveFormatting()` - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

---

## üìä –£–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –¢–µ–∫—É—â–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:

1. ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ `Table` –≤–º–µ—Å—Ç–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
2. ‚úÖ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
3. ‚úÖ –õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä –¥–∞–Ω–Ω—ã—Ö

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:

1. **–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏**
   ```nim
   type TokenizerCache = object
     cache: Table[string, seq[int]]
     maxSize: int
   ```

2. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Trie –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤**
   - –£—Å–∫–æ—Ä–∏—Ç –ø–æ–∏—Å–∫ –ø–æ–¥—Å–ª–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö —Å–ª–æ–≤–∞—Ä—è—Ö
   - –û—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –¥–ª—è WordPiece

3. **Batch processing —Å –∑–∞—Ä–∞–Ω–µ–µ –≤—ã–¥–µ–ª–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é**
   - –ò–∑–±–µ–∂–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–ª–ª–æ–∫–∞—Ü–∏–π
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `newSeqOfCap` —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º

4. **–ö–æ–º–ø–∏–ª—è—Ü–∏—è —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π**
   - –°–æ–∑–¥–∞–≤–∞—Ç—å `Regex` –æ–±—ä–µ–∫—Ç—ã –æ–¥–∏–Ω —Ä–∞–∑
   - –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ —Ñ—É–Ω–∫—Ü–∏—è—Ö –æ—á–∏—Å—Ç–∫–∏

---

## üîß –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ü—Ä–∏–º–µ—Ä 1: –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º

```nim
let rawCorpus = readFile("data.txt")

# –®–∞–≥ 1: –û—á–∏—Å—Ç–∫–∞
let cleaned = cleanText(rawCorpus,
  removeHtml = true,
  removeUrls = true,
  removeEmails = true,
  removeEmoji = true,
  normalizeQuotes = true,
  normalizeDashes = true
)

# –®–∞–≥ 2: –û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
let tokenizer = trainBPE(cleaned, vocabSize = 10000)

# –®–∞–≥ 3: –í–∞–ª–∏–¥–∞—Ü–∏—è
let warnings = validateTokenizer(tokenizer)
for w in warnings:
  echo w
```

### –ü—Ä–∏–º–µ—Ä 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è MLM

```nim
# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
let tokens = tokenize(text, tokenizer, flag = 0, addSpecialTokens = true)

# –ü–∞–¥–¥–∏–Ω–≥ –¥–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã
let padded = encodeWithPadding(tokenizer, text, maxLength = 512)

# –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è
let attention = createAttentionMask(padded, tokenizer.getPadTokenId())

# –ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è MLM
let masked = maskTokens(padded, tokenizer, maskProb = 0.15)
```

### –ü—Ä–∏–º–µ—Ä 3: –ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤

```nim
let testTexts = @["—Ç–µ–∫—Å—Ç 1", "—Ç–µ–∫—Å—Ç 2", "—Ç–µ–∫—Å—Ç 3"]

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
let comparison = compareTokenizers(testTexts.join(" "), 
                                  @[bpe, wordpiece, sentencepiece])

for comp in comparison:
  echo comp.name, ": ", comp.tokens, " —Ç–æ–∫–µ–Ω–æ–≤"
  echo "  –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: ", comp.avgLen, " —Å–∏–º–≤–æ–ª–æ–≤/—Ç–æ–∫–µ–Ω"

# –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
for tok in @[bpe, wordpiece, sentencepiece]:
  let metrics = getMetrics(tok, corpus)
  printMetrics(metrics)
```

---

## üìà –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞

### –î–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:

- ‚úÖ –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è (—Å–∏–º–≤–æ–ª–æ–≤/—Ç–æ–∫–µ–Ω)
- ‚úÖ –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —Å–ª–æ–≤–æ
- ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è (%)
- ‚úÖ –ü—Ä–æ—Ü–µ–Ω—Ç UNK —Ç–æ–∫–µ–Ω–æ–≤
- ‚úÖ –°–∫–æ—Ä–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (—Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫)

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:

- Fertility (—Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥—Å–ª–æ–≤ –Ω–∞ —Å–ª–æ–≤–æ)
- Vocabulary overlap –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –∫–æ—Ä–ø—É—Å–∞–º–∏
- Consistency score (–Ω–∞—Å–∫–æ–ª—å–∫–æ —Å—Ç–∞–±–∏–ª—å–Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è)
- Coverage (% —Å–ª–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –±–µ–∑ UNK)

---

## ‚ö†Ô∏è –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

1. **tokenizeWithOffsets** - —É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è, –º–æ–∂–µ—Ç –Ω–µ—Ç–æ—á–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏
2. **maskTokens** - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∞–∑–æ–≤—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–ª—É—á–∞–π–Ω—ã—Ö —á–∏—Å–µ–ª, –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—Ä–∏–ø—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏ —Å—Ç–æ–π–∫–∏–π
3. **removeEmoji** - –ø–æ–∫—Ä—ã–≤–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã, –Ω–æ –Ω–µ –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ emoji
4. **splitIntoSentences** - –ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞, –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –∏ –æ—Å–æ–±—ã–µ —Å–ª—É—á–∞–∏

---

## üîÑ –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å

–í—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π. –ù–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:
- –ù–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—Ç —Å–æ —Å—Ç–∞—Ä—ã–º API
- –ò—Å–ø–æ–ª—å–∑—É—é—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–µ —Å–æ–≥–ª–∞—à–µ–Ω–∏—è –æ–± –∏–º–µ–Ω–æ–≤–∞–Ω–∏–∏
- –ò–º–µ—é—Ç —Ä–∞–∑—É–º–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

---

## üì¶ –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π:
- `std/[tables, sequtils, strutils, algorithm, sets, unicode, json, os, re]`
- `math, times`

---

## ‚ú® –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–í–µ—Ä—Å–∏—è 0.3 –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥—É–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏:

1. **–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö** - –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
2. **–£—Ç–∏–ª–∏—Ç—ã** - —É–¥–æ–±–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–æ–∫–µ–Ω–∞–º–∏
3. **ML-–∑–∞–¥–∞—á–∏** - –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø–∞–¥–¥–∏–Ω–≥–∞
4. **–ê–Ω–∞–ª–∏–∑** - –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏

–ú–æ–¥—É–ª—å –≥–æ—Ç–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π!